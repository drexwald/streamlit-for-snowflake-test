{"docstore/metadata": {"a733ce94-88ff-4886-9e0b-6d820b3450c3": {"doc_hash": "01632aadd138fcb3a6dc80e35dc28c6eeee253629048ab54596855435bb4170f"}, "b00d7971-ba19-4e91-80b7-79e173259844": {"doc_hash": "53bb0e012cdb0e81effbc3f292fa724de01adc5b4ee400dc8aa5a525dcd0e89f"}, "f5520389-916a-4f5b-b7ad-5f2704df39ff": {"doc_hash": "dcafea6b2b54e3baa1012703dfd1ecfc332a8b9d6b280a986323e5700e0cb031"}, "a0c0e3ca-96fa-425a-ac47-934a5b00f302": {"doc_hash": "362b68b060a924b94f3aeb3ff57800ced489702574c907374a5ce867007481d4"}, "9d01a50e-edba-4f36-8e93-0d3b04349de8": {"doc_hash": "781f36ba92412729cb81d639bdf20016a00b7eedc0f6773f0294bedf4588931f"}, "79196d52-ee0a-4677-b3a7-608151870f60": {"doc_hash": "d8fc5c3ee3fa0c3246ce6d74486f994f7bccda544baa7c69dd891653670da1a7"}, "8e26a0b2-a45c-44d0-ae18-832336084a45": {"doc_hash": "5136cb26f666792ef39a7ad58961df5b945593b46ffd860bbc391fb5af21d241"}, "f0187d12-3c4a-4514-8b06-2b5587c530a2": {"doc_hash": "b46a0289185c442aa8e240b86cf49f62c3e8f175763b3201a3c78bc7b5e6c245"}, "b8fde317-d3b2-447a-8ca6-1f48a51493e7": {"doc_hash": "45c441bddf61804595bf93570dc1c04dc7983b17ca56697eb2a7715023a11cfc"}, "7fb7dadb-8891-473a-8a34-8bffc19aa4a1": {"doc_hash": "1f4ede5fee8ca9b53c82c247aa47895b98e98895ba63bba459efbabc46154e2a"}, "aa235d21-c392-45e9-96aa-9896d9459fed": {"doc_hash": "2277ee1717e1bdbd20c41ffb3bbefc9168ad989ff6922ca53e1b44f97020120b"}, "ea6bd15d-3ce7-4541-a567-133997af210e": {"doc_hash": "4761af4dba0242367bb58e9141807123c6e0ed2159985fa994ec05f9073b21e4"}, "adc92171-2048-4d45-83a7-78ec6015cbf3": {"doc_hash": "a04e18637bb01429c7fb7d6522a504a22ab23640bc96c02b4e11b941c5e5fbd7"}, "5ccb705f-32df-4078-8164-e24e18aba0b3": {"doc_hash": "97dcce27fc45baf20cc1bd670d2df7625f46553fc2064351906b4ab883c8055f"}, "32214cd6-3dc7-47ce-b6b3-a9e381e23ffd": {"doc_hash": "f45e20adca33fc0f282ab160a0849a7000350ea3b1626f0477440c77798e53a0"}, "cebfe9c4-47a4-4b54-89b2-82e9dbcc77bc": {"doc_hash": "3096cbf3da106524364804b6ef21603744bcf308535cdd11f08d25a7bfa7a4d4"}, "2a02ca36-b912-4254-a375-2d5a9381b4e2": {"doc_hash": "4820aac539384abddc92cc79c692b5729f2e271d3bbdccc1e05066c34284cd78"}, "b9074f0b-0481-4945-83bd-37f0f50ca259": {"doc_hash": "7592479bfe424b894c4b0b4c6fae8c66f781f3bbd61d6b64bf23a444b183ee99"}, "9e931095-a299-4167-be3b-24230692d612": {"doc_hash": "9fbfa869dccde122238d7fbc373d0249dd612570733c1f0dd741bcc05efd2d6d"}, "b594d96c-ee54-472f-a677-93acc0f02801": {"doc_hash": "e14527d6ac184b60702108f1dc8b6e52095472fbb0c86574c173b791a1789f3a"}, "ecbbc8a8-4438-49bb-b4a6-d0d04a57121d": {"doc_hash": "3e9b0e2aea001e1e5c164c5489c4c731850eb60b11b154763882574e438c3039"}, "c9a88c71-575c-4fec-bf08-a8c33c93ccd1": {"doc_hash": "822bab9ab848817d6c22df1d130bcb7fa0c7d6b220c3510016a0d2d36fb1b3dd"}, "5b8bf068-4a8d-4980-bd1a-23f5a9e5c6af": {"doc_hash": "a36ba42614a8cd2befe66fa94f1201cb77fc509a1af2fecce2f8c438bcee341b"}, "7110e9e1-dbc6-4d81-b637-7b742604fac3": {"doc_hash": "550d15c37c4b325116f36184ec98c5cb7a7591550f7db2b20568acbb3f409b73"}, "1606dffe-c3f2-4317-9b82-d8650be2c097": {"doc_hash": "b2085ff7e5c7eade840e81504b0d5f4f4e52fd3475f3ecc97745b4bef464cc1a"}, "57f4a40d-606e-46ed-9fb9-f5809e036e03": {"doc_hash": "a5e09e73eb03221575b65ccdba3ec8323d9514179dfb4b4ec9ff779ece407bfc"}, "27a84d3b-52ca-4584-b2f5-884794e5b582": {"doc_hash": "3ab7238f67e02335e2187a965fc8e6fdbc7aadb455950ccf6873521db3710883"}, "618328f2-b119-4312-a059-9b30454bfda0": {"doc_hash": "d119dd605a57c60ea6d180b23d77992c4120d11777100e5abb4fe6b9e3460c0d"}, "0c9304bc-f643-46e2-9836-15d41e33e71d": {"doc_hash": "e676c037476dc4d2ddbc2963f6e8aa663c13ef8d994cee7e8d72d9948f62ceae"}, "fb4d764d-4c1a-4708-8793-ab32c615f94a": {"doc_hash": "9320bae7cd2c2cc80cadb7957ebcd403ce164c7e04da75d8e434b7763ac16d1d"}, "3fb8747b-1767-4289-94bb-077baed77c24": {"doc_hash": "507ecc37a1fc65554d1d64ba309067721adc311bac64088013ad11f43e037aa1"}, "44e31ce9-c501-4c3d-8663-239a6a9aeebb": {"doc_hash": "402e2109b2798958e62cb6f565c5d649091fc03180cecca48d66428af01919f6"}, "de1ce3c1-eb1c-4fd8-b5e4-8a0f8e85f417": {"doc_hash": "23239bfd39dc597a3aab1ecad0214342bd7dca90e2cee7e4f0015e9fc735c657"}, "1ae2ac07-2489-4f80-84d2-a7304d412185": {"doc_hash": "3b31553f70c48fb03603202c295f61f96ebc94f0255387785f4d4ba89707604f"}, "e55948c6-d85c-420c-b00f-28fb5ca2c259": {"doc_hash": "402b7f4962fd396ba0f7bda1c55fd90a40dde6ae18df3d67a1d5c09eb5aced62"}, "538fe6aa-ce33-4c43-ab73-74f921399f3d": {"doc_hash": "67945af03ee96b0ba167d0f4e01cafcf7228ae760fb1e12e39979add1a4bfa96"}, "ef50c065-26ed-4c35-adc7-a4396ccb7ef4": {"doc_hash": "ccea9c8f03d9fcfef9e3c653866e5d007827155e175fa060d3c9028f1aaa458b"}, "ccfc5bef-2c66-49ea-8648-74470273d1d1": {"doc_hash": "866e1c9b061f6308673f610c8c4950774409ac44d400768eeaeed2a89dcbed3f"}, "78a62381-83b1-4285-82d2-7560cfc42b6d": {"doc_hash": "435c4d967430b0efaab9450f63b6cc53fa6cd3de203316412119ce0bf8cdc85e"}, "db699eaf-ef94-463f-a91f-cef0e7bec50b": {"doc_hash": "d9a985066847707f85e043f923ea52097b320620a162ee9f8a4ab164144ab88f"}, "912a9072-7a67-4441-8ede-0931561f03de": {"doc_hash": "e774684d334f70c0edf395fd3a4235062f20d1ed2a5a4f8d000bcfef84b885ac"}, "6b7a554c-7339-420d-bc4a-9851472bd4ab": {"doc_hash": "cb0382f1735ced2f58f4bbf2946b72572866ee9f04ead70103483c486e143813"}, "20f5fae9-e9ca-4a02-92c6-957811cc5480": {"doc_hash": "33a1df7c942d582daa6fb413e5e3e16455748d8d3124209626f4ae73de7060e5"}, "17a26717-b9ce-4080-93e4-d6d7b8327b1a": {"doc_hash": "0c0d8b1234c95b7d65b8300d9119dbf3cd3bb7b37189bc1ac61d4645f2f97b9f"}, "4431a0b2-ff50-474a-87be-251ac8adefdd": {"doc_hash": "1a7f23a1dd9ba87621705412adfa09263332b0468d0906c81fca30f87304af32"}, "08c578ed-4907-4a25-9475-a0c0946a2e29": {"doc_hash": "40eed7150f10060482aa7093ebc6baba0c43af220fff68719cc292afcb729f1c"}, "89d38fdf-8098-4191-8d54-80f5a9088055": {"doc_hash": "7bc2654ef0f8d687de9a4528517b54955f06b95af8831dbbc6febb0f780c5dd3"}, "22e42d19-9c1f-4972-bdf6-a919f6adb096": {"doc_hash": "9e6a8133cc801d45aa3b8a8e0567013c5ac578b2020d0b4828f0b6c0af527ee6"}, "73f1c556-d756-498c-9334-4f18bb8743fd": {"doc_hash": "4c97db1032c391e2c0024a244b6f380e28404ec5c9dcf1f2921fadd9d7f4e0a9"}, "7f7fefea-5060-47f4-b742-7dcd757dc6ce": {"doc_hash": "c5696d8508a2da50475c0892b868cad3b571752bb28c261c7f46faf8c718feea"}, "420c7222-1f01-4417-be2e-acbe12b5f0b6": {"doc_hash": "b2d13363bc920e1ab2025c01527624b7c909f35b6b6ee81f46a887e696770b15"}, "d55814fa-8638-4141-8418-bfeb18d7ec28": {"doc_hash": "e7c3c7a872d2bcd2365b2a4d3973284fd6609001a8f9456e8fc3c8ded7183a7a"}, "508ab91a-2f2e-4a50-a8b9-7c203c9c9e11": {"doc_hash": "073932a32110b840684f155c55245a9945a646b17796afc6d2c8e1491065a373"}, "691f3498-b4b5-471b-9f41-1cdc3e0f13fd": {"doc_hash": "15e0f5ee52599c93bb41e9e7e3e1c548492b7a286e579eee0503620fb11f2fa8"}, "1a7ec057-f8cc-49aa-ae5c-5aecd2755502": {"doc_hash": "36cce28e542a1809b74c9a28cbb1af2d69b3c10222cfa9227dc1896bf7509a10"}, "07c63003-cb6b-4d7d-88cd-8c8d85dd6a13": {"doc_hash": "d0ea7505cfeaa1f50694c704143374ef889549c733ce6544b7ad94fcb2dca54b"}, "43fae96a-eeea-4793-b587-dcfa3bd5d161": {"doc_hash": "2382b4beb3cfd8e70446b5b38de0d40ea815cbf0ab3edf61a5ebac2acc8c2b3f"}, "7f2de0fd-1545-464f-af39-77c2b309ef45": {"doc_hash": "4842b99877000675e49e6354f41902d9958f8ebf38314d971493c50e3b2328a8"}, "0e2e1c9f-ba94-4652-8c35-1a29af359be3": {"doc_hash": "74f5629d7a1fe711f246d6deda6738239916d46a16cf6ac054f939bb5e7f9dfc"}, "93f00707-9930-46d8-9f06-0fe4c663b71e": {"doc_hash": "8dbd2b787b3c301c391c41133f2f648c8a9c34ce8a33775accea47b47ce50a26"}, "14fbac6c-813b-4aa2-9bdd-bd28b6bf26de": {"doc_hash": "619249cfe1506ba151f1252fdfadd66527959e9a24c9891d2d01b2529eede8da"}, "ad700b0f-aa45-43ae-83a2-be28190729d3": {"doc_hash": "537dd3dec5470a8ba7dbe058a64d0ff57318d4a20feed640f929aa0ae424bc29"}, "0f5ad8b7-1624-4629-ace6-bcacee58fd7a": {"doc_hash": "87ce90e90ddcfee6fd32d3c357d68aa11b6a0ca779d10c257518961833ff69b9"}, "d573fc26-40e4-4053-8a73-065d4e7195a2": {"doc_hash": "3137f1154f0abd265b35d1dbed3ba2487df2433619b3eddf52b271d8d659363b"}, "37f9b070-a014-4040-a651-7bdd7ceead4d": {"doc_hash": "7392ccf5dec7adff94ddf5220d360df9b2412a9e4f99f29a7d3d7f2b392419d2"}, "d8abe616-ae9a-45ee-9812-de5e3ec23f53": {"doc_hash": "8c2a589d61b93aa6a7caab054a09b14faa7c99ec57cb0168919e7b64fe42b98f"}, "b20fb21f-1865-4b97-9360-ad5e7fe7887c": {"doc_hash": "da76843f3192eaa84aadb39437e6ae3833fc8a5c119fc673db2e4eb4f70d55e6"}, "cc8b1baa-3cfc-46c7-8ce5-2d2b3346356c": {"doc_hash": "3c677c521ffbd0e01e45aebcc11d2a493e0953824c8d9bde044dbb5109573f1e"}, "503f0a38-5972-4d79-95ff-23d521f6cb1c": {"doc_hash": "d5bdc39baedba82cbb0ceac4cebd7e747ab90b7e6bf41fe01bc31988ecee9134"}, "5c423d2f-9155-4291-b662-d574bf78b36a": {"doc_hash": "1b2fab49ceb859b8ed54186f94a192afdfbf7fa90867dc6439f92cae85ef1f17"}, "120f2c2a-99fd-417b-999d-3d57d3f141e1": {"doc_hash": "a509ddc2bfc03e0ba975879478f9b287b67b33766e4244b394811368d3d1fefb"}, "ac24a6f5-5d32-40b9-aa87-38cc1e8388b4": {"doc_hash": "cf832ec8e466ae0c37319ea286dac1be71e5add0596e755b673c540825a39e8f"}, "a7137453-c90a-419f-9667-aeb74d5b922d": {"doc_hash": "ee50dd863be091da1dfb89aa621a419f890f961d1e8b444ef1a026cd5110e090"}, "1f5b1576-4cb3-49fb-9f7a-b0f115c61c00": {"doc_hash": "77ce969b6b4564ecee263ebd17de48d90c933dd25735d878f7a0734c6f1589c5"}, "1f9fcb0b-9b01-4bcc-87f3-1f874bdd3beb": {"doc_hash": "c4d00b4bdb450bbefd535d696439aadcd1a6c4cbc212b1a1160f83792e51921f"}, "a3b26254-6fc9-4f1d-acb5-e0955092485a": {"doc_hash": "6ad1ff3106a1b789ca8496f8372bc81bd1a54b66c2ada17b9005875f6601e775"}, "483d1b09-d12b-4cb5-89b0-41060cf45f71": {"doc_hash": "cd2be00bd9599e57d767373b60d03e45087d8dc67b6ce5f917e2748e1aa1c40a"}, "d3328c64-b8ea-4e21-a342-3f888c306283": {"doc_hash": "904c648182bcd22140f2fcfe1a10e8039dec2e245a982d5e47bb31da519378a0"}, "6187b8ad-6bbc-485a-9982-605a29a083ff": {"doc_hash": "828bebba62808aecfe7bc9a31933cf36792dfa4504422f1adaff40d9eae24652"}, "53787251-c893-45bc-9459-e85fed273463": {"doc_hash": "9893fec601833b91a8b44edab9569d3bbae712bcd19d89782deb90ec5de35514"}, "e5a35427-a7b4-46a1-8687-c0191f6a1bb2": {"doc_hash": "4557b8241ccaf140c2ab10859b63292e82c4b4d71148c703d2f431380b7fe08a"}, "d2d99ca6-f126-44d4-9b71-596df4d5d36c": {"doc_hash": "11cce9c615f779e06c00947b05a58e5af8519fc1235a578871ca85d211b5676f"}, "b3cc1582-c7b3-48e2-92e0-663037e43711": {"doc_hash": "7f3d1d34ae5127dbc03c10122eecbdca27c1908269e47a6fcd75e73ef478432f"}, "9d2ee27e-3301-4b04-bc24-070959617957": {"doc_hash": "8684b78fe2d18bda2c0dc9cc4786038ee557cd61cf4acc815167403fed7fff3e"}, "b43d78fb-9919-4b37-80fb-47199f52e96a": {"doc_hash": "241626a7900e01de5aba7bdf35279e57668060af23ea6082b71ca42734885916"}, "a5d69d17-b9cc-4386-848f-0d5ce4bdc6b6": {"doc_hash": "2748d9430d37a20898a7d53df566db9e95136502f0a211d691c026680b941174"}, "0d2d79df-746d-497f-8113-2f8762d21247": {"doc_hash": "23ce285fac4f0ce63b6baa5a7d00eac546174e2caa35bbfb6596ac03f195e548"}, "18c40c4a-dcba-41f1-ad88-ad6af3c275b9": {"doc_hash": "afd9fc74dcabb0e734d238197fbc0b707145b1b41acbbfc26abd4aca488f1d51"}, "c68b55c3-28af-4588-90f3-ef2a438c3974": {"doc_hash": "95df5cde1af8551b693277f3d98f0cb87de7f3f2f82eed375533a1097d531cac"}, "cc362518-dce9-434d-be8c-82259f3d8849": {"doc_hash": "f8270b563332f8cf119d1bd0a6a58802debfc2637e350ebb8464a52e99f6bfde"}, "bbf3bb87-bb52-431b-8664-2f7fd39c79db": {"doc_hash": "68bd493ef7441ab6a88410d0030e51c6a5b7ad24143b23be53316689d4ca52fc"}, "1eda91c2-f7e0-4241-aaf5-4cbbf5f70d56": {"doc_hash": "8d8fee6e95541ad7a55081664346fce8aa6d17c4dce3dfaaa52664ff01e573d6"}, "b480a476-3b85-4054-ab87-6f772877103a": {"doc_hash": "a3a843fbd6e634f210be5e15a1b132da03d2ba3b1b17c2ce50e4f300efe333df"}, "519557be-c726-43f5-9451-55eb94154174": {"doc_hash": "dbc19f1927f88afe31ae1a22dfb57c1a7f7c04bad3073157554d692587621297"}, "0ac2473f-4d16-4a8e-8c20-a65723025631": {"doc_hash": "b6fc82a9f8cae881a8726ff5895fd2250d39a0e607b7cfd9da3b17e70e8057b2"}, "ae36a23c-fce4-4d72-9dab-e9de1f88bd75": {"doc_hash": "7bca1211146b21a13f5f45e6f62bfc9b9d29d5aa5dc4a23dcd8b27f7dd8c5a89"}, "e791a38f-9916-47d0-806c-cdcc8ac25756": {"doc_hash": "cfe386909fa13d26c97c3023a3802253489291d9cd6a2b44ca8939c2d72e1f60"}, "2f41d404-ece0-47fb-9dc4-af44212a0394": {"doc_hash": "f5302c12f7f09298ddd159b00cff932f3a93258ce603ed14022a2baf91f50734"}, "8bc41016-0ed5-46eb-a0ba-538cafc881c2": {"doc_hash": "93dd95138d130b79ce5c41aeab42fb37547fc6c0ebf071e927aabed4159111b5"}, "dd73e4dd-2354-4d4a-bf6a-b8ab18138c44": {"doc_hash": "253f661e26c6a5f494c0e3013cbb7ff3bba74d003334ecb7aae7953f1bdcdac0"}, "97c90058-8c5f-41b1-b0b3-4689824bd4b5": {"doc_hash": "aeda5578d80f52bc117a4145402acf4b991c05e6b3bbc782ca7646c0cfec2778"}, "ccd05586-212f-4737-b792-275a5a88886f": {"doc_hash": "3b7d2f321d65b1eceefb316cd295268d660c7f3c0ab85c1e2fa32be76b2fe2a5"}, "6b6d9dc6-154a-4247-9bd6-cf4b482b002f": {"doc_hash": "1781ff2b6b16eea9689d0f120d69c589a068a837f255a2fa413e52cff81ec7f2"}, "8bc124b2-cbb0-4133-9bbd-d964a0304bd0": {"doc_hash": "180f272fa63a79c33553f6f12e538bde0bf8bd467fa885fa19a4646089a8e518"}, "5df8b2c7-12a6-4784-a1a3-d954ab6c2ac7": {"doc_hash": "d8b5bf7201eca7bf89217932a66e28de93d5bc1c5c9050e9da2b0aec7c26f0a9"}, "64466d9f-fd2e-4f5e-b572-c45223a0592c": {"doc_hash": "82478e193eebd3d4216d14b0033f508649dd6df6eef31bad9f61ce3fb78f28d8"}, "034d36c7-152e-4f6a-afef-dc4ce449c2d3": {"doc_hash": "e427dd6a297838975681f2d32f0392050d68af7d2162186b1e79de0610c8dbe1"}, "4f5c7ad8-94b4-467a-85d5-7168af917740": {"doc_hash": "a6f15999ea9116672bfb2beaba0b772ac9ad5f8137fc8a61103c1e595dd16a39"}, "3d88415e-8318-4ddb-b469-dc9ff55175d4": {"doc_hash": "003e21d56b32d6cef55c11684b0139438978a0ba462c9764fd4cc677413f668d"}, "77b98518-12d7-458e-acf9-a9b44ee63255": {"doc_hash": "a36b3b67305c23a3eeb6dbe2ce65a5cf1322dea22c9cbdee25d9b7455951951e"}, "09b082f7-50be-4097-a775-7ff5b1174ca4": {"doc_hash": "aa3a18b1f3ffdbdfdbb7cca7a835956947f6006e93fe7482b4fd207f1e6bf911"}, "a2e2b7ec-fa03-4daf-bb58-038b18c00c08": {"doc_hash": "bd778265753ed9e6d540d7d560751f87c01a692321525815b725b7e8ac85a7d6"}, "8d1609f8-c946-4208-ad59-6d85e89286b1": {"doc_hash": "8ec8457dded9bd46c8d17ff698c7d8592f4cdf4f2c62a75744ec999d0f460271"}, "78d6d00c-3aba-4352-8a81-297fe1e48080": {"doc_hash": "77a292b8ab2b5a217f144609f5467ebed33f8940b5f66f01280be98aa786363e"}, "2ef0aaf4-f004-4d9d-b4a4-4a699e880b0c": {"doc_hash": "d5d949c3ac21b84d7be414838a5eb14deb3ea50824a4e97f53bd38fa992c0957"}, "43462acc-a4e5-4aa4-9726-7f2cdca2ea0b": {"doc_hash": "d46fa2b437b4be071b408bd433d5f23fbd8318931e088d7eb0ea1a2baefb9249"}, "38ddfa0d-00e4-4f26-8860-1efe4b85eb91": {"doc_hash": "d06e323014d80ae064486855aee3842eac44ee94f685a71385b473131219e10b"}, "11567799-1cda-4c36-a5b1-7e53d7a7a5bd": {"doc_hash": "e014a5992737095922dafbb7a50318b8097e35eb370e7a7b880c641a01a968ee"}, "2400f6aa-f12f-47ab-9ec0-ba379edd013c": {"doc_hash": "9ff05633d8721aff411f8853db0825becd68943bea02d0d01649c456989a7021"}, "ed01398f-a4d6-4179-9219-161d9510fc80": {"doc_hash": "e3e5bc3d5600ce00014a81a36b1bf8f298b4240c7c0083f610bb0a50e2aa247d"}, "1a7dd1f0-b8bb-48c8-9830-397c055d758f": {"doc_hash": "0ce10a4b20d99de74b895ab26bf13162abf864e1d2efee92dab5c34a6379eee1"}, "e0e6b4e5-595e-4ea1-add2-34c110f5c71c": {"doc_hash": "c0cccc2e16869723c466f0897b1e242823fe070d45786c6bdb5c8689762c0395"}, "0ed0ff19-5549-4533-95b0-0277a217e31b": {"doc_hash": "99010224d3dcf14fd4b8428272c28579f03107aece921fe08a1542a5c6e74529"}, "89baa835-38a8-4904-87d5-102bcb0e7361": {"doc_hash": "3f2616914394f4b43b45b85e8fa3c3ce7bef8d341b0dc126082132fcb4fa3e80"}, "6b31014a-eef2-41c3-9719-6f34d12a09e2": {"doc_hash": "204297a776128693f4240e183f0f58c7b0fd2d18c0e997f49baf548c733c6473"}, "6ca2c4c3-5820-4c8b-bbb7-1ab56dc84eb1": {"doc_hash": "2d689eb4aa87ac17adba2c29148984c95dbca029b44b2374bd60c45cf8e5371c"}, "cedd2731-34a2-494a-90c5-1bc11c9b9fb9": {"doc_hash": "4cce9e886afd56b4a06c2b1e833118ac4ade5fb3b117fddb7be539bfc59c82fd"}, "c3c6d656-037e-4bd2-9e97-1df1c58225e1": {"doc_hash": "e809e14f27c4f2805398fa38537d04be40bbc4b0ec695e92055896b717d1ff22"}, "3b09d4df-7047-4119-8caf-61645c291808": {"doc_hash": "78b0dad73f015b07c66b6656832857835e5b193130f7cbd2a7e1c448a4d95c30"}, "1d321286-2cbe-46fd-ac0b-5b8f5d721643": {"doc_hash": "17eac5a22dbe7b5378927e620c95d36e2c1ee1533d67bde2677025cefd921fb1"}, "768217be-95f5-4827-93d1-0fce03320ca7": {"doc_hash": "d7fadec146d8924460f78bee8f4984339c4e71caac12fda6f1a2ce181441240c"}, "1070558e-1571-4897-840b-14d0f7aaf36c": {"doc_hash": "cc0ad6d7d2e7433ec10af457afbd06507a6fc66b9ef9bc4127a3a275532da65a"}, "8e123543-6967-4d54-b0bc-43260d86f3ee": {"doc_hash": "c034020c681c34a0c7784c91abf45794b5b2393bdc5542807ad9766ad5d481fd"}, "d84de500-3e2b-4d48-a8eb-4043be4cb958": {"doc_hash": "952cbe4da904bc59366f4d7519f7c7816a781a0671bc8a05962c1acb631b0e0e"}, "f196f29c-9898-42c6-a88b-bf11d66c381b": {"doc_hash": "9c932a48adf151f3e3d6a52d7267e0ba415d95d3651130c843763588803cec7c"}, "bf8c9660-008d-4b09-84fe-d3503830c223": {"doc_hash": "0077447c34841884bf4716dfaca02e600179d31216137ec72c26a71209cc5f28"}, "d8ae9318-28d9-45d5-8281-af39698cf6c1": {"doc_hash": "1743a4cdfc69dc44cb096233a9829545da1449d0b507ed2f3828f34a17547d45"}, "ef20831b-1347-4943-88b0-20c8feaeb95b": {"doc_hash": "16db2c4686707854683ea297b5df7c27b111f109e410ec568db0c91558e55d43"}, "2f0920b8-0f84-45b1-bf24-f9c08140496c": {"doc_hash": "bda32b5aae5dd96bb08b79e2e4327f2b54034bb1528d1147bada8197cc67796e"}, "1d2987ff-b8b2-43de-8580-453697c32c65": {"doc_hash": "e4af435a25667f963bbe73e86d1b6ed1b8b3ae8e02e7e1131f5ca5f68dd3107d"}, "6ee05085-cccc-4bae-b76a-17e5e282cf37": {"doc_hash": "dba8c2af61ad3308c54f6bb0b64a5baff487cdcd8e8cba3b7655463b20901d96"}, "828605ba-5131-4e7e-8acb-6f58ee4f9e30": {"doc_hash": "65514bfc2cfb8337bc68bc3adeaaa9bd20b3ba042ae82cc7c6e82cbf74df5b43"}, "d08dfede-3afb-4489-822a-7baa362c62b7": {"doc_hash": "33e12071bdb83f37ed646dc6f5a7495b2670c28c5724da885106efc142eff5a5"}, "1b7c521c-b322-4871-b154-cb009d64055d": {"doc_hash": "fdd8450f112f89b47abb09fc32dc32ab60f155070d2a4fee02863bdf5d805473"}, "cb6712de-db0a-4ce5-b7af-044b7e734d8d": {"doc_hash": "5900798a5778691084edce538bd235c5e6881dfbeef61162a73716b0bdf881af"}, "4a2633dc-9e5e-4263-bc05-80db13c174d2": {"doc_hash": "b1bdb3dfbdbeecf649a6a7278f3c4a21d6460ea550440d7c355085dd94574f4c"}, "75839b32-9fad-4693-925b-b66c76b23b3c": {"doc_hash": "6ca5996aefc16e245dc7f840c8269fe7dea8c85f02d0ead18c5a3ecec6192229"}, "41154eb6-1efe-48ef-a419-bfc90bf9147d": {"doc_hash": "c2be901d927b539ce1930f40f7a0280b931ea4eb80ca951782c8362780d31e22"}, "ca7a12fd-763a-4702-a99c-571c4d379705": {"doc_hash": "5fb6563cbd32d0c084a1482dd29e47362b9269759b25bb96165ac24d30d836b1"}, "40463d32-8daf-4a36-8770-499d5ccf226d": {"doc_hash": "0abc2930521b6950819d1d6dbfee0eb204f318bedb489e0f1ea574070a19a078"}, "5d096a7b-6ece-4607-bbb1-bd33aec2bceb": {"doc_hash": "6c562c2a12410e0ca72fc456f413cfe0755c8947f0217a6aec710f255d4cfc70"}, "fb39b745-b644-41a1-872e-94da3cc1e42c": {"doc_hash": "f4f7b47a08a1703005bb14149da0cde8c618c49780bad1cdb07a1f5211986438"}, "f9c0f9cb-6fec-44bc-8ed1-0f2f6b6ab300": {"doc_hash": "92c4d9a5a66b1a615fa762cb3b9d405eb9ad8e372cd325d610d26353c3a93d89"}, "5b59bd34-a1ea-4d8e-b737-b22ac7698046": {"doc_hash": "dd7abb5ae62bfff0363e50e6785d0f1936b1f74e605e5190979b2771e27074e8"}, "9ccdc480-3dc8-482d-9c6f-7abe0d886db8": {"doc_hash": "dd7abb5ae62bfff0363e50e6785d0f1936b1f74e605e5190979b2771e27074e8"}, "98294a4b-8e60-4209-9d18-6a97ca746522": {"doc_hash": "af25dcfd96bb17ee72053eeb4e704def04ebdbf1da67a939fe33dfadeb5624c5"}, "dd6b45a7-b792-4530-a976-dae2d96a08b9": {"doc_hash": "5fbb484464c7629906e74c4dce4cbb1b2eeabb3d1880ce2a9790256e3966538c"}, "fcc786f0-f412-41fc-88d7-7af3ff617a73": {"doc_hash": "9226f9a9e30f03ce2934fa94f66a05d8f9faab4f973cd9080dcce30a74848854"}, "dfb3ed91-2fff-4c1c-a57b-07476bc54a56": {"doc_hash": "6f68b32e0d79e10258d637bf4e64742543096a526f8d859799e59080fd214e2b"}, "6dd8cb00-c892-41df-a601-7d9ecdd8c329": {"doc_hash": "9d9ba4da7309f0ba29b00109c83d5bbf33a617847fd1622aea5e84fed0b7c0e8"}, "38d78a61-473a-4827-b6c4-da6dcf9941f9": {"doc_hash": "5391a9ffe1c3ca52c9ae99ee1c0088830beb2f0af106d9a2a1a4feaa4b6b5888"}, "ad594b28-ca2a-4efe-ab33-34d74644e6fe": {"doc_hash": "ed6be61649a10838ad645f5513441acfd2e0e16baf1926a4d91d31a2ff73097a"}, "d4a63ea7-31af-4e4e-b525-98b9f6562d2e": {"doc_hash": "7c6c59433e19e3021c2db4ecf442baff9b7f79b4b23a24a7d0959da1a6f4e70c"}, "1d269188-7a16-434c-8bde-f23e8896f10d": {"doc_hash": "eedec6a06bcb2bcd94a04b09ac778df96fc5af7abdb0572a2d734cdfec204f20"}, "d9b8fa70-5582-4dc6-8999-b43d6b85d217": {"doc_hash": "2562bcda001cc05fa0610c60089a93d8386c8aa6c8f73e5f4edcb113ede81504"}, "e9d9002a-756c-48f4-9bcb-3107519b31a2": {"doc_hash": "7225181edbcff9f956969ca7c43674f42679df2198cc4857579aa08b2082ab40"}, "738bf1a2-eaa3-4dde-91da-dee0de861383": {"doc_hash": "6a9c210216190b3ed95c51332ed2077f67b0adad2c9503a97168702ad534deb2"}, "808c1c41-6d28-441d-81cb-b6120b8f9dab": {"doc_hash": "6e37f75e1b0a950853771e46699ec9c02422601543c07e31f212503edd846a66"}, "af78da87-a865-428e-b2d3-940b2483d507": {"doc_hash": "3a07da1fcee4fb72b93f7810bfa50f9cc12d2b3032b3e5e95590d7a53de204ff"}, "2b1ded5d-f7ae-48c5-9d8d-fe2bc4859c33": {"doc_hash": "bbe2094a5bc830f0099925d95b0c997dea61bb5196a09f53b5feb0967f9be532"}, "95437d44-1e4a-4f50-a06b-a70eca0d15b0": {"doc_hash": "dd604fbb223a9195e4ae61975cff9bd6219d6947e7a492b77c6ec665db4dcb4b"}, "ed1a9377-cdfc-4f41-b2b7-d2d1c1a66073": {"doc_hash": "dc1eee4a7c078471069983d385ffd23572076b59d08dc41fc0a24f13ae155b70"}, "9b0cadaf-a01a-4018-b59b-bc92ec0d4da0": {"doc_hash": "02b12088a5a95540ee2c6fe16a55be110cb4b4ca7cefc3e1cd4bd88bc4038de0"}, "070616f9-0c5f-42b5-adac-e3751842b2e7": {"doc_hash": "bafd1b4ee6c57749f39b9a9c530fc7fb8ad026625e9cc2d88e5cb9dd249f3596"}, "0228951c-1a5a-4a45-bde8-20e01de18878": {"doc_hash": "73abba5f9354fb4315873cdfb5c8bcd67162cdcc3cc79e6775d98e9549abaa46"}, "af3d69e9-ff54-431e-8eac-f06dd4eb3e76": {"doc_hash": "6ae1938c5443ffe1bdbb3d8bdadb1d837170d5b68c9cdfd0002b3dd2d335f416"}, "86d2c220-0a8b-4337-ac14-86b64db7f6e1": {"doc_hash": "1385313cfa3a92b642f82ab6cf70ce9038ff7c61e2e7b571ecfd681695731369"}, "66dafcac-9034-400d-b7d1-6cb021583260": {"doc_hash": "f87a870b0708d950028bbd7ef4c307023af6ff4d1c8a6a3d82dc370486e2fe30"}, "24d4f034-b9e4-4369-9c49-3119b310b548": {"doc_hash": "ac8d848ab3de9cf3e06741cea251fd421506fce9b2b5d3d5c8502abdac9ce643"}, "dc87abf2-5f82-4618-a906-f262c7e32818": {"doc_hash": "1b282df3abc787a6e021930f0d7377bcca7a9d3ab82e3cf1a1da898229a44cf0"}, "1e170d7e-0219-44ef-90ac-680024e4a38f": {"doc_hash": "c2f03cf7e14bb2dbad234e890ab370706302418fd1dad5288b3f456e6fc16ffa"}, "2a3b5954-007d-43a5-904d-29a6a9052e82": {"doc_hash": "2354a8da4dae7118e7cd6712491761dd059b3eef2a0af8a1558e96bc1223f3c7"}, "fcca436e-32b6-4131-9965-551529181e79": {"doc_hash": "f7e1640517e2df2261059758144f98618f723dc136f7446d7a96edca8989cfec"}, "541021e3-62fe-4a44-af91-cbc6f109976a": {"doc_hash": "60f6ec35f95680ce2e26eb1ecd183a58f02d260cae3cdee9c77cce5e8f837520"}, "16ac756b-c246-4cff-b668-49cda942e4b3": {"doc_hash": "dd4b4ee526c13a426fa1b5757818637032d532bf0a5dc2316187b881a8b3b40b"}, "319c2a65-fcff-4bdb-bc01-11ae4ad79a6e": {"doc_hash": "54e3e13a5a5d4530230d86153e45bf1eed390661ab12a95bda12f3b0940f643a"}, "ba434c09-b641-4a78-b697-b9e3ea4e85f8": {"doc_hash": "ea901055725528abae1717d9318ecbfbeb11250cc187a67ccaa4f17b2fc525ad"}, "59d99ede-5036-4de8-9cbd-9c179de7dd1d": {"doc_hash": "17b9c01a6fac053e2719ed6c4a18fb9f289bf576acf45fffd9d0314d7a35876e"}, "2dfbf854-899d-4813-9a03-9ac74644979f": {"doc_hash": "d8791a055c9f63aad8928dd27afce07641f002787404ac632b3f0dff3a90460b"}, "56ddda2a-f3a5-4595-a525-a6234ecdd8c2": {"doc_hash": "300df847f63a2d96ca1e33d91cef273061c0d2ee40b3d4c9e5ad6367f721b686"}, "60ab73d7-371e-4eb4-86d0-dac99b341355": {"doc_hash": "9d785d94a4e86bb3df943bc96abd38905c565fef60eeb8dc50366a86616f87f2"}, "826696e2-85bc-4f46-a80e-0281e76111bb": {"doc_hash": "eae7aa8ce16630441991a1ff96cbe6fd64cf6d6bb0b70f55bb4d9d4c03934bd4"}, "95f884ae-7871-4dd0-8826-34b4a1b8892f": {"doc_hash": "9f9646995ce4eca7ffea86b43f1eb0164c318b54941c1afdc7733fa9d45186c2"}, "84ea060b-8647-46f1-b040-ff3ba0bac1e2": {"doc_hash": "b48043687f61b9e31c1a2a3b4709d291253a3cf4b693803bd9c61e1a6320d9fa"}, "8c092a1d-422d-4aaa-b04d-d595eebe7a2e": {"doc_hash": "6398340aee318d0882b13f59a01d4de8c65f3051a14137b0e48404121e310241"}, "608aafea-5510-4f18-b150-b146f3027a75": {"doc_hash": "15ed8b57925f72e3041b9e5c874ca7d12e34ed4996cf10939f9dd7275088e05f"}, "b49ef040-ef86-4025-834e-5632d2352d13": {"doc_hash": "dddb8a566927221dbf892de9ea7a2f47e7a462655fb98be1f1972c28f32dbc19"}, "258ec2ca-b291-4034-8c40-8bc47c56f7d1": {"doc_hash": "acf2334abae025b608e0a3bb4fea38586325388be2f9e7e090b5ebf06ae8c5b1"}, "ffc11a9c-7ce3-4d0b-b52c-d14b46f8c798": {"doc_hash": "f89e6056ad5749a2f32e62b9836f5afa8aad6f6971aa1e6be66811fc8f82d11c"}, "c5fd03f5-cc00-47d8-ba88-1c4574a3f31c": {"doc_hash": "eb2560679a1155ec8b4f8ab2aa988cf73acb5440f1a0f7772a1d2d2672d4e6ac"}, "e19d7283-f1f9-4a4b-8aa2-3edd8c2b6448": {"doc_hash": "a1ef686ad98beb9e88522251ab0e99618be0c4b90649968d0b3cfb4838133de5"}, "b55bbe6f-bb9d-49a8-a54d-bc137e07bfca": {"doc_hash": "5fbcb1c448b993047b1af2bf244dd55cd1054218ad32cfa909f8e8b8d882113d"}, "d2d9ce3b-0ce5-4437-b8f4-e7b2a73f6326": {"doc_hash": "86407d933bcf049528ec0d56a470e95e285618511e916eaf16b8882144ed7bd5"}, "3a57f718-94cc-4f09-9700-3543e95c4e50": {"doc_hash": "467ed2693ce4845dfa98bfe2223b547496d4cacdc8a8dcbd33fc30e853f48a60"}, "cc6917de-9329-4048-9f16-c251e2d1863d": {"doc_hash": "cd8872aa2c84b1b41b96fa0897f4de2e8c181c4863a04a6fcd17cc778dd8d691"}, "25ce4b9f-9800-435a-97aa-6f0425f75594": {"doc_hash": "d18c92d03103be64ede3ca8cb2e9a5165c3943465fd77b3dcabd8e0bc7e5625b"}, "c0a67858-64ea-4182-a21f-07453bc5a663": {"doc_hash": "b93baba5f136c880bef0578cf40600e73b81af2a73fa4931b850bd757889b5e0"}, "5582b301-a846-491a-b4f9-88eb44b26f81": {"doc_hash": "e7b96246c25229aff1db2d4d419e97706abb6f1bcffa3253c9242128071772cf"}, "ec540a68-1ac1-4c09-a70d-d55fcaaf2d6a": {"doc_hash": "df944e2163827d837ab0c6df7b0a46a742b30489423d2a54b2a5d290fefc3a03"}, "ab18cadd-350e-48ee-afdb-a0751b03d4ed": {"doc_hash": "43c5933553bc6ac8e2c4bf5c925c2a9ceb4e4695303e787c6ccb9765f5879079"}, "b2c71e46-5c60-4cf4-974e-891020707f46": {"doc_hash": "73f8ee5f151feec99aef070fcdc97417e9a822236010b60f6feba41563e2f958"}, "a583d714-c337-4b39-9e86-9e626c953529": {"doc_hash": "d922cff99c35303ecf1ceff02525e84ef537304ed73e1de0b37637cf31aa7ded"}, "d2b6cd77-b343-42a4-84bf-b4771a56e23e": {"doc_hash": "6007b3f3f8598e651cb263e09f3e39708172aef0e1ba6ed1ff7f0c8d425f7f68"}, "d71cc488-5d71-4826-a19e-fcde02a901fc": {"doc_hash": "e1ad32ffac32aed7b29f5369a2388a0ff87f9f470a2668be7e00b8eaef00bd9d"}, "7cd4c9bb-1162-47a3-8c0a-5ef8c67050f2": {"doc_hash": "574d70b55d97566500cadd7494b438f8b4ea79f24bfc752aaf90c38c30f2e183"}, "939cc4d6-0010-4590-be52-5307ef2fccd6": {"doc_hash": "2e945b39008eb170e7662febd96c6c6535b35e06004afafb4316746d7f4d9640"}, "68f64ab3-96f3-44c9-b985-f615b75345a7": {"doc_hash": "4a3d0d2ff9fdea0c51a624a4c838ab922ac5e31880795cc7e235ce43f917f7cb"}, "977d2ba1-26ba-4e1f-9ad6-b3e49f26e190": {"doc_hash": "e8c0d1455367276247202b43ac9923aa3863a0ce982d86827b0ce6b14446106a"}, "a5adb3c3-d665-44f5-aeeb-ae061cdab3c2": {"doc_hash": "315a0ac5cdfa9b838a8ee4ba859800d552da1c79dc5abbd9ca2aa2e01ba42ded"}, "68c8eea7-0e8e-4ccd-9c97-bb909a682b72": {"doc_hash": "1bc6d3e6075b6835aef83887377dc1df6e7974ee76ec107da5f80a05a3afec64"}, "388f6cb4-6216-416f-8f29-a4713a0e820a": {"doc_hash": "5e53da8bf9d8742ea9c1f95994f5e790f5a31eb1ca315d76dede01cab4a8cde0"}, "b59e7ddf-4e06-45f8-98fd-1be93facbf52": {"doc_hash": "3f5564a4f41f2dd6fa6a392a141fb8dfadda7c372124415c19b828a08e4c29cb"}, "b96cdbe8-fc7e-4740-89ea-3fb497343439": {"doc_hash": "d193f0eea77dec7cd008603c6de5a6cb997f8ff0504d96875d0a9facd8515d3e"}, "5dafe8e9-9c65-4a5b-88c1-6c01a380db43": {"doc_hash": "c028c8f7ae625353508cc9d4ef22edefb14079eefc52590adbecd7e676c5becb"}, "79e8e835-1128-480a-a8c8-4dbd220db150": {"doc_hash": "291e0fabe8e3e6b5be75eb80de07bdee715cf9a70f5807ac267d78920b1e2f2c"}, "5b5c33d1-cbfc-4552-8761-8277156d2981": {"doc_hash": "fd19e08f983ee44ebc83a50108e45aa5375721e3803b145a7e3f398a7052e781"}, "ded36305-a0ed-4456-9622-353b15d30a01": {"doc_hash": "c95fd847b35771fdcc8484d41a4c76e64ae5fbdc834e850ca3f05b89ce51b99b"}, "bbff4927-6f8a-4717-a7b5-4faca5c3ca75": {"doc_hash": "165fa0b757047d6088ec0349c7979ff60ca3d11c62d1654f24e422ff16361563"}, "202dbead-1d12-4e62-93dc-001ca0f99504": {"doc_hash": "d352910ee73bf7ee1026a2daeb9b99df95eadc5b7aca207e8115524aabfdcf94"}, "bf6128b9-70c3-4494-a4ae-99105d7678e7": {"doc_hash": "20ccbf5d3dd914f01857b46d8fafb5705715b566ae66142d400361f4dd29b9f9"}, "e1df3f0a-8d73-4c0b-8721-17fe836343ba": {"doc_hash": "00968f8ef28aa0dd697be334122ec1349818b1ea03d58dfdfc117130c3e380bb"}, "147d18ed-d32d-443e-82e6-76ab6e61e411": {"doc_hash": "a2baa829439fc75a1aab37db3a8a6c865661cce44d2a5228e1f563c23a106e83"}, "5e29c248-6458-428c-8f85-9c0c1761f29d": {"doc_hash": "cc376b8c44100e262b556067a57b59cfb7f25dc1adf4a1c5e3c955031c799fac"}, "c90174a3-70e5-46d9-a27c-48b681479604": {"doc_hash": "60846caf586e38134940fe025b640101fccf65c0fd13d4b94c07399720f42fa3"}, "8ddc942e-33ac-48a5-a9d3-ec7613333513": {"doc_hash": "9f94c3fbbe70513fa3a314cec5cddf6630a0a2ddc29a247802ba0604add2d0fa"}, "44c59420-9342-4295-9c90-2ca56c95ccbc": {"doc_hash": "2c48533b4ec4ef3f10d1cc91eac1f09da996fd36ac9e1b8681992eada77cacb4"}, "264e76d8-6025-4ff0-a8cb-16c7fbcac64b": {"doc_hash": "14f46dbb82a2daf9e36357eaa6e096d237ad1f8f57fad1e9dc92ae6695489d4b"}, "5d2b6663-bfa5-41b9-b417-66edc8457af8": {"doc_hash": "e7a1a86fa17881649bd6f8cc79415a0d6e89ae375488b378bbae0d2d7146055a", "ref_doc_id": "a733ce94-88ff-4886-9e0b-6d820b3450c3"}, "a6235638-b32c-4ef1-b4f6-b14ed4f57dba": {"doc_hash": "73671477f9498fab5376f6b096a6079d938e90bcf96212401cd884f3ee07e392", "ref_doc_id": "b00d7971-ba19-4e91-80b7-79e173259844"}, "2b05e1a6-2fda-4981-9c3a-fff8e4e9dd3b": {"doc_hash": "13e744f598c61d3a514802fe3847598f4b1d7f89c56f10788dfa24c64882abd8", "ref_doc_id": "f5520389-916a-4f5b-b7ad-5f2704df39ff"}, "5d0a84cd-6cb1-4c76-88f7-ce6cf4887e02": {"doc_hash": "1f1a65f265d05ddc10fa6c18c85d41f974d71bcd8f1c4e4fddd491e2f9da9a70", "ref_doc_id": "a0c0e3ca-96fa-425a-ac47-934a5b00f302"}, "3bf8ad8a-a03a-4bb9-aac9-f3d0e211afd2": {"doc_hash": "6ea61a1e0de80e93423f2aa32f0d128885efdd5de46d1bd83cb8a0851d957866", "ref_doc_id": "9d01a50e-edba-4f36-8e93-0d3b04349de8"}, "23e91978-8728-435a-b1eb-deb923d1b20e": {"doc_hash": "e3d69ddfef6e247b23f932eb9285a2101fa217dfaa985152b0f3bacd42c6077a", "ref_doc_id": "79196d52-ee0a-4677-b3a7-608151870f60"}, "f0761b1f-60b6-47ab-969b-409e541859a3": {"doc_hash": "29aafa5c326de486e3a414acf5d6b48be70c93c65e751fe58ac2f75da8845f84", "ref_doc_id": "8e26a0b2-a45c-44d0-ae18-832336084a45"}, "6a18737f-5ec5-45cb-87d2-baaec0f24708": {"doc_hash": "3edc96a1bd874b74099d3ea64f3a9011b9f641e61a6d551a4bd7057788acc81d", "ref_doc_id": "f0187d12-3c4a-4514-8b06-2b5587c530a2"}, "9fe3054c-a13b-48a5-bf79-a618b8859d4d": {"doc_hash": "343e13ec5d7e70e89b5a925bccbc9d568957522fea0b2726e269f8cfdb8a5ddb", "ref_doc_id": "b8fde317-d3b2-447a-8ca6-1f48a51493e7"}, "fd95fa48-f7c7-491b-9fe6-7fa5a654c47d": {"doc_hash": "878cf6551ee7a9187cbef0d04504a547fa2fceccc5663c9a53301046bac9010f", "ref_doc_id": "7fb7dadb-8891-473a-8a34-8bffc19aa4a1"}, "e69719e5-1b5d-4d59-b54c-3e7a1964bea4": {"doc_hash": "c82bfcf49901f1f98f0d12b7a748cc90ef5ea5e592198c4e6a095903d9d9c93c", "ref_doc_id": "aa235d21-c392-45e9-96aa-9896d9459fed"}, "67208868-b858-48be-8736-7add805b496e": {"doc_hash": "db4ea3b5a3057e797add1990c043ac6377e154b3f6e2895fd45961ae24889f3f", "ref_doc_id": "ea6bd15d-3ce7-4541-a567-133997af210e"}, "3dc6f43c-ee22-42dd-a83f-f5f67c8ff76a": {"doc_hash": "ce885813735fa58a98c9739d0647f65ec4271e8e9c013abbb60c97ff02e8f177", "ref_doc_id": "adc92171-2048-4d45-83a7-78ec6015cbf3"}, "48ff5b0d-e13c-4d5c-801a-a4b6e7ad2abf": {"doc_hash": "3a5c0bc8a474db01d7d1b3d6f15a4b23be8b9f3032127411ba263f6d6d34055a", "ref_doc_id": "5ccb705f-32df-4078-8164-e24e18aba0b3"}, "fb12d6fa-7d97-4c1f-9292-e78f73c8aa72": {"doc_hash": "582e693bcfbd1e2035c3d70379a2871de7608f0e04e954a530f637cf778d8423", "ref_doc_id": "32214cd6-3dc7-47ce-b6b3-a9e381e23ffd"}, "6dddb114-d2c5-4913-adb0-0727f23b2680": {"doc_hash": "64b1355b212687026274a6444502b1cd9ad05026ce4c4e34071024e777e365af", "ref_doc_id": "cebfe9c4-47a4-4b54-89b2-82e9dbcc77bc"}, "ea810547-dc8f-4e4e-a5a1-a18ab5a8b61c": {"doc_hash": "19180c07501ce92ca9743c9a1e322aabc1d4496e27bd2f0fc494fea66a0b90b0", "ref_doc_id": "2a02ca36-b912-4254-a375-2d5a9381b4e2"}, "ec4da695-7c10-403f-a985-b884c88307cf": {"doc_hash": "6e4dbfb753bfba2b80e451551f591b488b9ae124d6d0586a38cfa15a3af6c663", "ref_doc_id": "b9074f0b-0481-4945-83bd-37f0f50ca259"}, "3ecacc0f-5b65-4cf9-9d6f-11a60ff2de42": {"doc_hash": "4ddc543d847af5bd907fa78253590332b1d480ca63aee18e90e1273e114ae954", "ref_doc_id": "9e931095-a299-4167-be3b-24230692d612"}, "756dddd6-c3c7-458e-bf6e-8b72f89e10f9": {"doc_hash": "066b6a12174a5f7ca47e34502bdb5e0dff0eb5cc9a9d56ecf941e5bb86b03595", "ref_doc_id": "b594d96c-ee54-472f-a677-93acc0f02801"}, "33f73657-1e4c-4b7f-8322-883943963d96": {"doc_hash": "405861a72c46c7e39028b1616597d925e7625d795ea7d20206df8da6d04d97e3", "ref_doc_id": "ecbbc8a8-4438-49bb-b4a6-d0d04a57121d"}, "a2d96439-173d-43b9-a7a3-e8ce4d71144b": {"doc_hash": "4b29d3cee53bb10d2b9afde240cd9efa17815d1b71f4aee8eaa48963cae7a417", "ref_doc_id": "c9a88c71-575c-4fec-bf08-a8c33c93ccd1"}, "a5465e7b-a35b-4da9-b5ca-a66f36b5f1e4": {"doc_hash": "7ac6d303907196bf0f9cb23ac629836bd81329551cbcb421978d9215b044da44", "ref_doc_id": "5b8bf068-4a8d-4980-bd1a-23f5a9e5c6af"}, "3f920df6-f97d-416c-9f7b-1b278a02c490": {"doc_hash": "7aea88bad7b794b69b33174a94017291aa0ccf18050a12cd401ef83920e6fcae", "ref_doc_id": "7110e9e1-dbc6-4d81-b637-7b742604fac3"}, "d1e67f00-0ece-4632-bcc1-d26e71984054": {"doc_hash": "37a3b638714866c5f41f3bde5d2dd562fe988082ea67e3b86719f2ba2fe2ac86", "ref_doc_id": "1606dffe-c3f2-4317-9b82-d8650be2c097"}, "367a84c9-208b-488a-9988-638b80c5a0ad": {"doc_hash": "93f9eb07413e7dadca4d313dbd48524c24093f7ee3c22fbdd6898738bb1186b8", "ref_doc_id": "57f4a40d-606e-46ed-9fb9-f5809e036e03"}, "ec526738-0282-41cf-a0de-313334a93d70": {"doc_hash": "4fe33e79cd386875b1ecc679195b171c91acacb8d747270ae28ee0b845061ee2", "ref_doc_id": "27a84d3b-52ca-4584-b2f5-884794e5b582"}, "392d3bae-4b0f-4304-84e6-f5173ab19f0f": {"doc_hash": "9dd5c84b3fad8a7f92198bbee6b1afe888344f30a388e43aa9985b4e180a14fb", "ref_doc_id": "618328f2-b119-4312-a059-9b30454bfda0"}, "38562437-e31e-4e7b-bfbb-6e2682e30ee9": {"doc_hash": "7432136ef33ac31a21ac93b0de99d3d4e245b08994769148c075d80619c076f0", "ref_doc_id": "0c9304bc-f643-46e2-9836-15d41e33e71d"}, "83655f7e-595a-4372-80f8-64f3a84cc172": {"doc_hash": "08eda3661e81aa2a1948471f9c379e7543baa214d091940af39b34abf0a9f0d6", "ref_doc_id": "fb4d764d-4c1a-4708-8793-ab32c615f94a"}, "ceef73dc-f07e-4c71-b419-444d35e5e383": {"doc_hash": "b3bae785e1175101a4397edb232d035304adcf761e0ec618f575d74f639f4972", "ref_doc_id": "3fb8747b-1767-4289-94bb-077baed77c24"}, "068d4911-a197-4c66-83a0-c353e89fdebf": {"doc_hash": "bc1cf442ae2400b48f07f0e21c795cb5107618871e95f0a44ebd0e80f52d5a09", "ref_doc_id": "44e31ce9-c501-4c3d-8663-239a6a9aeebb"}, "c13691d8-0a4f-458a-bb29-a03df2027d5a": {"doc_hash": "57573fa1b3640fdc88231f0b39869b6c54e362450c578c3ddeba159889534551", "ref_doc_id": "de1ce3c1-eb1c-4fd8-b5e4-8a0f8e85f417"}, "8c366fd1-8c9b-4938-8638-c456f5fcca6a": {"doc_hash": "68414d8de291f79da7b5ff8800860a90ab0b866ee710878e13b2b66e3bcc6d6a", "ref_doc_id": "1ae2ac07-2489-4f80-84d2-a7304d412185"}, "8fdec053-3cc2-4fae-86a1-028cd2b4dc8e": {"doc_hash": "fb78d478f693c3da14c6cb6d17fbe4f6441b3796e6f2e86127b20ee5c765db11", "ref_doc_id": "e55948c6-d85c-420c-b00f-28fb5ca2c259"}, "0edef6de-a2c6-4b04-8719-c35191181190": {"doc_hash": "f841a4698151a8b920f557b386a96f535eba0ea8737846f44cb5872292a33d40", "ref_doc_id": "538fe6aa-ce33-4c43-ab73-74f921399f3d"}, "9e04ebb0-a7bb-46da-9e77-ab856d3142db": {"doc_hash": "d13ab30f5aeec27ffb96f1924e8530b34eaa94c3c0ea2fd6429111eba170cecb", "ref_doc_id": "ef50c065-26ed-4c35-adc7-a4396ccb7ef4"}, "6beef081-ee2a-4e8a-81f1-0e597a7c2668": {"doc_hash": "f50ff39877dc215325900ebdf7e63704025d04f99514a5ab8e79dec2450341c3", "ref_doc_id": "ccfc5bef-2c66-49ea-8648-74470273d1d1"}, "042925c9-6d16-4844-a556-030f20972bf3": {"doc_hash": "33f6a8c0cff981d8919138342e1e7412bd3b1c4fa215002e9f14f4ec54d89517", "ref_doc_id": "78a62381-83b1-4285-82d2-7560cfc42b6d"}, "90167495-0368-42a9-8fdf-c74274f2a9e4": {"doc_hash": "5d2bac389351521c9878edf085b878d40971230bb4996a941c178243d86facb9", "ref_doc_id": "db699eaf-ef94-463f-a91f-cef0e7bec50b"}, "9726bc30-5961-431d-b58a-16f0023b6e76": {"doc_hash": "0d21b44aeb2eb8d555409c382041a63ac174615f5fddacda8c1ff7f261b8d8e1", "ref_doc_id": "912a9072-7a67-4441-8ede-0931561f03de"}, "8175030f-2acd-4a52-8dee-aa8dfc5954bb": {"doc_hash": "aa093e46891cde3479cbf6063a9f201e0836a137cc6e6ad7e975ed22a96a58ea", "ref_doc_id": "6b7a554c-7339-420d-bc4a-9851472bd4ab"}, "ebc7e6f9-42dd-4c4f-8b97-c6aa52e92a18": {"doc_hash": "058ab97bccab7f348001cb8269abad981f33f0e7660d48bd72ee9fe47c3306b6", "ref_doc_id": "20f5fae9-e9ca-4a02-92c6-957811cc5480"}, "5f07d091-422a-470f-a382-75a192f7b20b": {"doc_hash": "6e4355631f06d5d362dea81e4cde75ed06d3c0e7c55ad4ad40a6b149188a3fdd", "ref_doc_id": "17a26717-b9ce-4080-93e4-d6d7b8327b1a"}, "00b183b0-f5f8-4274-9c1c-7231d917af0c": {"doc_hash": "0df4a628a53f152bd778094f92fd2ef9db47f63e97b071fda8fe6a38122add37", "ref_doc_id": "4431a0b2-ff50-474a-87be-251ac8adefdd"}, "0e148fd4-0955-4a37-8a25-8c8ce265aaf2": {"doc_hash": "8ddc62bcc1fec22c408643f9c7387d777e17b14402d36f1ebcd6a1eb2335397b", "ref_doc_id": "08c578ed-4907-4a25-9475-a0c0946a2e29"}, "f730207f-749d-477b-af7a-3a81cc7c21bf": {"doc_hash": "3435f694b5759ba5ec1a00f79614d14d297d24557999cafb8dac7fc121aa0206", "ref_doc_id": "89d38fdf-8098-4191-8d54-80f5a9088055"}, "bf05d258-264e-4544-aaff-749b1390b51d": {"doc_hash": "d5072ef98071ded4d8759a62aafbab734332d6ac5b52d67d151e9241a93ee723", "ref_doc_id": "22e42d19-9c1f-4972-bdf6-a919f6adb096"}, "64b973f3-b332-4da1-b068-945986a9e3b6": {"doc_hash": "f96d323405a7cb8e155f63ed461de31584713941978430dfb92bc1e5b5481d2a", "ref_doc_id": "73f1c556-d756-498c-9334-4f18bb8743fd"}, "58beacb6-5cdf-47c8-b231-9a646b86ebd4": {"doc_hash": "3fe997a64c2e205e488cf31cf8c72a20f7d46a2422fea7a0f6db204e4373e039", "ref_doc_id": "7f7fefea-5060-47f4-b742-7dcd757dc6ce"}, "188baac2-b610-401a-aae4-c3b009c292ec": {"doc_hash": "5140372eb7aceb43dd0f58617407659c52b33f0c905c31962bb0558e3e6a7b5c", "ref_doc_id": "420c7222-1f01-4417-be2e-acbe12b5f0b6"}, "615c8611-8c13-4c60-be41-077e144055e2": {"doc_hash": "51e7fc86033691630828673f8a2181444cc49d5be51a6e99691c3d92f24990ab", "ref_doc_id": "d55814fa-8638-4141-8418-bfeb18d7ec28"}, "38f88f33-7cc5-456e-93c8-37d1881ff767": {"doc_hash": "146907d1c3e7985720b17485a0b6c8038f2eb3e063bf37ecb64e58833e4b7794", "ref_doc_id": "508ab91a-2f2e-4a50-a8b9-7c203c9c9e11"}, "f3b7b744-3f39-46c1-8c89-9109bbed095b": {"doc_hash": "adfa8a6608c056fb55be4a9481e70c9095e5c3ad551e62ca3afa11b606b08190", "ref_doc_id": "691f3498-b4b5-471b-9f41-1cdc3e0f13fd"}, "c03dcf62-dfa5-42cc-98a3-b4f31122381c": {"doc_hash": "4d26588b587fe64b37f8b6717abebccc26fb1d7aae27fa10dd27947cabc474e2", "ref_doc_id": "1a7ec057-f8cc-49aa-ae5c-5aecd2755502"}, "2d4e0e90-6ce3-4474-84a8-8831602a5859": {"doc_hash": "ca037ef74dadc8df4306126e688b5357c1f8c0c817912b4f15e368200cdd88f9", "ref_doc_id": "07c63003-cb6b-4d7d-88cd-8c8d85dd6a13"}, "52b8265f-1227-453b-9d58-fcd43121ef5f": {"doc_hash": "fdfc8b995ff2f8608509ef9c1d7ed74bdaf4bc00851bf6582c45704ecb018aaa", "ref_doc_id": "43fae96a-eeea-4793-b587-dcfa3bd5d161"}, "8040c4aa-f781-4c72-82bc-7c0a91f57eb0": {"doc_hash": "3fdd5a6edd45de386c69a49948018976834099513468942955abc4792551e0b8", "ref_doc_id": "7f2de0fd-1545-464f-af39-77c2b309ef45"}, "d9ccbe0f-301e-479d-b543-2d6efc573db3": {"doc_hash": "a0ab57c87c44edc3b0152b21e6b0c9fe4ba73304dce4b752c41bdbed934ea4ec", "ref_doc_id": "0e2e1c9f-ba94-4652-8c35-1a29af359be3"}, "6ce5e8d4-78c2-417a-afa0-2001e6c46de6": {"doc_hash": "7c146c7c26dda378e3c0bcca334c3a99dfa76c018bdbdb426c850422b95905fe", "ref_doc_id": "93f00707-9930-46d8-9f06-0fe4c663b71e"}, "30731fcb-d003-4657-b818-4b4fb419f461": {"doc_hash": "6cdaece6f7f6ab438b4bd201565f4299a96d192d3b0bf892dd5c59ad8bc9d75a", "ref_doc_id": "14fbac6c-813b-4aa2-9bdd-bd28b6bf26de"}, "77c56b04-ca29-4acc-939d-d5d45197091c": {"doc_hash": "014ce219081eb51c0b2d11a0a876e8ab58f9806b7177714cd3624a2124334608", "ref_doc_id": "ad700b0f-aa45-43ae-83a2-be28190729d3"}, "1f0517d6-21a0-4d6c-8925-be1034d8a24f": {"doc_hash": "29ac417f17a7a43bad453763cb8da563af8fdcd09780f5e0ab874f273ab033ac", "ref_doc_id": "0f5ad8b7-1624-4629-ace6-bcacee58fd7a"}, "1335defa-aa12-4033-aeea-4a2305b37ce9": {"doc_hash": "a29f59f115efe6876017ac411521502b617fc54d4f5fb1650232da9ca8b0dffb", "ref_doc_id": "d573fc26-40e4-4053-8a73-065d4e7195a2"}, "e922d88a-e3d3-497b-a825-404210e20d1c": {"doc_hash": "fdb20bbc22cd6ce1620d64507e54e905c7f2d592651c90936054162b8e7c441b", "ref_doc_id": "37f9b070-a014-4040-a651-7bdd7ceead4d"}, "418eb099-3ef3-44c1-bf97-c592e09e8579": {"doc_hash": "ca4f30487015f846bc9097e11b121e22206bc431448d5b7bd278131b1369120b", "ref_doc_id": "d8abe616-ae9a-45ee-9812-de5e3ec23f53"}, "c54409ad-989e-49fd-ad89-ed4570c0a31d": {"doc_hash": "d71ec9f75c36f3f1904b4814de7abeb029392ca66a0ef75c469f942af535f0c2", "ref_doc_id": "b20fb21f-1865-4b97-9360-ad5e7fe7887c"}, "ca00c42d-0770-48b7-ae2b-4f1c781e8d4c": {"doc_hash": "351d669d046d8e0c51b2b6de96c3789f65258da41eec088d7a89a8c10d771f2f", "ref_doc_id": "cc8b1baa-3cfc-46c7-8ce5-2d2b3346356c"}, "f2dceb09-3e69-472c-bc53-e826db2be258": {"doc_hash": "68b45a08e549d6abdf87cb3244657b26eb47a186910bbf1ff80d9d4911cf1d78", "ref_doc_id": "503f0a38-5972-4d79-95ff-23d521f6cb1c"}, "71f5e318-a189-4fde-b39f-f947426ac059": {"doc_hash": "1cdaedf4f44525b13a9c8533f355a24e0d948211244b9df8cd9de812ef07bcba", "ref_doc_id": "5c423d2f-9155-4291-b662-d574bf78b36a"}, "63544937-a439-4096-8802-1ffefcfa1d9d": {"doc_hash": "83a97e5d448015a6b7a26739d8fddfa1022df86a7eff30320d5af6e4d3c0d131", "ref_doc_id": "120f2c2a-99fd-417b-999d-3d57d3f141e1"}, "edc709b9-24eb-40c5-856a-7a2346821c35": {"doc_hash": "1b8b2ce474966d23b89de7006b9e361a0f2e12db65e0867255ea01e21cff21f3", "ref_doc_id": "ac24a6f5-5d32-40b9-aa87-38cc1e8388b4"}, "30a4a5ec-6abd-40f1-9879-59d193f41e51": {"doc_hash": "23912364b76ba0cf33a42e856b371089881fab64ee0d98eada92542844bff602", "ref_doc_id": "a7137453-c90a-419f-9667-aeb74d5b922d"}, "591444bc-4af0-4aa7-af3a-b37dbdc7754f": {"doc_hash": "e02e7031c9b17bb2ba0e51962e47a5d67fb5e8260165b405da4538ecf40947ac", "ref_doc_id": "1f5b1576-4cb3-49fb-9f7a-b0f115c61c00"}, "c24476fa-86b9-4ccd-a4d5-bbdfdc4d0891": {"doc_hash": "a97a85d11f10b01f5037cbf514fa27ea085768dee3f8c577d3eaf82c52da6677", "ref_doc_id": "1f9fcb0b-9b01-4bcc-87f3-1f874bdd3beb"}, "84f1bbea-95d9-47f7-894b-816ec83880f2": {"doc_hash": "62c881111158318e5d6b4761bb6bd6dee77f883f166cdc94773417b485983c12", "ref_doc_id": "a3b26254-6fc9-4f1d-acb5-e0955092485a"}, "41715f74-6bb0-4894-bccc-873c3fbe9d05": {"doc_hash": "434312f2c951b328badf73ef778cb7330ccecbe2718b5e07bc54f04a8f313c3d", "ref_doc_id": "483d1b09-d12b-4cb5-89b0-41060cf45f71"}, "7c6ea3be-5f06-470c-8076-c639723823ce": {"doc_hash": "e36a354a1f18d5a3ca7e563876789be96ab9f82480300ea5bac22e7559673ba6", "ref_doc_id": "d3328c64-b8ea-4e21-a342-3f888c306283"}, "a7c1653b-bbfb-4578-ade0-c8eeabdbce73": {"doc_hash": "008c316c903f2646eced19b591b36f625fcf305ec90b247a30656461af9c7e7b", "ref_doc_id": "6187b8ad-6bbc-485a-9982-605a29a083ff"}, "901fc56b-8dc7-4027-a250-4c8eed419dbd": {"doc_hash": "faaa6981fad6a97e51d7a49f089f94f07053740153234611e8ed328a2ffc073b", "ref_doc_id": "53787251-c893-45bc-9459-e85fed273463"}, "43942304-f46d-4def-a0e0-30461f666486": {"doc_hash": "793368b7caf439e47ea7966881b803ca362f5631094a4226db4b782ff3d0579d", "ref_doc_id": "e5a35427-a7b4-46a1-8687-c0191f6a1bb2"}, "dd3500c1-9674-4ef5-a9da-1c73a7db2d83": {"doc_hash": "bdd6a55f60eac2632437ecad2e7333f662358a349e37a4375c2906eb6a7ac266", "ref_doc_id": "d2d99ca6-f126-44d4-9b71-596df4d5d36c"}, "844227b3-6eee-4736-81e7-b501de1c62ff": {"doc_hash": "0e1bb677c861b3bd4d50ce5d93c30acf5dbc8bdc916e434c580b6e2857514113", "ref_doc_id": "b3cc1582-c7b3-48e2-92e0-663037e43711"}, "786a2024-7669-4e46-885d-e176b9c1f52c": {"doc_hash": "8d1fe2c1fc7115ca2d84bcc84fd57a973133ffafefefc614cbd864c0bf7381bb", "ref_doc_id": "9d2ee27e-3301-4b04-bc24-070959617957"}, "3c4e4c71-d3d4-40fc-8716-0ccf19496c08": {"doc_hash": "1df664522762ce0ea1858a6317685c66eef9af8b114c992ef8e1601ccc0327e5", "ref_doc_id": "b43d78fb-9919-4b37-80fb-47199f52e96a"}, "b9069655-a8af-4294-821b-9ebab8a39a7b": {"doc_hash": "c4384b9088001a0af35e46415ff8e907fae8890f82a87248997079cf41dd24e0", "ref_doc_id": "a5d69d17-b9cc-4386-848f-0d5ce4bdc6b6"}, "61667a92-ebad-442d-a1ee-ece1402be1b3": {"doc_hash": "a7011218942616ea6aaa98bf47aad7f9bdf58a2392b728524c61da841660b8aa", "ref_doc_id": "0d2d79df-746d-497f-8113-2f8762d21247"}, "9fd86b9e-3224-4bdb-8d17-eb43ab2104a7": {"doc_hash": "8eb9061aa8cf00ef1551ac936aa7ca0ef4793b83bb979fdabc6bbca1f3d6d825", "ref_doc_id": "18c40c4a-dcba-41f1-ad88-ad6af3c275b9"}, "d6a1accf-9f0e-48f5-a6f8-7e6752763ef4": {"doc_hash": "1f7b4bc85fe03b38e5a6dea9177262ac1ca46ac9a8552de50276896fafe9facc", "ref_doc_id": "c68b55c3-28af-4588-90f3-ef2a438c3974"}, "a53baff8-cd4e-4f34-a3ed-3ff460a6119d": {"doc_hash": "127d50b5e74636131e3825be2ecdfd0c0fdb206e6e66c4f4994e3391b32a6f99", "ref_doc_id": "cc362518-dce9-434d-be8c-82259f3d8849"}, "11a24237-4ed1-40bb-ba44-3f060ed53019": {"doc_hash": "9109456164d3780fa88233d800afcdefb079d6d3765ab715aa2611b8d73b245c", "ref_doc_id": "bbf3bb87-bb52-431b-8664-2f7fd39c79db"}, "c1932a9a-8359-441a-820f-c5b90cc71bbb": {"doc_hash": "b9bd84d4020e9a94391b701e10556d087ec703d55ee240342e5caa773e67810b", "ref_doc_id": "1eda91c2-f7e0-4241-aaf5-4cbbf5f70d56"}, "69f3e43a-1c26-49d0-a744-1c213352d812": {"doc_hash": "01a0cf80e5122deecd1e5d11c63b32f95a3a5437b8526aaf31b90fb01ca6f30e", "ref_doc_id": "b480a476-3b85-4054-ab87-6f772877103a"}, "d1b88e21-fad4-40e1-a393-53890dcb9995": {"doc_hash": "7684f0aa58cd1652e0b23231b87e3e2228b902ef9789dacb65189272a588145c", "ref_doc_id": "519557be-c726-43f5-9451-55eb94154174"}, "bf3677e0-c9ea-4344-9851-7530f2a73a9a": {"doc_hash": "12def7580d689c65937622b9fd6eacc0cea0077adcd123ba2b4a7c4092933c14", "ref_doc_id": "0ac2473f-4d16-4a8e-8c20-a65723025631"}, "e0717bf6-5865-4a1c-95c0-458d929dcc6d": {"doc_hash": "aafeed950fa3f2b24dcbb838d3dffa3022c8cfd0acc18597b6bd4b3b845a28a7", "ref_doc_id": "ae36a23c-fce4-4d72-9dab-e9de1f88bd75"}, "b726a81b-1aae-4f8f-9f23-1b1d81896f12": {"doc_hash": "c78aff38f5b5457de94ba0cfe8cf8c5c2fd00b590336008f4eb5fa89dbf3d96a", "ref_doc_id": "e791a38f-9916-47d0-806c-cdcc8ac25756"}, "5f7b4cea-f295-457d-943e-29b7a4e589cf": {"doc_hash": "023b78188e7e0ca992258b58a397a4a4d197953891ab1cc4091569d5ba5ec0fe", "ref_doc_id": "2f41d404-ece0-47fb-9dc4-af44212a0394"}, "e44431c4-52e8-4f72-91cc-2b7a5af611b6": {"doc_hash": "3749014f519ce399c2e61ebdbe6925b8cd440457f7da7c2c1cd5a90148c8a374", "ref_doc_id": "8bc41016-0ed5-46eb-a0ba-538cafc881c2"}, "1af41e5a-afbd-486a-9c77-f61c4187cbbe": {"doc_hash": "67c180976935f203dd2f6b9c96303670207e5bbbdf87ca269b1825a46e10f3c3", "ref_doc_id": "dd73e4dd-2354-4d4a-bf6a-b8ab18138c44"}, "bc0d0604-ead4-40d1-a9c9-bb61c3a70551": {"doc_hash": "0c243652637cfd4130e4f1b2098f6d1370f9d6d8e0c181be3416dfd6a7c0ea35", "ref_doc_id": "97c90058-8c5f-41b1-b0b3-4689824bd4b5"}, "9b2ab42e-2b31-4d8b-90e2-fd5a7eb5c847": {"doc_hash": "198dc7ea2026ea8962bc4b8c1a6b3f3f7eee554ab82d8c0cb3f1b58fc7d9a8a8", "ref_doc_id": "ccd05586-212f-4737-b792-275a5a88886f"}, "68640ba3-5b21-4f9d-8bbc-70f4cb0aea44": {"doc_hash": "63a35ea7465caaa26972673d6c88f96a83f193edee8900fb331d8e55a7eebf67", "ref_doc_id": "6b6d9dc6-154a-4247-9bd6-cf4b482b002f"}, "0683b035-25be-412d-b37e-b721ebfaa012": {"doc_hash": "9e42357b6d5bc8fe2fc0d1d11e85d87f2408a71670aa6dcc266f8c2893b72364", "ref_doc_id": "8bc124b2-cbb0-4133-9bbd-d964a0304bd0"}, "c438f145-78c5-45b0-8a87-80151b5e1c0d": {"doc_hash": "fb5564c8b162a63d20e0236a44ff13b739c72027ae7d27ad9299557f1fbea7c4", "ref_doc_id": "5df8b2c7-12a6-4784-a1a3-d954ab6c2ac7"}, "28fd5933-2014-4bff-830e-3185e3e03cc7": {"doc_hash": "61beace3ae7f1b04c68c47a62818b842c8823c03947ebe263590ebde7ab98aa5", "ref_doc_id": "64466d9f-fd2e-4f5e-b572-c45223a0592c"}, "a34e35d0-1275-4cc5-8849-9ac9f5eec6c5": {"doc_hash": "133607311fd2a529b8e22ade75a3a0af607478057e26473829c710450140fb62", "ref_doc_id": "034d36c7-152e-4f6a-afef-dc4ce449c2d3"}, "5b6bbf0d-b5d1-4eaf-82de-b440c8de0ad5": {"doc_hash": "607e4a47f1a3644f0d4a724ea3b6b42216689442ad5bf56fd125b56302a4205f", "ref_doc_id": "4f5c7ad8-94b4-467a-85d5-7168af917740"}, "1df25d21-5bb4-42a9-9b25-ff87ef0f2dde": {"doc_hash": "2a327f3cafc0e2f754df5c0b0fc39bd6ee6583fd12cc672f97ca3c6a3b5bf2c2", "ref_doc_id": "3d88415e-8318-4ddb-b469-dc9ff55175d4"}, "7e0ae116-320d-4bac-a6db-269d8a9b8eac": {"doc_hash": "ffcde57ac94f60694c5fe90d475136ac0fbf7a3636ab243d9717bdb837a3453a", "ref_doc_id": "77b98518-12d7-458e-acf9-a9b44ee63255"}, "cf260c36-2e62-490f-a348-cd6b6a705735": {"doc_hash": "cee155230387dbbc1674abba3570b052e9016375bfddaa786ecdf58ff6f94936", "ref_doc_id": "09b082f7-50be-4097-a775-7ff5b1174ca4"}, "43c06e1b-66b7-4e1d-a6d3-bdb79b299565": {"doc_hash": "3be02212892bffa221a2d712417591ac1f58a495f3d812b466d6e4f809678e9f", "ref_doc_id": "a2e2b7ec-fa03-4daf-bb58-038b18c00c08"}, "7f362c69-2ce7-4699-8cdd-cc2d1e39b77b": {"doc_hash": "5bb0a2cd53d23fe21610af9873602fe3a89b7448f64f91a88e8a402049d19d58", "ref_doc_id": "8d1609f8-c946-4208-ad59-6d85e89286b1"}, "bf950538-c27d-4a43-b571-3725bcba7b16": {"doc_hash": "24db5d228e480b94ac601a08fea2e1bafee0e71b5a2c237994d5aa440644238f", "ref_doc_id": "78d6d00c-3aba-4352-8a81-297fe1e48080"}, "d18a704c-b7f6-4ac9-9c8c-9efbcbd87e32": {"doc_hash": "bc7826ced401fea15e68ad80e86023c35d91c7530f3c7b60d5357b8c3bfcbb69", "ref_doc_id": "2ef0aaf4-f004-4d9d-b4a4-4a699e880b0c"}, "3260cbd3-00a1-4e35-b98e-8a69a61dc99f": {"doc_hash": "1c358dc2250b7ff65298596af10b42c7173ca990dd1f6388cdfae76d0efe9054", "ref_doc_id": "43462acc-a4e5-4aa4-9726-7f2cdca2ea0b"}, "743616f4-5b01-4dde-9dd7-96cb495b478d": {"doc_hash": "06359a7685ae54672811a5805a956bd156f80719282d5992dbd878dd9d72c082", "ref_doc_id": "38ddfa0d-00e4-4f26-8860-1efe4b85eb91"}, "53e52ceb-d20e-4cd3-b675-21823768669d": {"doc_hash": "afb026b23f1e1fc66ef45dc1f6162e9c77e7692ebf0469d9a1e2e62445b2ec9e", "ref_doc_id": "11567799-1cda-4c36-a5b1-7e53d7a7a5bd"}, "a42142e9-583a-4cd2-9596-9c8d9863ea98": {"doc_hash": "0bdd6ed34f747aadc1c6d862484a5d73d37c634847dac68d32b9fcce78aec7f5", "ref_doc_id": "2400f6aa-f12f-47ab-9ec0-ba379edd013c"}, "d4a3eba8-9828-42b7-bb04-feec0b4de226": {"doc_hash": "1fdc90371b1a3d0b5d9d1549084124558e64ac702b954c96a29eabcb0f832bab", "ref_doc_id": "ed01398f-a4d6-4179-9219-161d9510fc80"}, "b8789a67-460d-47a6-9fc6-02c0db3428ad": {"doc_hash": "c6276d62a62b7ff1494458a8afdd6049bdeef124f23e5d993c97db57149bd679", "ref_doc_id": "1a7dd1f0-b8bb-48c8-9830-397c055d758f"}, "292e71ce-f384-46e4-bf32-78ba5bbc6d47": {"doc_hash": "175a9918456fd30f0bd29e1a84e9f56095a07d8f75b70aba930dbc339f96fbcb", "ref_doc_id": "e0e6b4e5-595e-4ea1-add2-34c110f5c71c"}, "c58c5f80-1598-4aa2-976c-4bfdaffa4498": {"doc_hash": "27aad9a903f49c9ac48b5774794629e26af104091776da85b505b5fedbd5e742", "ref_doc_id": "0ed0ff19-5549-4533-95b0-0277a217e31b"}, "445e167a-4689-4331-a409-1e370b452599": {"doc_hash": "f53f91506d9d9a7a3033f6fca08e084f7e181cfef2f71b0f5f509eb9553aa4c5", "ref_doc_id": "89baa835-38a8-4904-87d5-102bcb0e7361"}, "60de9efb-6aa1-4b4e-9756-6dad713abea2": {"doc_hash": "8e81c8f220fee50a460bd26cc67b42521185dce3be52a8d0a3a45b5f02bdeb73", "ref_doc_id": "6b31014a-eef2-41c3-9719-6f34d12a09e2"}, "4e3e983b-ea5d-4cfe-a39c-3c2a214cfd76": {"doc_hash": "c50954a046957e1b982eb6ac4da07daf58a54d90a5e7e2ffa3cfc2e49fe65cd1", "ref_doc_id": "6ca2c4c3-5820-4c8b-bbb7-1ab56dc84eb1"}, "3e72652f-02a6-43d9-bb43-49bc513ba4f4": {"doc_hash": "46082bd9b1379ba1c5d857272bd358996d5ebf14758d4a2591dbae163b166486", "ref_doc_id": "cedd2731-34a2-494a-90c5-1bc11c9b9fb9"}, "7bf7ccc9-7c8e-4621-9344-a672aec1be2c": {"doc_hash": "5a5751428d31899286deda0ddb14c277a3228f1b01feab4c42c053dc97ed3618", "ref_doc_id": "c3c6d656-037e-4bd2-9e97-1df1c58225e1"}, "f82ed1b4-935a-4ffb-a3f3-5306607decba": {"doc_hash": "f8554bba415885ffef6c0f00dc69ea06eb6727637b4d90236032d06435d501ae", "ref_doc_id": "3b09d4df-7047-4119-8caf-61645c291808"}, "e8ada223-9db8-40e7-91e9-2eae49f34c17": {"doc_hash": "bf4c39377ada6a356218e9bf1d98cc923c38a42e0afcf0cf6cb39fc71f3bfcda", "ref_doc_id": "1d321286-2cbe-46fd-ac0b-5b8f5d721643"}, "c2154083-8137-4184-9cbd-d8cda3dc56a0": {"doc_hash": "e85d5e84133dd2ead40bfa0ed4562c50d14b1d30fab439e25ec37045211c0bfa", "ref_doc_id": "768217be-95f5-4827-93d1-0fce03320ca7"}, "90a6c7e9-4b00-4e51-9c4d-512bf5d639b6": {"doc_hash": "679ee7c3bc6754565b9a6a44e287c2cb6c79a175b5b63fb0831e5e6dcdfaefc1", "ref_doc_id": "1070558e-1571-4897-840b-14d0f7aaf36c"}, "1c7707de-f394-4264-aa7a-2d470ee00f17": {"doc_hash": "cdf9c36d79df7d2cc6d9d3b6ef221b111a4e998c2973ddcea705839a840bdd93", "ref_doc_id": "8e123543-6967-4d54-b0bc-43260d86f3ee"}, "aeffc19e-b955-4967-ae15-f84aaecd67ca": {"doc_hash": "924cc378817227a702b5dd884855640c3798f1a73c14d49008b32fd562f9c524", "ref_doc_id": "d84de500-3e2b-4d48-a8eb-4043be4cb958"}, "0d475db3-41dc-4bad-b9d7-ac51a7a43026": {"doc_hash": "42fe76802e7497dde219ad370e9d7fb4c1ed836ea3228bdd37be227e42de996e", "ref_doc_id": "f196f29c-9898-42c6-a88b-bf11d66c381b"}, "0cf1b229-5b3d-442b-93ed-7bfb11cc2aab": {"doc_hash": "e51dd606f0b307a0b360e736000b88221f7d3b05d56c796d745bb2e492c4b171", "ref_doc_id": "bf8c9660-008d-4b09-84fe-d3503830c223"}, "7c393514-7a78-4fa6-b116-e710527cb20a": {"doc_hash": "017d483f4b146429c418a9f6ed7551228a25d423b3d390965c0ec28ad82d2749", "ref_doc_id": "d8ae9318-28d9-45d5-8281-af39698cf6c1"}, "cb5cac62-8a8e-4b0f-84cb-602f79eae657": {"doc_hash": "a3dd6208e5df66c0c644a8394b1b32674d13754c2142d86d18a3ae5b6d9ed4d8", "ref_doc_id": "ef20831b-1347-4943-88b0-20c8feaeb95b"}, "5aa29ed7-fa53-4131-b69d-02503a6d9968": {"doc_hash": "17ce7b30afcc56be286e65bcbc67f484d4fba478f2403cb462052290b8ae7910", "ref_doc_id": "2f0920b8-0f84-45b1-bf24-f9c08140496c"}, "ee0c215b-8152-48fb-b5ad-4d7f0c56fe7c": {"doc_hash": "dcbcd861d026564151fa5cfc826c5ce5921209d87f067f4b04a37cdc643ffb6a", "ref_doc_id": "1d2987ff-b8b2-43de-8580-453697c32c65"}, "7145d89d-b33d-47af-a6a6-6a8b5d9435e8": {"doc_hash": "ab5c1418291278b24bd79af61f0c54554472721691385caa0dcdf9a90c8ab243", "ref_doc_id": "6ee05085-cccc-4bae-b76a-17e5e282cf37"}, "69f1c7b4-8fe1-4035-8ce5-5897e3bcf694": {"doc_hash": "e574982e96a7f4a38c7a302baa38171c149f2b6a982448158dd3c39bf2d2e077", "ref_doc_id": "6ee05085-cccc-4bae-b76a-17e5e282cf37"}, "c5a452c5-b0f9-4ea2-9953-8d5cf5f8c1ed": {"doc_hash": "c351d080cc1cddc18180da3d874d3dd0ff923ecb68c567586b6db01c1c288dcf", "ref_doc_id": "828605ba-5131-4e7e-8acb-6f58ee4f9e30"}, "762461c3-1ef7-4dfc-84fa-600fe234ec74": {"doc_hash": "e9d4b5fd451934ba76a74d4df24eeca1fc6b69f06857a40c98ee708d02c9ec23", "ref_doc_id": "d08dfede-3afb-4489-822a-7baa362c62b7"}, "4288bf7b-dcc8-40c8-b1de-d8b47c34c282": {"doc_hash": "a2185f761f0eb1c811c1fdcc5701621b0ef13f375a4fc4faee6dd5153518b1ca", "ref_doc_id": "1b7c521c-b322-4871-b154-cb009d64055d"}, "a2ecec2c-ffa6-4aaf-b106-5dff918770d4": {"doc_hash": "044ff3b2cbddda992164dbf9361c63a061fca63897fc05965b28dbf265f42020", "ref_doc_id": "cb6712de-db0a-4ce5-b7af-044b7e734d8d"}, "dcd8b469-9fb2-481d-978d-575e744d615c": {"doc_hash": "9a709c5ef5cf159898e161d4846844281a7a26aa50081cc318591fbc46d4154c", "ref_doc_id": "4a2633dc-9e5e-4263-bc05-80db13c174d2"}, "498666a1-d7b0-4fe2-acfd-5de631d924dd": {"doc_hash": "946a2ff2942330632b4888732452697ca15b22debbd490a2cd64adebe0b684fb", "ref_doc_id": "75839b32-9fad-4693-925b-b66c76b23b3c"}, "2e42307c-1e89-4fd3-a115-838fdd8283b6": {"doc_hash": "8a5f9c39ffb98b80b792d32f234319ebee20fa95bfe8508d116aadb9e7a24f4f", "ref_doc_id": "41154eb6-1efe-48ef-a419-bfc90bf9147d"}, "916dcd27-48d7-4aa6-b5e3-2fbc2daef0bd": {"doc_hash": "39d9829b8d4014ebee4293c78e5e1b89caa364eb7c849c881547194cb349a267", "ref_doc_id": "ca7a12fd-763a-4702-a99c-571c4d379705"}, "db39e369-5f2d-4e16-8262-2785cb7290f4": {"doc_hash": "a21dff10cdc591750e5e11ee328557471262bc7194f7f706142027751d82cefd", "ref_doc_id": "40463d32-8daf-4a36-8770-499d5ccf226d"}, "04e1b5a9-60bb-49e1-9736-5dd07f82ca8d": {"doc_hash": "9c6e8c922bcb6d9de688ca5cfe9bfd9f63f4d9f9a02391c37d03d4b9af5c181e", "ref_doc_id": "5d096a7b-6ece-4607-bbb1-bd33aec2bceb"}, "b8e90324-ec46-48a8-a092-55eddabd02ed": {"doc_hash": "b0c0bedf0dbe467a2aac7d4941841833d5f9c4ae225124a3eddde3558f553d6c", "ref_doc_id": "fb39b745-b644-41a1-872e-94da3cc1e42c"}, "bbbc5c48-7fd2-4ed7-a6aa-5919688f154d": {"doc_hash": "7f2dd3687ed690e3bedfa33d60d02376de849343fdc45df0a2076cd8e7e5769a", "ref_doc_id": "f9c0f9cb-6fec-44bc-8ed1-0f2f6b6ab300"}, "f59e818b-28d6-497f-9218-8ef20a3b9f08": {"doc_hash": "7617923e8cfc77962303c0ac1e86bec1532e97a121c222c574f78b4d9c98f930", "ref_doc_id": "5b59bd34-a1ea-4d8e-b737-b22ac7698046"}, "7aa2eff4-307a-43c4-97de-e0173d4f3cc1": {"doc_hash": "7617923e8cfc77962303c0ac1e86bec1532e97a121c222c574f78b4d9c98f930", "ref_doc_id": "9ccdc480-3dc8-482d-9c6f-7abe0d886db8"}, "8386241a-f297-47b1-8f02-3fe4471a473d": {"doc_hash": "fc279c4a97abc3619b776ed0354e206c4f524414463696466efa90f0dea6317c", "ref_doc_id": "98294a4b-8e60-4209-9d18-6a97ca746522"}, "f830c562-8ba3-44cd-a4fc-0304d76ac599": {"doc_hash": "5f51604136df19f4f025fba13f51a144911b60bc7c9804e58dc3b93c19b80d64", "ref_doc_id": "dd6b45a7-b792-4530-a976-dae2d96a08b9"}, "5b45e197-453b-4179-ae28-61f5741c8d48": {"doc_hash": "656ae5369eb641762228c99ccb78d85df841f9932895efba6f9316ad819405c7", "ref_doc_id": "fcc786f0-f412-41fc-88d7-7af3ff617a73"}, "af77e483-1b0f-4f46-a24f-a6d1a9ca54b0": {"doc_hash": "e86795a930ff585df67f01c2c7946493ed7f6c0f62bfd050ad6d73359f0a8b0e", "ref_doc_id": "dfb3ed91-2fff-4c1c-a57b-07476bc54a56"}, "882434e1-92d6-489a-99a7-67aadb38ae59": {"doc_hash": "4fcbe73ba0aab1d7aefbe2c57ad4d2e5c2e1950615eed0b592ca924447a8fcc9", "ref_doc_id": "6dd8cb00-c892-41df-a601-7d9ecdd8c329"}, "f807e6a9-6f78-4400-836b-451cb960375b": {"doc_hash": "2a347a9f03ffe3ec8d45f9e19b6bcae3c4ee21862e8381887d369e51f00d16b6", "ref_doc_id": "38d78a61-473a-4827-b6c4-da6dcf9941f9"}, "4c3a62d4-a515-4616-a8c9-9b121e0a8954": {"doc_hash": "3b08d8aca5c7aadd23d4b8aed04a6a31fd5ab52e51b5feb7635bad3ba9dc2036", "ref_doc_id": "ad594b28-ca2a-4efe-ab33-34d74644e6fe"}, "d18e74c6-aed3-4ecb-bfab-28292bd17697": {"doc_hash": "c59a4df343f8e72cd1f28ee2a1ec6958142f019eb027e31db9612151a3198076", "ref_doc_id": "d4a63ea7-31af-4e4e-b525-98b9f6562d2e"}, "29039804-9cfd-4cc5-943f-2a817869666b": {"doc_hash": "bb61ae90d28dd4d4cf4d4c02cd1dad5d68c82815cbf24dfa0a73af55e1330faa", "ref_doc_id": "1d269188-7a16-434c-8bde-f23e8896f10d"}, "b8d2c07b-9d4e-4d1d-8216-e5a36d512466": {"doc_hash": "ce4f716035ced4b581d508f0e87969c805a98409576cc4d457fc2ecee3d7481e", "ref_doc_id": "d9b8fa70-5582-4dc6-8999-b43d6b85d217"}, "c3c58786-4fcb-41a7-92e6-1c26c23e5f49": {"doc_hash": "6c13e7c3a448faadcfe201b6d89508b2a859014fc6667ad5cfcef6e0f2146b5b", "ref_doc_id": "e9d9002a-756c-48f4-9bcb-3107519b31a2"}, "43bc2fcd-08a3-4eea-ab8b-62811f76b31d": {"doc_hash": "8e86ed14857392f50fc1ac127395f27a60f914094c5d479f360f2a829c45b059", "ref_doc_id": "738bf1a2-eaa3-4dde-91da-dee0de861383"}, "27c6a234-3a46-493d-83cf-98070463b42e": {"doc_hash": "3f63f512f32db5e7754a36810a6ff7c7f0acb96254feb92079530bb990a02be6", "ref_doc_id": "808c1c41-6d28-441d-81cb-b6120b8f9dab"}, "8e0f8ecd-4e52-48e6-b294-5f8afef9b61f": {"doc_hash": "aba543e3151643b256bed7477ddfc7ec9c7f00b00818a5e9bfac09005bf7d58a", "ref_doc_id": "af78da87-a865-428e-b2d3-940b2483d507"}, "f833fc80-f5b2-43c2-b0c6-0ea92d6c935e": {"doc_hash": "1bbf2f8763428b833951323533d69ae34b6fdb209ff41488cf048e659d1e8079", "ref_doc_id": "2b1ded5d-f7ae-48c5-9d8d-fe2bc4859c33"}, "35e7b24e-6c4c-4254-9756-0d4129057640": {"doc_hash": "f832cd5183c9f8a7c1276d362d948d473b6eb3fed717ded3b315a8aed0f81e34", "ref_doc_id": "95437d44-1e4a-4f50-a06b-a70eca0d15b0"}, "baa07f0e-84d8-46a7-abb5-d323e0cb8ccc": {"doc_hash": "482cd0a7402f86a91e7faa658043d1bd3b1c037ef898c0c4468a7cf943adbc2a", "ref_doc_id": "ed1a9377-cdfc-4f41-b2b7-d2d1c1a66073"}, "d6b4b736-3119-464a-bfdc-f2dfeb90012e": {"doc_hash": "a32d9c849a878c2a2a84bc0ad6626d931fd3aeef81cf1475084bdd4edfa54d00", "ref_doc_id": "9b0cadaf-a01a-4018-b59b-bc92ec0d4da0"}, "33fe586e-3838-44aa-8b6e-03619939c9d6": {"doc_hash": "8a111dc747c396a774624e1a1c88d180b8ed39cf6c9666d90119e3ed9de2d090", "ref_doc_id": "070616f9-0c5f-42b5-adac-e3751842b2e7"}, "4362f1ee-fdf4-4261-bc07-a512169ece67": {"doc_hash": "2df6cfbaced5de4792e60aa6967033f9195e016110fc2a9e67d8d7710d03df56", "ref_doc_id": "0228951c-1a5a-4a45-bde8-20e01de18878"}, "7967424c-25c1-4b7c-a598-acbe39882de9": {"doc_hash": "eb079649857749a7a853836ba6d82db990b8cb517d771acc2bb90b63a9a5fd91", "ref_doc_id": "af3d69e9-ff54-431e-8eac-f06dd4eb3e76"}, "4400a42d-b508-4e26-a90f-8bddc97f9446": {"doc_hash": "c084cb01aa2a480d13dc1a3f7d90f760736f0fb74fcf6bc609a4d84c9caa6aac", "ref_doc_id": "86d2c220-0a8b-4337-ac14-86b64db7f6e1"}, "c97fbcc8-5500-4b11-8660-2e0fbf202b79": {"doc_hash": "0c546f607a56d23f2f6cce373809e8bab489614ad702d4b4ba3c39dbeccaf538", "ref_doc_id": "66dafcac-9034-400d-b7d1-6cb021583260"}, "721f456c-7bf4-4a24-85e4-6e45ce3e6574": {"doc_hash": "354e6b0d46af801adc588e7c7ba219769581a524efaf047aaf562c2d67bdbe2b", "ref_doc_id": "24d4f034-b9e4-4369-9c49-3119b310b548"}, "b5b19af8-2842-4176-91bd-3ddacab95a4f": {"doc_hash": "d4cf1a5a8c65b94ed65fd313dbaf622ae10b5697786701418ca9d2f0156826aa", "ref_doc_id": "dc87abf2-5f82-4618-a906-f262c7e32818"}, "5cd927f3-7c2b-4720-9431-321cdb1ae55b": {"doc_hash": "85d571ad625bb7172d9cd6ea56d1514ab2248735df9aaa6d74bff685d2a67a55", "ref_doc_id": "1e170d7e-0219-44ef-90ac-680024e4a38f"}, "eefab3a0-fb96-4be7-83c3-dd085ce1c263": {"doc_hash": "c4275140db614425f0c468b07d8e715148e480fb9557baa49c62b1a1a5ddaede", "ref_doc_id": "2a3b5954-007d-43a5-904d-29a6a9052e82"}, "65f6b9ef-98c2-4030-a7cb-368f752b212d": {"doc_hash": "041a93b237a7a8343cc92d3917378d5bd740860f0dd3b3085ba1f9bde53c4293", "ref_doc_id": "fcca436e-32b6-4131-9965-551529181e79"}, "ee4a0c98-3a55-4f71-a0c3-d5fefc536e44": {"doc_hash": "b12a6b3d19dd1c23e162f7097dec22a7fc6223184ca0a423ef4176869552bce0", "ref_doc_id": "fcca436e-32b6-4131-9965-551529181e79"}, "631cac94-e2cd-472d-886b-9b3f38e5ba32": {"doc_hash": "27a82c4eda3f50eb4168b38c54a935ea77edd9aa833d15f13dba4d2e6d64f000", "ref_doc_id": "541021e3-62fe-4a44-af91-cbc6f109976a"}, "2ab02b58-f0f0-44de-8328-9afe8253a9fa": {"doc_hash": "6faf64c522c4103df58392f458d2e0c8dfaa3eb27d8fc98900f1791f5d427ac3", "ref_doc_id": "16ac756b-c246-4cff-b668-49cda942e4b3"}, "2775871e-a372-4724-af90-b61a4d694032": {"doc_hash": "1da69bd73c93024a114bf07e37a1571d19ff6291731537e1cf471048966c90bb", "ref_doc_id": "319c2a65-fcff-4bdb-bc01-11ae4ad79a6e"}, "6827eb2b-db43-4da4-ad8d-d08429642a91": {"doc_hash": "a56c17dd78ecc95cb3da5aedcbe66e42d3eb387f7cca680b72d4c862c6119bd1", "ref_doc_id": "ba434c09-b641-4a78-b697-b9e3ea4e85f8"}, "c4ab0e8f-c2f6-4718-aea7-b62eabab107d": {"doc_hash": "73587907ff67a5d392166ed0902aff237c344657d3a60c3e99cd3054fda52868", "ref_doc_id": "59d99ede-5036-4de8-9cbd-9c179de7dd1d"}, "14ad0550-d114-4ef8-b9c5-6a684abb6126": {"doc_hash": "26190faa183a716ef725484c6c8db778bce4aea1d5f1bc454b7f08e1c443f011", "ref_doc_id": "2dfbf854-899d-4813-9a03-9ac74644979f"}, "d81971b1-5ece-45cf-b76d-9dd7155fd164": {"doc_hash": "e3343c14a10decb1203afb845af4f86b98da1c8376fb61aa21bab7596f3d671d", "ref_doc_id": "56ddda2a-f3a5-4595-a525-a6234ecdd8c2"}, "eff7c45b-8876-4552-9c0a-4f48e5331e39": {"doc_hash": "5dc01b57b60a33ccef9102e32a991f5306f0491c942256f49a486036712c775d", "ref_doc_id": "60ab73d7-371e-4eb4-86d0-dac99b341355"}, "2444b2a1-fa23-4023-ad27-72bbd98c297e": {"doc_hash": "90a6a27315f4c019be55c80334546c135d69af3c5da8e41acba4b77e552310d7", "ref_doc_id": "826696e2-85bc-4f46-a80e-0281e76111bb"}, "7fa9b4a3-008e-49a5-849b-55cb1b58aa4e": {"doc_hash": "f978f125b94943a500707bacfe66419349d24f6f73fa9321f0a55b33f1b7e938", "ref_doc_id": "95f884ae-7871-4dd0-8826-34b4a1b8892f"}, "3f9fe088-82a8-4953-b649-3d801781ac02": {"doc_hash": "19524b2ced77ed21bb625d6b383089dee7d04be988dd230d408b54a622577690", "ref_doc_id": "84ea060b-8647-46f1-b040-ff3ba0bac1e2"}, "393670cc-d306-4499-84f0-ca41247fb5de": {"doc_hash": "683d2ff318325a29582e1c5982f94c2d9296177f53bcc7e81774186dd4a20736", "ref_doc_id": "8c092a1d-422d-4aaa-b04d-d595eebe7a2e"}, "348009a3-6d66-4603-813f-31143d202cc9": {"doc_hash": "0aeb66e9fdc7899b93ec9f3a8b4de0205e37397c9bcfb18f6931f4b96c6e9d49", "ref_doc_id": "608aafea-5510-4f18-b150-b146f3027a75"}, "c497fa09-6589-4b07-b9f2-27798f0857d9": {"doc_hash": "81551858f1fbf5284ced1f87ebaaaff8e7e331f7e693eab9553265e8099c61dd", "ref_doc_id": "b49ef040-ef86-4025-834e-5632d2352d13"}, "79e31419-4df2-48d1-8527-70a0e2caee0a": {"doc_hash": "4e82341269f695bacdb2d2742bb62ae629cdc37e3b0fc8138d1bc0b8d2b111f9", "ref_doc_id": "258ec2ca-b291-4034-8c40-8bc47c56f7d1"}, "a728d1c7-b7d2-40cd-bbe7-6814eaa3c218": {"doc_hash": "bdfa67c9100a326bc7bb6957608267482f80f587ea5491223eb2cd7ca5460ea6", "ref_doc_id": "ffc11a9c-7ce3-4d0b-b52c-d14b46f8c798"}, "b235cb7f-a146-46c7-bc27-1cb248e26112": {"doc_hash": "c972ace9584b7f3a059c3d952893b95c31999dbb2d9c8f82ceb3a05f7cc6a5a0", "ref_doc_id": "c5fd03f5-cc00-47d8-ba88-1c4574a3f31c"}, "25acd883-9570-4d9e-8863-6a6007877b53": {"doc_hash": "f8a69cc363e0fdae425dc8303246d3c982f12482ce35e9cc6857acf92d197d44", "ref_doc_id": "e19d7283-f1f9-4a4b-8aa2-3edd8c2b6448"}, "c6fc1c36-c758-469e-8b64-9b93b55412ba": {"doc_hash": "aad515e5350920d6e6bf5b56b79d49652ea86f03cf4064f10100f2ef5708a1ea", "ref_doc_id": "b55bbe6f-bb9d-49a8-a54d-bc137e07bfca"}, "f3a7eacc-d017-4ab0-a460-2d24f50a3e95": {"doc_hash": "34e9837bbfe8c63454d0e15a0d24b90e562031758ab70ed7969c6ca7df55ac53", "ref_doc_id": "d2d9ce3b-0ce5-4437-b8f4-e7b2a73f6326"}, "14fbc204-f12a-42c8-ae50-38ee75535741": {"doc_hash": "138d014f4eb3cee0638de4971d4e8f5e04170f07aa882a41608a84cc6a3260e5", "ref_doc_id": "3a57f718-94cc-4f09-9700-3543e95c4e50"}, "d044ef7b-4eb4-45f2-bdb4-cf9801ee3b5f": {"doc_hash": "d807ebcae1c495ee07d6613e6060fbe00d64c15961e30dfc060bc5ea2ae85e4d", "ref_doc_id": "cc6917de-9329-4048-9f16-c251e2d1863d"}, "57060b29-5d65-4dc2-af2b-82ef79c2238c": {"doc_hash": "dc5f3639f471693059aa60069e60d0a27e5e39c1028925bf7e240d74a717037a", "ref_doc_id": "25ce4b9f-9800-435a-97aa-6f0425f75594"}, "411371eb-a753-463c-86ea-933296b29622": {"doc_hash": "89dfa82b9fd5faf9edad3d08a033e9e70d7e9951a4422c1c0d1692ff80ef3166", "ref_doc_id": "c0a67858-64ea-4182-a21f-07453bc5a663"}, "62de375d-817f-445d-af47-285008aafb00": {"doc_hash": "20a01c2926070f472af1dc0249c0433311aa860c7fa6846019b6b90e78a51578", "ref_doc_id": "5582b301-a846-491a-b4f9-88eb44b26f81"}, "33f630c9-c9d7-4305-911a-0960e42bb20b": {"doc_hash": "724e80343e94e3730de68bf5e4a8caa2e459d5c7636f585bed793d7976ea528f", "ref_doc_id": "ec540a68-1ac1-4c09-a70d-d55fcaaf2d6a"}, "17b89515-8c0e-4643-913e-49f9149af623": {"doc_hash": "81983d7209f86e3b30e38c108c3d5a04456f9d8e4bb238ddfa3a9a87dbfdfc1b", "ref_doc_id": "ab18cadd-350e-48ee-afdb-a0751b03d4ed"}, "386e8c27-bcb2-4af4-b520-268b22193dca": {"doc_hash": "5f8c4cdecce1b07a841e6e67c3e647f1a5712af8a4de45afb4e39295d5e45586", "ref_doc_id": "b2c71e46-5c60-4cf4-974e-891020707f46"}, "e47f2d16-2f8c-4bb7-8b86-1c23da1e9d00": {"doc_hash": "ceacfcdcaa1af14157bf60b9fc58a83291a073e96e918dfda95df0c0c563f834", "ref_doc_id": "a583d714-c337-4b39-9e86-9e626c953529"}, "48d53300-7a44-4c95-9df9-5fcd8e5a5360": {"doc_hash": "eb00f7bd092dc0963e8f553951156deb1a24f00cea5980f0c29000062c043bb3", "ref_doc_id": "d2b6cd77-b343-42a4-84bf-b4771a56e23e"}, "23e3a298-6ea1-465d-b644-22c2745c7f45": {"doc_hash": "6ad9a8910d5dca1e287f8e6a4c246c711af290f72d4dbca9d1cbc1fa3b02cdd1", "ref_doc_id": "d71cc488-5d71-4826-a19e-fcde02a901fc"}, "ae29f452-2855-466e-8fef-e4cc83d347e4": {"doc_hash": "6a528b71d0eead86a84216c46e3e56ad9f7d48563123f18ec5102fbdebce8116", "ref_doc_id": "7cd4c9bb-1162-47a3-8c0a-5ef8c67050f2"}, "5eaf5487-0725-445c-b6fd-aa36b3783233": {"doc_hash": "adabb477727dea80971405a3199cf439a1a05cb5d73bf40c22bfa191eb69ac57", "ref_doc_id": "939cc4d6-0010-4590-be52-5307ef2fccd6"}, "59bc6271-d860-4282-a06d-cc9db401b0a9": {"doc_hash": "b0899599d0ab47523ac6a2ce20311cfd8688b548a57b3a28f7f24d225211939e", "ref_doc_id": "68f64ab3-96f3-44c9-b985-f615b75345a7"}, "b8646851-2a6b-460d-9eca-c81dfceb4009": {"doc_hash": "c6808f5f92460ca1b9ae347bae20f0969937cb2d9e737a69a0d2453e042bdeb9", "ref_doc_id": "977d2ba1-26ba-4e1f-9ad6-b3e49f26e190"}, "3db3c190-8725-4159-b5aa-670947300255": {"doc_hash": "ed8a785a7a3ddd9767a847e0655316073961c227ac15e5d111ca0e6026c885cc", "ref_doc_id": "a5adb3c3-d665-44f5-aeeb-ae061cdab3c2"}, "cde97679-392c-49ce-9864-c495cfe102ea": {"doc_hash": "1067f8654696a603a370fae0be55688a04e5e4b70f08c8c09853d2f9beffdfaf", "ref_doc_id": "68c8eea7-0e8e-4ccd-9c97-bb909a682b72"}, "b3197e30-2954-4f1d-8b2b-c85e39a2a0a0": {"doc_hash": "910e6077b51a8a0022d59465c8e1f50e9ecfeb159c903993f08fc56a210207cc", "ref_doc_id": "388f6cb4-6216-416f-8f29-a4713a0e820a"}, "6fdaf52b-d1f7-4105-ac30-fa2fa1a97fa9": {"doc_hash": "32374d2d09b630a6351861d70cf39523945bd82898a3da22a9728df66cd22ec7", "ref_doc_id": "b59e7ddf-4e06-45f8-98fd-1be93facbf52"}, "892707aa-c62a-4f1c-a02a-3be6d7268ed7": {"doc_hash": "2e771841203ed4f1ebca19bb501bac55b8319da362c4adf552612c951d3d0226", "ref_doc_id": "b96cdbe8-fc7e-4740-89ea-3fb497343439"}, "5bdbd137-a951-49f7-a0f3-a2e61c89ee81": {"doc_hash": "86baba025dbab5af965f0f13d85c3e33da057f2ef287a40ce460b3dbbb9524be", "ref_doc_id": "5dafe8e9-9c65-4a5b-88c1-6c01a380db43"}, "6badb99c-27b8-451e-a1e5-8f7a2fa2665a": {"doc_hash": "6b944c8b487a820e9cb65c1054f3229197fffbc8848c453cafc49ea61685675c", "ref_doc_id": "79e8e835-1128-480a-a8c8-4dbd220db150"}, "cb24ccbf-9161-49f3-bd4d-1574b0dab7ba": {"doc_hash": "e57bd9a7d2c853d524f2c43f12ca58c1fb7655ae061a9a3eeed8f7e9e9a1693c", "ref_doc_id": "5b5c33d1-cbfc-4552-8761-8277156d2981"}, "d7429b22-b3fa-4ca3-8d2e-fca40bcdb02b": {"doc_hash": "38280d4eee673496cd0d2933b46afa3d69c37790f95cfa1d67b59bfcda4ac2cb", "ref_doc_id": "ded36305-a0ed-4456-9622-353b15d30a01"}, "cf756990-45c6-48ce-8dc9-dbae27209b69": {"doc_hash": "ff625fb04d8a45b8d8f1e23ce2156ff50e59ec1c0180087e7e458353036e0e5d", "ref_doc_id": "bbff4927-6f8a-4717-a7b5-4faca5c3ca75"}, "c1a8a8ba-e2a0-4d78-8326-d22cf10e2b4a": {"doc_hash": "5504fe010868e3d40ea3ec85a131b4c1af94bc44d72b839de4ba37fdd3388feb", "ref_doc_id": "202dbead-1d12-4e62-93dc-001ca0f99504"}, "c43ec9d4-300f-4667-aafb-70ee67eb608e": {"doc_hash": "75da3e57d302e54d555f73d5ed9c4cdb892a566d534888cacd1554ca8d79b2a6", "ref_doc_id": "bf6128b9-70c3-4494-a4ae-99105d7678e7"}, "e159ac80-5025-4508-afc1-2c0a6fc23180": {"doc_hash": "db6de1204f21ca50c536c58a2b35a82b14ac58484fddf1750a94a78382ced475", "ref_doc_id": "e1df3f0a-8d73-4c0b-8721-17fe836343ba"}, "734abeab-32a4-4daa-a9f1-de9549ac0522": {"doc_hash": "3b80c619f957bfd655e40beabe71a799e0373d8de8444f5f6d8bfc7167559540", "ref_doc_id": "147d18ed-d32d-443e-82e6-76ab6e61e411"}, "8f44c2d0-feb2-4854-98d7-8cd6310b9d3d": {"doc_hash": "4b1989f9de21b7f5489f36eead1d3e3eba489123a26aa9ad210e523f04aa9986", "ref_doc_id": "5e29c248-6458-428c-8f85-9c0c1761f29d"}, "4610c535-7ea7-442e-94e0-10d70077b49c": {"doc_hash": "f5d53d1398ba44b83f2031a5e26ff1f81cfcca90c393e16c2b5cc7fda2617c38", "ref_doc_id": "c90174a3-70e5-46d9-a27c-48b681479604"}, "ec74560d-6930-4d06-934d-93d544f32f70": {"doc_hash": "eea3a3623e22c2f6bc6df780da322b54178a12190308bbb6439bffdfae74e21c", "ref_doc_id": "8ddc942e-33ac-48a5-a9d3-ec7613333513"}, "a1e7ea5c-59d1-4714-a57a-0910efc22dc7": {"doc_hash": "6fd11f86956c1b34c081439a6cfc6fb09e9d94c47277bd42d7714f1f5b90deda", "ref_doc_id": "44c59420-9342-4295-9c90-2ca56c95ccbc"}, "5f8180db-7320-48f6-9d9f-f078abe7c774": {"doc_hash": "c7cae1999eff2a018504e797ebe771a9d68ff58435ad68542a61b1be881b9851", "ref_doc_id": "264e76d8-6025-4ff0-a8cb-16c7fbcac64b"}, "5232df30-d306-4b3f-913f-1d15d4fa847f": {"doc_hash": "e92e954953b7591dcb82aef89fd8409557f5d900189220cfb06062d403834e50"}, "733e5456-7fb7-44c0-b5da-2f1b07323d5a": {"doc_hash": "da3b17341b8ed8ce587f85f5d0f4397731ccbbe89b5a0763aa190e121b58b0f1"}, "0ff8f71b-233e-422f-97cc-9e18e3f235cb": {"doc_hash": "f8410b1d0f632293e7e022564eeb215b1dd58dbae530b66fbafddb1060307d97"}, "32dccfd9-54d0-4ee0-8042-cda283c63da9": {"doc_hash": "eff6cd8b0db8f93b651d6487eec50cb475d25c1ce44f758dc94decd9be59de53"}, "15132885-82ae-4f91-9a0c-7a90628fca79": {"doc_hash": "af1e4e3a70e62cdc893f4ce53d2ad3e36d02239b28eda14cc2867320504a95ca"}, "cbeebb7b-90a3-40ee-9da1-731ed89da8c8": {"doc_hash": "f3a1418eff92eed4af868763bd11eaa63399cb6db2d69f7a2cc65757300b1535"}, "2d9f7c2f-2508-4971-abbf-7173f6e18224": {"doc_hash": "fc43517e3a2a92b3cab75aec1b64ff0f7d731cc4974a8e2237c208213840366a"}, "4cfcc553-4d6d-47bc-920d-1113db94514b": {"doc_hash": "32afb4373d6649155c5d76188e45223aa156077d398d7dc66f8b28c2301b7276"}, "a34e275b-5d72-4eee-83c5-942ada88e87f": {"doc_hash": "1bac07a48e812aad07098bb989df61126c4067037558f62d60c57c6c6dacdd52"}, "38a3d2ba-d63a-4716-bee8-8bdea6a79f00": {"doc_hash": "08d91bf52207a747130bf42f8a921a40f25125e0fbeac53f4ae9f1b66fa153bf"}, "f1891453-d80f-4530-9c69-93367a8b6fda": {"doc_hash": "2bc106b2145ade78a7f70b2c17c179aa5fdbc163e7d6d1ffe1c6f88854117ba5"}, "3d87735f-c4ee-456f-b1dd-2b91a5e68a78": {"doc_hash": "cb1bbe4df56942b2aa4b7bda7c640c0c87edf1f7e752d8f5f3e84e3921d74073"}, "d40067f8-4c45-4c79-9a07-be5407fe3e78": {"doc_hash": "63cb29b26954afe4bc8943a21c927cb5e862fdb8bba69e698c2a901773d3e3ea"}, "4435a791-582a-48ff-ac93-357740d43bee": {"doc_hash": "d7408f5bc1bf5f78a957dfa91db33d0f14b0c54dea2220f0f2d4e6879f255c87"}, "9cae9f41-4e8a-4d9f-a373-806188db1987": {"doc_hash": "80bd8c43883ae30170774bbddd35ab2361042474471d2763aee1d5e0c5e584a8"}, "d9e4072d-302e-442c-892d-90d3799dcd8e": {"doc_hash": "86f0bb9b49bb56c89b43766b25a303a5206374715d22f8ab641420e7dcb47a4e"}, "4109ca5e-1a25-47aa-bbfd-d7af66b5a60c": {"doc_hash": "c37b305a668c18d1eb8768fd69cdbd29a91ab47e39c28c397124cbf498439fba"}, "94ba990e-9836-4650-9f03-f31df2444a7b": {"doc_hash": "5d770ede05d8a4ee107f189ef95d4fac224c71a0487060aa6cde8f4a7f23f0dd"}, "a6d937ef-dcab-4123-9293-1ee32e7b2ecc": {"doc_hash": "68355be5eb8e3fdd66bd5b4ccc95d5ec2c1011b48167bdb4526c740c25e050ab"}, "8e886dc2-85f8-4cbf-bc6e-d6b8a6d35a89": {"doc_hash": "9bcdc450b5d16c3785c77ce06a919a8453f4b2ed7ccf8ce897eeceff2c7ac178"}, "e0077b45-902c-4a43-be8a-d0a18cb600c2": {"doc_hash": "84c001c90c32de61eda49a9c42a4ba5c624142255766dec873ba0c41844b86c0"}, "d999afd1-e232-441f-9ad9-6979f9b6eefe": {"doc_hash": "4a8a5422babf0e0a938c85a94dbdb00d09e86b0969739f11e553ed9b4b76ba12"}, "58f4e91c-010e-4039-a292-ae413046daf2": {"doc_hash": "f45349c59bebfc52f5ae81e837cd2f45526de4174ccc23d524190b984b6e60b1"}, "4cc77699-cebb-49f0-a0c6-48a9423f5c5b": {"doc_hash": "a3c685b1561bbef21e5dd92e066245f028154a206f45e9b1eec193aa506aa90b"}, "35dda69f-47d7-4d01-938b-557c210df6ff": {"doc_hash": "8e6b2b9f85dc751cfa3160eccec7eeea67fe21c2df4ed4f48de405d00cccafdd"}, "f707390c-e85f-4864-bb7a-e2fa9df457ce": {"doc_hash": "c85da92f04aeed62ee239232c99d3d0041875abc32508e94abed5b346184615b"}, "7de2996a-063e-41d0-aa45-83104914b21e": {"doc_hash": "4ca1df24005f2ce6d8ae4ce4a567971921be12bc085ecdb6607695bb7ff4d7f9"}}, "docstore/data": {"5d2b6663-bfa5-41b9-b417-66edc8457af8": {"__data__": {"id_": "5d2b6663-bfa5-41b9-b417-66edc8457af8", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a733ce94-88ff-4886-9e0b-6d820b3450c3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "01632aadd138fcb3a6dc80e35dc28c6eeee253629048ab54596855435bb4170f", "class_name": "RelatedNodeInfo"}}, "text": "1. Overview\n  2. Lab Scenario\n  3. Let's Get Started With Snowflake\n  4. Launching Matillion ETL from Partner Connect\n  5. Matillion ETL- Creating a New Project\n  6. Creating an Orchestration Job\n  7. Creating a Transformation Job to determine the Current Position\n  8. Creating a Transformation Job: Profit & Loss Calculation\n  9. BONUS - Daily Updates from Yahoo! Finance\n  10. Conclusion\n\n[ _bug_report_ Report a mistake](https://github.com/Snowflake-\nLabs/sfguides/issues)\n\n _close_ _menu_", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a6235638-b32c-4ef1-b4f6-b14ed4f57dba": {"__data__": {"id_": "a6235638-b32c-4ef1-b4f6-b14ed4f57dba", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b00d7971-ba19-4e91-80b7-79e173259844", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "53bb0e012cdb0e81effbc3f292fa724de01adc5b4ee400dc8aa5a525dcd0e89f", "class_name": "RelatedNodeInfo"}}, "text": "Cloud Native Data Engineering with Matillion and Snowflake\n\n _access_time_ 140 mins remaining", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b05e1a6-2fda-4981-9c3a-fff8e4e9dd3b": {"__data__": {"id_": "2b05e1a6-2fda-4981-9c3a-fff8e4e9dd3b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5520389-916a-4f5b-b7ad-5f2704df39ff", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dcafea6b2b54e3baa1012703dfd1ecfc332a8b9d6b280a986323e5700e0cb031", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Overview\n\nModern businesses need modern analytics. Businesses that fail to capture data\nand transform it into timely and valuable information will struggle to stay\ncompetitive and viable. Snowflake and Matillion help agile enterprises convert\nraw data into actionable, analytics-ready data in the cloud in minutes for new\ninsights and better business decisions.\n\nLet's get started.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d0a84cd-6cb1-4c76-88f7-ce6cf4887e02": {"__data__": {"id_": "5d0a84cd-6cb1-4c76-88f7-ce6cf4887e02", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0c0e3ca-96fa-425a-ac47-934a5b00f302", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "362b68b060a924b94f3aeb3ff57800ced489702574c907374a5ce867007481d4", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Use\n\n  * A trial Snowflake Account with `ACCOUNTADMIN` access\n  * A Matillion account, provisioned through snowflake's partner connect", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3bf8ad8a-a03a-4bb9-aac9-f3d0e211afd2": {"__data__": {"id_": "3bf8ad8a-a03a-4bb9-aac9-f3d0e211afd2", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d01a50e-edba-4f36-8e93-0d3b04349de8", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "781f36ba92412729cb81d639bdf20016a00b7eedc0f6773f0294bedf4588931f", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * How to source 3rd party data from Snowflake Marketplace\n  * How to use Matillion's GUI to build end-to-end transformation pipeline\n  * How to use Matillion to extract real time data from public APIs\n  * How to leverage Matillion scale up/down Snowflake's virtual warehouses", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "23e91978-8728-435a-b1eb-deb923d1b20e": {"__data__": {"id_": "23e91978-8728-435a-b1eb-deb923d1b20e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79196d52-ee0a-4677-b3a7-608151870f60", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d8fc5c3ee3fa0c3246ce6d74486f994f7bccda544baa7c69dd891653670da1a7", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * An end to end data transformation pipeline for Financial Services data leveraging Matillion and Snowflake, leveraging different data sources - joining, transforming, orchestrating them all through user friendly, and easily managed GUI services", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f0761b1f-60b6-47ab-969b-409e541859a3": {"__data__": {"id_": "f0761b1f-60b6-47ab-969b-409e541859a3", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e26a0b2-a45c-44d0-ae18-832336084a45", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5136cb26f666792ef39a7ad58961df5b945593b46ffd860bbc391fb5af21d241", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Lab Scenario\n\nYou are a stock portfolio manager of a team of 10 traders !!! Each of your\ntraders trade stocks in 10 separate industries. You have with you available 10\nyears of historical data of trades that your team performed, sitting in an S3\nbucket - you know what stocks they traded (BUY or SELL), and at what price.\n\nYou would like to aggregate their Profit & Loss, and even get a real time\naggregated view of total realized and unrealized gains/loss of each of your\ntraders. To accomplish this, we will follow the following steps:\n\n  1. Acquire stocks historical data, freely provided by Zepl, from Snowflake Marketplace. This will create a new database in your snowflake account.\n  2. Launch a Matillion ETL instance through snowflake partner connect.\n  3. Use Matillion to :\n\n  * Ingest your traders' historical data sitting in a S3 bucket, into a Snowflake table.\n  * Develop a transformation pipeline to create each trader's PnL as of today, by joining with stock data from Zepl\n  * Leverage Yahoo Finance API to get real time stock data\n\nThe 10,000 foot view of what we will build today:\n\n!2_Lab_Overview\n\nSneak Peek of the orchestration job that will accomplish all this, nested with\n2 transformation jobs within it:\n\n!2_sneak_peek", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6a18737f-5ec5-45cb-87d2-baaec0f24708": {"__data__": {"id_": "6a18737f-5ec5-45cb-87d2-baaec0f24708", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0187d12-3c4a-4514-8b06-2b5587c530a2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b46a0289185c442aa8e240b86cf49f62c3e8f175763b3201a3c78bc7b5e6c245", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Let's Get Started With Snowflake\n\nLogin to your snowflake account. For a detailed UI walkthrough, please refer\n[here](https://docs.snowflake.com/en/user-guide/ui-snowsight-gs.html#getting-\nstarted-with-snowsight).\n\nAs the `ACCOUNTADMIN` role, navigate to Marketplace, and search for \"zepl\".\nClick on the tile.\n\n!3_zepl\n\nNext:\n\n  1. Click on \"Get Data\" on the right.\n  2. A pop-up screen opens: prefix the database name with \"ZEPL_\" so the name becomes `ZEPL_US_STOCKS_DAILY`\n  3. Click on \"Get Data\" in the center.\n\n!3_get_zepl\n\nSo what is happening here? Zepl has granted access to this data from their\nSnowflake account to yours. You're creating a new database in your account for\nthis data to live - but the best part is that no data is going to move between\naccounts! When you query, you'll really be querying the data that lives in the\nZepl account. If they change the data, you'll automatically see those changes.\nNo need to define schemas, move data, or create a data pipeline either!\n\nClick on Query Data to access the newly created database.\n\n!3_query_datal\n\nA new worksheet tab opens up, pre-populated with sample queries. The newly\ncreated database has 3 tables. Feel free to click on them and browse what\ntheir schema looks like, and preview the data they have.\n\n!3_zepl_worksheet\n\nCongrats ! You now have decades worth of stock data acquired in minutes !\n\nOne more thing: we need to locate and note down our snowflake account\ninformation for subsequent steps. To locate snowflake account information,\nnavigate to **Admin \u2192 Accounts** , and click on the link icon next to the\nAccount name to copy the account name URL to your clipboard (The **text that\nprefixes .snowflakecomputing.com** is the account information needed to\nconnect Matillion to Snowflake). Paste it in your worksheet, we will need it\nin section 5.\n\nIn the screenshot below, the account text we are look for is: `bjjihzu-\nji91805`\n\n!3_account_id", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9fe3054c-a13b-48a5-bf79-a618b8859d4d": {"__data__": {"id_": "9fe3054c-a13b-48a5-bf79-a618b8859d4d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8fde317-d3b2-447a-8ca6-1f48a51493e7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "45c441bddf61804595bf93570dc1c04dc7983b17ca56697eb2a7715023a11cfc", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Launching Matillion ETL from Partner Connect\n\n  1. Navigate to Admin \u2013> Partner Connect, then click on the **\"Matillion ETL\"** tile\n\n!4_pc_metl\n\n  2. All fields are pre-populated, **give additional \u2018Optional Grant' to****`ZEPL_US_STOCKS_DAILY`****database** (created in previous section), then **click Connect**\n\n!4_og\n\n!4_metl_connect\n\n  3. Once the partner account has been created, **Click Activate**\n\n!4_activate", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fd95fa48-f7c7-491b-9fe6-7fa5a654c47d": {"__data__": {"id_": "fd95fa48-f7c7-491b-9fe6-7fa5a654c47d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fb7dadb-8891-473a-8a34-8bffc19aa4a1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1f4ede5fee8ca9b53c82c247aa47895b98e98895ba63bba459efbabc46154e2a", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Matillion ETL- Creating a New Project\n\nYou will be redirected to the Matillion ETL web console. Your username and\npassword will be auto-generated and sent to the same email you provided to\nlaunch your Snowflake trial account.\n\n  1. Once logged in to Matillion, you will be prompted to join a project. Click **Create Project** to get started.\n\n!5_metl_cp\n\n  2. Within the **Project Group** dropdown select **\"Partner Connect\"** , add a new name for the project (for the purpose of this lab we will name it **\"TraderPnL\"**). You can leave Project Description blank, and the check-box's with the default settings. Click **Next**\n\n!5_project_name\n\n  3. In the **AWS Connection** set the **\"Environment Name\"** (for the purpose of this lab we will name it **\"Lab\"**). Click Next\n\n!5_lab\n\n  4. Enter your Snowflake Connection details here. The Account field is the same text you saved from Snowflake UI in section 3. Also enter your Snowflake account \"Username\" and \"Password\". Click Next.\n\n!5_metl_sf_connect\n\n  5. Now we will set the Snowflake Defaults. Select the following default values:  \nDefault Role: `ACCOUNTADMIN`  \nDefault Warehouse: `PC_MATILLION_WH`  \nDefault Database: `PC_MATILLION_DB`  \nDefault Schema: `PUBLIC`\n\nClick **Test** , to test and verify the connection. Once you receive\n**success** response, you are properly connected to Snowflake. Click\n**Finish** , and now the real fun begins!\n\n!5_sf_defaults", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e69719e5-1b5d-4d59-b54c-3e7a1964bea4": {"__data__": {"id_": "e69719e5-1b5d-4d59-b54c-3e7a1964bea4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa235d21-c392-45e9-96aa-9896d9459fed", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2277ee1717e1bdbd20c41ffb3bbefc9168ad989ff6922ca53e1b44f97020120b", "class_name": "RelatedNodeInfo"}}, "text": "6\\. Creating an Orchestration Job\n\nWe will now create our first orchestration job. The job will consist of first\nloading the trading history from AWS S3 to a single Snowflake table. To\nefficiently work with the data, we will modify the warehouse to the\nappropriate size using the Alter Warehouse component. We will then create two\nseparate transformation jobs to perform complex calculations and joins and\ncreate new tables back in Snowflake. Finally, we will scale down our warehouse\nwhen job completes. By the end of it, the orchestration job should look like\nthis:\n\n!2_sneak_peek\n\nLets get started!!\n\nWithin the Project Explorer on the left hand side, right-click and select\n**Add Orchestration Job**.\n\n!6_add_orch\n\nName your job **\"VHOL_orchestration\"** and click \"OK\". You will be prompted to\nswitch to the new job, click **\"Yes\"**. You should now see a blank workspace\n(new tab)\n\n!6_orch_name\n\nThe following steps will walk through adding different components to the\nworkspace to build our data pipeline. The first step is to load trading data\nfrom S3 using the **S3 Load Generator** component.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "67208868-b858-48be-8736-7add805b496e": {"__data__": {"id_": "67208868-b858-48be-8736-7add805b496e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea6bd15d-3ce7-4541-a567-133997af210e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4761af4dba0242367bb58e9141807123c6e0ed2159985fa994ec05f9073b21e4", "class_name": "RelatedNodeInfo"}}, "text": "S3 Load Generator\n\n  1. From the Components section on the left hand side, expand the Wizards folder. Find the S3 Load Generator component.\n\n!6_s3_lg\n\n  2. Drag and drop the S3 Load Generator component onto the workspace as the first step after the Start component.\n  3. A S3 Load Generator menu will automatically pop up. Click the ... button to explore S3 bucket\n  4. Copy and paste the S3 bucket into the wizard: `s3://mtln-techworkshops/VHOL_Trades/`\n\nClick **Go** to explore the contents of the bucket, you should see several CSV\nfiles - these are trade history data of 10 traders, trading in 10 different\nindustries. Highlight the file name `ARYA_SWINFRA.csv` and click **Ok**.\n\n!6_s3_files\n\n  5. You can now sample the dataset by clicking **Get Sample** , it will return a 50 row sample of the dataset. Click **Next**.\n  6. Matillion will guess the schema on the dataset, you can make any modifications to the configuration. For the purpose of this lab, we will keep the configuration settings as **Default**. Click **Next**\n\n!6_file_schema\n\n  7. Click **Create & Run**, this will render two components on the VHOL_orchestration canvas (Create Table and S3 Load).\n\n!6_create_run\n\n_Note if you click test you may receive a permission error on the S3 bucket.\nYou can ignore this for the lab, and move on to the next step. Don't worry\nabout any errors at this point, we will resolve them in the upcoming steps._\n\n  8. Link the **Start** component to the **Create Table** component.\n\n!6_start_ct\n\n  9. Click on the **Create Table** component, in the Properties Tab you will see several parameters. Note the **Create/Replace** parameter by default is set to Create. Click the ... button and change it to `Replace` from the dropdown menu.\n\n!6_replace\n\n  10. Now we will modify the size of each column. In the properties tab, click on the ... button for the **Columns** parameter. Update the Size for each Column name as shown in figure below, then click Ok.\n\n!6_column_size\n\n  11. Change the component name and table name to `TRADES_HISTORY`, by clicking on the ... button in the Properties tab\n\n!6_comp_name_TH\n\n  12. Right click on the TRADES_HISTORY component and select **Run Component**. This will create a new table in your Snowflake account !\n\n!6_run_comp\n\n  13. Next, Select the S3 Load component, and change the Name in the Properties tab to **LOAD TRADES_HISTORY**.\n  14. Change the **S3 Object Prefix** by clicking on the ... button to select the VHOL_Trades directory, and then click OK.\n\n!6_s3_prefix\n\n  15. Change the **Pattern** parameter bu clicking on the ... button, and change to `.*`. Click OK.\n  16. Change the **Target Table** parameter by clicking on ... button, and select TRADES_HISTORY from the drop down, click OK.\n  17. The LOAD TRADE_HISTORY component Properties should now reflect as shown below, all other fields should be left as default.\n\n!6_load_history\n\n  18. Right click on the LOAD TRADE_HISTORY component and run it by clicking **Run Component**.\n  19. Check back in your Snowflake console to confirm the TRADES_HISTORY table was created, and data loaded - 1.7 million rows, and compressed to < 10 MB !\n\n!6_sf_th", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3dc6f43c-ee22-42dd-a83f-f5f67c8ff76a": {"__data__": {"id_": "3dc6f43c-ee22-42dd-a83f-f5f67c8ff76a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adc92171-2048-4d45-83a7-78ec6015cbf3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a04e18637bb01429c7fb7d6522a504a22ab23640bc96c02b4e11b941c5e5fbd7", "class_name": "RelatedNodeInfo"}}, "text": "Alter Warehouse\n\nThe next step of the orchestration is to scale up Snowflake's Virtual\nWarehouse to accommodate resource heavy transformation jobs.\n\n  1. Find the Alter Warehouse component from the Components pane.\n  2. Drag and drop the component as the last step, connected to the LOAD TRADES_HISTORY component. Click on the component to edit its Properties.\n  3. Rename of the component to `Size Up Warehouse to M`.\n  4. Change the **Command Type** to Set.\n  5. A new field will appear, edit Properties to add a new line with Property set to **WAREHOUSE_SIZE** and Value set to `MEDIUM`.\n\n!6_WH_M\n\n  6. Your orchestration job should now look like this:\n\n!6_end", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48ff5b0d-e13c-4d5c-801a-a4b6e7ad2abf": {"__data__": {"id_": "48ff5b0d-e13c-4d5c-801a-a4b6e7ad2abf", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ccb705f-32df-4078-8164-e24e18aba0b3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "97dcce27fc45baf20cc1bd670d2df7625f46553fc2064351906b4ab883c8055f", "class_name": "RelatedNodeInfo"}}, "text": "7\\. Creating a Transformation Job to determine the Current Position\n\nThe trading history data from S3 gives a listing of ten traders with both BUY\nand SELL actions. In this transformation job, the transactions will be\naggregated to find out the number of shares bought/sold and for how much. With\nthose figures, the net # of shares and value will be calculated, and a table\nwill be created, enriched with each traders' average price for each stock. The\nbelow figure shows the end product of the transformation pipeline we will\ncreate in this section:\n\n!7_1_job_view", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb12d6fa-7d97-4c1f-9292-e78f73c8aa72": {"__data__": {"id_": "fb12d6fa-7d97-4c1f-9292-e78f73c8aa72", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32214cd6-3dc7-47ce-b6b3-a9e381e23ffd", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f45e20adca33fc0f282ab160a0849a7000350ea3b1626f0477440c77798e53a0", "class_name": "RelatedNodeInfo"}}, "text": "Let's get started!\n\n  1. Within the Project Explorer, right-click and select **Add Transformation Job**.\n\n!7_2_add_tran\n\n  2. Set the title to `VHOL_CURRENT_POSITION`, and click Ok.\n  3. Next prompt will ask you to switch to the new job, click NO.\n  4. From the explorer, drop the newly created job as the next step after the Alter Warehouse component within the previously created orchestration job (VHOL_orchestration) and complete the connection, as shown below:\n\n!7_3_orch_view\n\n  5. Double click the new transformation job VHOL_CURRENT_POSITION. A new tab gets opened with a blank canvas. We will now build a transformation pipeline.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6dddb114-d2c5-4913-adb0-0727f23b2680": {"__data__": {"id_": "6dddb114-d2c5-4913-adb0-0727f23b2680", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cebfe9c4-47a4-4b54-89b2-82e9dbcc77bc", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3096cbf3da106524364804b6ef21603744bcf308535cdd11f08d25a7bfa7a4d4", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\nTRADES_HISTORY\n\nFind/search the **Table Input** component in the component palette under Data\n> Read folder and drop it on the blank canvas, then set it up with the\nappropriate properties below:\n\nName: `TRADES_HISTORY`  \nDatabase: [Environment Default]  \nSchema: [Environment Default]  \nTarget Table: `TRADES_HISTORY`  \nColumn Names: Select all columns by clicking the ... button\n\n!7_4_Table_Input", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ea810547-dc8f-4e4e-a5a1-a18ab5a8b61c": {"__data__": {"id_": "ea810547-dc8f-4e4e-a5a1-a18ab5a8b61c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a02ca36-b912-4254-a375-2d5a9381b4e2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4820aac539384abddc92cc79c692b5729f2e271d3bbdccc1e05066c34284cd78", "class_name": "RelatedNodeInfo"}}, "text": "Filter \\- Filter Buy\nactions\n\nNow, let's add a second step to filter the data based on the type action.\n\n  1. Find/Search the Filter component in the component list under Data > Transform folder and drop it on the canvas, connect it to the TRADES_HISTORY component.\n  2. Click on the component and update the **Name** property to `ACTION = BUY`\n  3. Then use the Filter Conditions property wizard, add a line with the following settings:\n\nInput Column: `ACTION` Qualifier: `Is` Comparator: `Equal to` Value: `BUY`\n\nYour Transformation Job should now look like this:\n\n!7_5_Filter", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec4da695-7c10-403f-a985-b884c88307cf": {"__data__": {"id_": "ec4da695-7c10-403f-a985-b884c88307cf", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9074f0b-0481-4945-83bd-37f0f50ca259", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7592479bfe424b894c4b0b4c6fae8c66f781f3bbd61d6b64bf23a444b183ee99", "class_name": "RelatedNodeInfo"}}, "text": "Calculator\n\nNow we will add a calculator to calculate the amount of investment in each buy\ntransaction:\n\n  1. Find/Search the CALCULATOR component under **Data > Transform** folder and link it to the ACTION = BUY component created in the previous step.\n  2. Click on the component and name it `TOTAL_PAID`\n  3. Edit the Calculations property and use the expression builder to create the calculation:\n\n  * Add a new field with \"+\" button, name it TOTAL_PAID\n  * Build the expression: `-(\"NUM_SHARES\" * \"PRICE\")`\n\n!7_6_Calc\n\nClicl OK. Your transformation job should now look like this:\n\n!7_7_calc_view", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ecacc0f-5b65-4cf9-9d6f-11a60ff2de42": {"__data__": {"id_": "3ecacc0f-5b65-4cf9-9d6f-11a60ff2de42", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e931095-a299-4167-be3b-24230692d612", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9fbfa869dccde122238d7fbc373d0249dd612570733c1f0dd741bcc05efd2d6d", "class_name": "RelatedNodeInfo"}}, "text": "Aggregate\n\nNext we will sum up the investments made in each stock by aggregating.\n\n  1. Let's add an Aggregate component from the palette under **Data > Transform** folder and link it to the previous Calculator component.\n  2. Click on the component to edit the Properties:  \nName: `BUY_AGG`  \nGroupings: `TRADER, SYMBOL`\n\n  3. Open the Aggregations field wizard and add 2 lines then configure them like this:  \n**Source Column: Aggregation Type  \n** TOTAL_PAID: `Sum`  \nNUM_SHARES: `Sum`\n\n!7_8_agg\n\nClicl OK. Your transformation job should now look like this:\n\n!7_9_agg_view", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "756dddd6-c3c7-458e-bf6e-8b72f89e10f9": {"__data__": {"id_": "756dddd6-c3c7-458e-bf6e-8b72f89e10f9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b594d96c-ee54-472f-a677-93acc0f02801", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e14527d6ac184b60702108f1dc8b6e52095472fbb0c86574c173b791a1789f3a", "class_name": "RelatedNodeInfo"}}, "text": "We will now copy & paste the Filter, Calculator, and Aggregate components\nto create a similar pipeline, but for SELL filter.\n\n  1. Right-click on each of the components and select copy.\n  2. Paste the component by right clicking on a blank area in the canvas, and selecting paste. Connect the new components as shown below. Your Transformation Job show now looks like this:\n\n!7_10_cp\n\n  3. Update the properties of the new components with the information below:  \n3.1 Filter:\n\n  * Name: `ACTION = SELL`\n  * Filter Conditions: \n    * Input Column: `ACTION`\n    * Qualifier: `Is`\n    * Comparator: `Equal to`\n    * Value: `BUY`\n\n!7_11_filter\n\n3.2 Calculator:\n\n  * Name: `TOTAL_GAIN`\n  * Add a new field with \"+\" button, name it TOTAL_GAIN\n  * Build the expression: `(\"NUM_SHARES\" * \"PRICE\")`\n\n!7_12_calc\n\n3.3 Aggregate:\n\n  * Name: `SELL_AGG`\n  * Groupings: `TRADER, SYMBOL`\n  * Aggregations: `TOTAL_GAIN, Sum, NUM_SHARES, Sum`\n\n!7_13_agg\n\nWe are now going to join the 2 flows together.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33f73657-1e4c-4b7f-8322-883943963d96": {"__data__": {"id_": "33f73657-1e4c-4b7f-8322-883943963d96", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecbbc8a8-4438-49bb-b4a6-d0d04a57121d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3e9b0e2aea001e1e5c164c5489c4c731850eb60b11b154763882574e438c3039", "class_name": "RelatedNodeInfo"}}, "text": "Join \\- Join the BUY\nand SELL aggregations into a single dataset\n\n  1. Find/Search the Join component under Data > Join folder and drag and drop it as the last step of the job. Connect the Join component to both the BUY_AGG and SELL_AGG.\n  2. Click on the Join component to edit the Properties:\n\n  * Name: `Join BUY and SELL Transactions`\n  * Main Table: `BUY_AGG`\n  * Main Table Alias: `buy`\n  * Joins: `SELL_AGG, sell, Inner`\n  * Join Expressions \u2013> buy_Inner_sell: `\"buy\".\"TRADER\" = \"sell\".\"TRADER\" and \"buy\".\"SYMBOL\" = \"sell\".\"SYMBOL\"`\n  * Output Columns: \n    * buy.TRADER: `TRADER`\n    * buy.SYMBOL: `SYMBOL`\n    * buy.sum_TOTAL_PAID: `sum_INVESTMENT`\n    * buy.sum_NUM_SHARES: `sum_SHARESBOUGHT`\n    * sell.sum_TOTAL_GAIN: `sum_RETURN`\n    * sell.sum_NUM_SHARES: `sum_SHARESSOLD`\n\n!7_14_join\n\nYour Transformation Job should now look like this.\n\n!7_15_join_view", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2d96439-173d-43b9-a7a3-e8ce4d71144b": {"__data__": {"id_": "a2d96439-173d-43b9-a7a3-e8ce4d71144b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9a88c71-575c-4fec-bf08-a8c33c93ccd1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "822bab9ab848817d6c22df1d130bcb7fa0c7d6b220c3510016a0d2d36fb1b3dd", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the amount of investment in each buy transaction\n\nAdd a new Calculator component to the canvas and set up with the below values\n(_use the same steps than in previous Calculator components to set up the\nexpressions_).\n\n  * Name: `NET_SHARES NET_VALUE`\n  * Include Input Columns: Yes\n  * Expressions: \n    * NET_SHARES: `\"sum_SHARESBOUGHT\" - \"sum_SHARESSOLD\"`\n    * NET_VALUE: `\"sum_INVESTMENT\" + \"sum_RETURN\"`\n\n!7_16_calc", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a5465e7b-a35b-4da9-b5ca-a66f36b5f1e4": {"__data__": {"id_": "a5465e7b-a35b-4da9-b5ca-a66f36b5f1e4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b8bf068-4a8d-4980-bd1a-23f5a9e5c6af", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a36ba42614a8cd2befe66fa94f1201cb77fc509a1af2fecce2f8c438bcee341b", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the average price of stocks traded\n\nAdd another Calculator component to the job and configure it as follows.\n\n  * Name: `AVG_PRICE`\n  * Include Input Columns: Yes\n  * Expressions: \n    * AVG_PRICE: `-(\"NET_VALUE\" / \"NET_SHARES\")`\n\n!7_17_avg", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f920df6-f97d-416c-9f7b-1b278a02c490": {"__data__": {"id_": "3f920df6-f97d-416c-9f7b-1b278a02c490", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7110e9e1-dbc6-4d81-b637-7b742604fac3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "550d15c37c4b325116f36184ec98c5cb7a7591550f7db2b20568acbb3f409b73", "class_name": "RelatedNodeInfo"}}, "text": "Rewrite Table\n\nAdd a last component to the job to write the result of the transformation to\nthe CURRENT_POSITION table.\n\nFind/Search the Rewrite Table component and drag and drop it as the last\ncomponent in the flow. Connect to the AVG_PRICE calculator, and edit the\nproperties as below:\n\n  * Target Table: `CURRENT_POSITION`\n  * Warehouse: [Environment Default]\n  * Database: [Environment Default]\n  * Schema: [Environment Default]\n  * Target Table: `CURRENT_POSITION`\n\n!7_18_table\n\nThe job flow should look like this now:\n\n!7_19_flow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1e67f00-0ece-4632-bcc1-d26e71984054": {"__data__": {"id_": "d1e67f00-0ece-4632-bcc1-d26e71984054", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1606dffe-c3f2-4317-9b82-d8650be2c097", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b2085ff7e5c7eade840e81504b0d5f4f4e52fd3475f3ecc97745b4bef464cc1a", "class_name": "RelatedNodeInfo"}}, "text": "Execute this job\n\nRight click anywhere on the job and select **Run Job**. To preview the result\nof the job:\n\n  * Click on the last component (Write to CURRENT_POSITION)\n  * Open the Sample tab\n  * Hit the **Data** button\n  * Data will be sampled and previewed in the pane below\n\n!7_20_sample\n\nYou can now go back and validate the CURRENT_POSITION table is generated in\nSnowflake:\n\n!7_21_sf\n\nCongratulations, you're done with building and running the first\ntransformation job!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "367a84c9-208b-488a-9988-638b80c5a0ad": {"__data__": {"id_": "367a84c9-208b-488a-9988-638b80c5a0ad", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57f4a40d-606e-46ed-9fb9-f5809e036e03", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a5e09e73eb03221575b65ccdba3ec8323d9514179dfb4b4ec9ff779ece407bfc", "class_name": "RelatedNodeInfo"}}, "text": "8\\. Creating a Transformation Job: Profit & Loss Calculation\n\nThe previous Transformation job provided a snapshot of every trader, based on\nthe BUY and SELL transactions which took place. This job will take it a step\nfurther by calculating the profit or loss each trader is experiencing by\nstock, as well as the cumulative profit or loss, based on their entire\nportfolio. The below figure shows the end product of the transformation\npipeline we will create in this section.\n\n!8_flow\n\nLet's get started!\n\n  1. Within the Projects Explorer, right click and select **Add Transformation Job** , title it **VHOL_PNL_xform** and drop it as the last step step after the **VHOL_CURRENT_POSITION** transformation component in the VHOL_orchestration job.\n  2. Double click on the newly created **VHOL_PNL_xform** component to wwitch back to the new workspace to start building the job.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec526738-0282-41cf-a0de-313334a93d70": {"__data__": {"id_": "ec526738-0282-41cf-a0de-313334a93d70", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27a84d3b-52ca-4584-b2f5-884794e5b582", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3ab7238f67e02335e2187a965fc8e6fdbc7aadb455950ccf6873521db3710883", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\n**STOCK_HISTORY**\n\n  1. Find the Table Input component and drop it into the canvas. Click on the component to configure as per the table below.\n\nNote that we are switching database to point to **ZEPL_US_STOCKS_DAILY** to\nget the **STOCK_HISTORY** table.\n\nName: `STOCK_HISTORY`  \nDatabase: `ZEPL_US_STOCKS_DAILY`  \nTarget Table: `STOCK_HISTORY`  \nColumn Names: `Select all columns`\n\n!8_stock_history", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "392d3bae-4b0f-4304-84e6-f5173ab19f0f": {"__data__": {"id_": "392d3bae-4b0f-4304-84e6-f5173ab19f0f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "618328f2-b119-4312-a059-9b30454bfda0", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d119dd605a57c60ea6d180b23d77992c4120d11777100e5abb4fe6b9e3460c0d", "class_name": "RelatedNodeInfo"}}, "text": "Filter \\- Only include\nthe most recent close date for the stock\n\nWe will filter to only include the most recent clost date for the stock.\n\n  1. Right-click on the canvas and select Manage Job Variables.\n\n!8_filter\n\n  2. Fill out the Manage Job Variables as follows, using the !8_add to add a new variable as follows:\n\nName: `yest_date`  \nType: `DateTime`  \nBehavior: `Shared`  \nVisibility: `Public`  \nValue: `1900-01-01`\n\n!8_stock_jv\n\n  3. Click **Ok**\n  4. Drag and drop a filter component as the next step after the STOCK_HISTORY table input. Fill out the filter properties as follows:\n\nName: `FILTER ON YEST_DATE`\n\n  5. Update the Filter Conditions and Combine Conditions as follows:\n\n**FILTER CONDITIONS:**\n\nInput Column: `DATE`  \nQualifier: `Is`  \nComparartor: `Equal to`  \nValue: `${yest_date.now().add(\"days\", -1).format(\"yyyy-MM-dd\")} `\n\n**Note** If you are doing this lab offline (not on the webinar day),\nsubtracting -1 days may or may not work. You basically have to subtract enough\ndays so that the resultant date is a date when the stock market was open. So\nif you're doing this lab on Monday, subtract -3 days so that the date becomes\nFriday (assuming the stock market was open on Friday)\n\nCombine Conditions: `AND`\n\n**Note** that we entered sets the variable yest_date to yesterday's date, in a\nyyyy-mm-dd format.\n\n!8_yest_date\n\n  6. **Sample** the data by switching to the Sample tab and clicking !8_data to validate the filter is working correctly, with the DATE field reflecting yesterday's date.\n\n!8_sample.png\n\n  7. Locate the Table Input component and drag and drop it to the above-right of the FILTER ON YEST_DATE Filter component. Click on the component and edit the Properties as follows:", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38562437-e31e-4e7b-bfbb-6e2682e30ee9": {"__data__": {"id_": "38562437-e31e-4e7b-bfbb-6e2682e30ee9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c9304bc-f643-46e2-9836-15d41e33e71d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e676c037476dc4d2ddbc2963f6e8aa663c13ef8d994cee7e8d72d9948f62ceae", "class_name": "RelatedNodeInfo"}}, "text": "Table Input \\- Read\nCURRENT_POSITION\n\nName: `CURRENT_POSITION`  \nTarget Table: `CURRENT_POSITION`  \nColumn Names: `Select all columns`\n\n!8_current_pos\n\n  1. Locate the Join component, drag and drop into the workspace and connect the previous Filter and Table Input components. The flow should look like this:\n\n!8_flow_join", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "83655f7e-595a-4372-80f8-64f3a84cc172": {"__data__": {"id_": "83655f7e-595a-4372-80f8-64f3a84cc172", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb4d764d-4c1a-4708-8793-ab32c615f94a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9320bae7cd2c2cc80cadb7957ebcd403ce164c7e04da75d8e434b7763ac16d1d", "class_name": "RelatedNodeInfo"}}, "text": "Join \\- Join\nyestserday's stock close with the CURRENT_POSITION dataset\n\n  1. Click on the Join component to edit its properties as follows:\n\nName: `Join CURRENT_POSITION and STOCK_HISTORY`  \nMain Table: `CURRENT_POSITION`  \nMain Table Alias: `current_position`  \nJoins: `FILTER ON YEST_DATE` , `stock_history` , `Left`\n\n  2. Edit the Join Expressions property to add the following:\n\n**Join Expressions:  \n** current_position_Left_stock_history: `\"current_position\".\"SYMBOL\" =\n\"stock_history\".\"SYMBOL\"`\n\n!8_join_expressions\n\n  3. Update the Output Columns property to add the following:\n\n**Output Columns:  \n** current_position.TRADER: `TRADER`  \ncurrent_position.SYMBOL: `SYMBOL`  \ncurrent_position.sum_INVESTMENT: `sum_INVESTMENT`  \ncurrent_position.sum_SHARESBOUGHT: `sum_SHARESBOUGHT`  \ncurrent_position.sum_RETURN: `sum_RETURN`  \ncurrent_position.sum_SHARESSOLD: `NET_SHARES`  \ncurrent_position.NET_SHARES: `NET_SHARES`  \ncurrent_position.NET_VALUE: `NET_VALUE`  \ncurrent_position.AVG_PRICE: `AVG_PRICE`  \nstock_history.CLOSE: `CLOSE`\n\n!8_output_col\n\n  4. Click **OK**\n\n!8_join_prop\n\nThe job flow now looks like this:\n\n!8_join_flow\n\nLet's now calculate the realized and unrealized gains/losses for each trader.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ceef73dc-f07e-4c71-b419-444d35e5e383": {"__data__": {"id_": "ceef73dc-f07e-4c71-b419-444d35e5e383", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fb8747b-1767-4289-94bb-077baed77c24", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "507ecc37a1fc65554d1d64ba309067721adc311bac64088013ad11f43e037aa1", "class_name": "RelatedNodeInfo"}}, "text": "Calculator \\- Calculate\nRealized & Unrealized Gains\n\n  1. Locate the calculator component. Drag and drop it to the end of the flow and connect it to the Join component.\n\n!8_calc\n\n  2. Click on the Calculator component and edit the Properties as follows:\n\nName: `GAINS`  \nInclude Input Columns: `Yes`\n\n  3. Edit the Calculations property, and add the following expressions.\n\n**Expressions:  \n** UNREAL_GAINS: `(\"NET_SHARES\" * \"CLOSE\") - (\"NET_SHARES\" * \"AVG_PRICE\")`  \nREAL_GAINS: `CASE WHEN \"NET_SHARES\" = 0 THEN \"NET_VALUE\" ELSE \"sum_INVESTMENT\"\n- (\"sum_SHARESSOLD\" * \"AVG_PRICE\") END`\n\n!8_real\n\n!8_unreal\n\n!8_gains_flow\n\nThe flow should now look like this:\n\n!8_gains_flow2\n\n  4. Now, let's write the results to a new table called TRADER_PNL_TODAY using the Rewrite Table component.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "068d4911-a197-4c66-83a0-c353e89fdebf": {"__data__": {"id_": "068d4911-a197-4c66-83a0-c353e89fdebf", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44e31ce9-c501-4c3d-8663-239a6a9aeebb", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "402e2109b2798958e62cb6f565c5d649091fc03180cecca48d66428af01919f6", "class_name": "RelatedNodeInfo"}}, "text": "Rewrite Table\n\n  1. Locate the Rewrite Tablecomponent and drag and drop into the workspace. Link it to the GAINS calculator, and click to edit the Properties as follows:\n\nName: `TRADER_PNL_TODAY`  \nTarget Table: `TRADER_PNL_TODAY`\n\n!8_pnl_today\n\nThe flow should now look like this:\n\n!8_pnl_flow\n\n  2. Now, locate a Aggregate component to it connect to the Calculator GAINS component, creating a parallel flow.\n\n!8_para_flow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c13691d8-0a4f-458a-bb29-a03df2027d5a": {"__data__": {"id_": "c13691d8-0a4f-458a-bb29-a03df2027d5a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de1ce3c1-eb1c-4fd8-b5e4-8a0f8e85f417", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "23239bfd39dc597a3aab1ecad0214342bd7dca90e2cee7e4f0015e9fc735c657", "class_name": "RelatedNodeInfo"}}, "text": "Aggregate \\- Sum up the\ngains, both realized and unrealized by each trader\n\n  1. Click on the Aggregate component and edit the Properties as follows:\n\nName: `SUM GAINS PER TRADER`  \nGroupings: `TRADER`  \nAggregations: `UNREAL_GAINS, Sum` , `REAL_GAINS, Sum`\n\n!8_agg_prop\n\nThe flow should now look like this:\n\n!8_agg_flow\n\nFinally, we are going to create a view to store this last aggregation result.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c366fd1-8c9b-4938-8638-c456f5fcca6a": {"__data__": {"id_": "8c366fd1-8c9b-4938-8638-c456f5fcca6a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ae2ac07-2489-4f80-84d2-a7304d412185", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3b31553f70c48fb03603202c295f61f96ebc94f0255387785f4d4ba89707604f", "class_name": "RelatedNodeInfo"}}, "text": "Create View \\- Write\nthe trader and gains fields to a new view in Snowflake\n\n  1. Locate the Rewrite Table component and drag and drop it to connect to the SUM GAINS PER TRADER Aggregate component.\n  2. Click on the component and edit the Properties as follows:\n\nName: `TRADER_PNL_TOTAL_VIEW`  \nTarget Table: `TRADER_PNL_TOTAL_VIEW`\n\n!8_rewrite_today\n\nThe final flow of the job, should look like this:\n\n!8_today_flow\n\nYou can check the datasets either with the Matillion sample function or go to\nSnowflake UI. There should be two tables created TRADER_PNL_TODAY and\nTRADER_PNL_TOTAL_VIEW.\n\n!8_snowflake_today", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8fdec053-3cc2-4fae-86a1-028cd2b4dc8e": {"__data__": {"id_": "8fdec053-3cc2-4fae-86a1-028cd2b4dc8e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e55948c6-d85c-420c-b00f-28fb5ca2c259", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "402b7f4962fd396ba0f7bda1c55fd90a40dde6ae18df3d67a1d5c09eb5aced62", "class_name": "RelatedNodeInfo"}}, "text": "Completing the Orchestration Job:\n\nReturn back to the VHOL_orchestration job, and drag and drop an **Alter\nWarehouse** component as the final step, linked to the VHOL_PNL_xform\nTransformation component.\n\n_Pro tip: you can also COPY and PASTE the other Alter Warehouse component to\njust edit it._\n\nEdit the component to reflect as follows:\n\nName: | `Size Down Warehouse to XS`  \n---|---  \nCommandType: | `Set`  \nProperties: | `WAREHOUSE_SIZE XSMALL`  \n  \nThis will scale down your Virtual Warehouse after the orchestration job is\ncompleted.\n\n!8_alter_final\n\nYour final pipeline result should now look like this:\n\n!8_final_flow\n\nRight click anywhere on the workspace click **Run Job** to run the job and\nenjoy seeing the data being loaded, transformed, while scaling up and down\nSnowflake warehouse dynamically!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0edef6de-a2c6-4b04-8719-c35191181190": {"__data__": {"id_": "0edef6de-a2c6-4b04-8719-c35191181190", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "538fe6aa-ce33-4c43-ab73-74f921399f3d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "67945af03ee96b0ba167d0f4e01cafcf7228ae760fb1e12e39979add1a4bfa96", "class_name": "RelatedNodeInfo"}}, "text": "9\\. BONUS - Daily Updates from Yahoo! Finance\n\nThe portfolio manager wants up-to-date stock information to know exactly where\ntheir realized and unrealized gains stand. Utilizing Matillion's Universal\nConnectivity feature they can pull real-time market prices and make the\ncalculation.\n\n  1. Begin by right-clicking in the Project Explorer and select Add Orchestration job to create a new Orchestration job. Name it Yahoo_Orch.\n  2. Righ click on the canvas, and click Manage Grid Variables.\n\n!9_1\n\n  3. Create a Grid Variable called `gv_tickers`, with a single column (`gvc_tickers`) populated with: AAPL and SBUX.\n\n!9_2\n\n  4. Click **Next** to add the columns AAPL and SBUX.\n\n!9_3\n\n  5. Click on **Project** dropdown and select **Manage Environment Variables**\n\n!9-4\n\n  6. Create a Environment Variable called `ev_tickerlist` using the following properties:\n\nName: | `ev_tickerlist`  \n---|---  \nType: | `text`  \nBehavior: | `Copied`  \nValue: | `AAPL%2CGOOG`  \n  \n!9-5\n\n  6. Drag and drop the **Query Result to Grid** component as the first step in the flow. Fill out the component as follows:\n\nName: | `Tickers to Grid`  \n---|---  \nBasic/Advanced | `Advanced`  \nSQL Query | `SELECT DISTINCT(\"SYMBOL\") FROM \"TRADES_HISTORY\" WHERE \"TRADER\" = 'CERSEI' LIMIT 10`  \nGrid Variable | `gv_tickers`  \nGrid Variable Mapping | `gvc_tickers: SYMBOL`  \n  \n!9-6\n\n!9-7", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9e04ebb0-a7bb-46da-9e77-ab856d3142db": {"__data__": {"id_": "9e04ebb0-a7bb-46da-9e77-ab856d3142db", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef50c065-26ed-4c35-adc7-a4396ccb7ef4", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ccea9c8f03d9fcfef9e3c653866e5d007827155e175fa060d3c9028f1aaa458b", "class_name": "RelatedNodeInfo"}}, "text": "Python Script\n\nWe will incorporate a Python script to \"unpack\" the Grid Variable set in the\nnext step. With the stock symbols saved to a variable called loc_TICKERS, a\nloop will be performed to reformat a query parameter needed for a call to the\nYahoo! Finance quote endpoint.\n\n  1. Locate the **Python Script** component and drop as the last step in the flow:\n\n!9-8\n\n  2. Update the Python Script component with the following:\n\n**Script** :\n\n    \n    \n    print (context.getGridVariable('gv_tickers'))\n    loc_TICKERS = context.getGridVariable('gv_tickers')\n    \n    api_param = ''\n    \n    for layer1 in loc_TICKERS:\n      for each in layer1:\n        api_param = api_param + each + '%2C'\n        #print(each) validate unpackaging of array\n        \n    api_param = api_param[:-3]\n    print(api_param)\n    \n    context.updateVariable('ev_tickerlist', api_param)\n    \n    print(ev_tickerlist)\n    \n\n**Interpeter:**`Python 3`", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6beef081-ee2a-4e8a-81f1-0e597a7c2668": {"__data__": {"id_": "6beef081-ee2a-4e8a-81f1-0e597a7c2668", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ccfc5bef-2c66-49ea-8648-74470273d1d1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "866e1c9b061f6308673f610c8c4950774409ac44d400768eeaeed2a89dcbed3f", "class_name": "RelatedNodeInfo"}}, "text": "API-Extract \\- Pull\nCurrent Stock Price Data\n\n  1. From the Projects drop down in the top left, select **Manage API Profiles** > **Manage Extract Profiles**.\n\n!9-9\n\n  2. Add a new **Extract Profile** using the information below:\n\n**Profile Name:**`YahooFinance`\n\n!9-10\n\n  3. Click **Ok** and select **New Endpoint** , and update with the following information:\n\n**Endpoint Name:**`QuotesByTicker`\n\n  4. Click **Next**.\n  5. Set the Endpoint Configuration GET to the following URI: `https://yfapi.net/v6/finance/quote`\n  6. Select the **Params** tab and update with the following:\n\n**Params:**\n\n| |   \n---|---|---  \n`lang` | `en` | `Query`  \n`region` | `US` | `Query`  \n`symbols` | `AAPL,BTC-USD,EURUSD=X` | `Query`  \n`X-API-KEY` | **`SEE NOTE BELOW`** | `Header`  \n  \n**NOTE:** Your X-API-KEY must be obtained from Yahoo Finance API (This can be\nretrieved by following the instructions\nHERE)\n\n  7. Click **Next** and **Finish** to complete creating the Endpoint.\n  8. Locate the **API Extract** component and place is after the **Python Script** component.\n\n!9-11\n\n  9. Update the component as follows:\n\n|  \n---|---  \nProfile | `YahooFinance`  \nData Source | `QuotesByTicker`  \nQuery Params | `lang - en`  \n| `region - US`  \n| `symbols - ${ev_tickerlist}`  \nHeader Params | **`YOUR API KEY`**  \nLocation | `Select the default S3 bucket provided by Partner Connect`  \nTable | `VHOL_YAHOORAW`  \n  \n  10. Now we will create a new Transformation Job - Yahoo Transform - which will sit as the next step after the Yahoo Orchestration job just worked on.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "042925c9-6d16-4844-a556-030f20972bf3": {"__data__": {"id_": "042925c9-6d16-4844-a556-030f20972bf3", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78a62381-83b1-4285-82d2-7560cfc42b6d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "435c4d967430b0efaab9450f63b6cc53fa6cd3de203316412119ce0bf8cdc85e", "class_name": "RelatedNodeInfo"}}, "text": "Transformation - Yahoo Transform\n\n  1. Within the Projects Explorer, right click and select **Add Transformation Job**. Name it Yahoo_transform.\n  2. Create a new Table Input component and update as follows:\n\n| |   \n---|---|---  \nName | `VHOL_YAHOORAW` |   \nTarget Table | `VHOL_YAHOORAW` |   \nColumn Names | `Data Value` |   \n  \n!9-12", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "90167495-0368-42a9-8fdf-c74274f2a9e4": {"__data__": {"id_": "90167495-0368-42a9-8fdf-c74274f2a9e4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db699eaf-ef94-463f-a91f-cef0e7bec50b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d9a985066847707f85e043f923ea52097b320620a162ee9f8a4ab164144ab88f", "class_name": "RelatedNodeInfo"}}, "text": "Extract Nested Data \\-\nWe will flatten the semi structured format & extract the values needed\n\n  1. Find the **Extract Nested Data** Component and drag and drop it after the Table Input.\n  2. Update the component as follows:\n\n| |   \n---|---|---  \nName | `Extract Nested Data` |   \nInclude Input Column | `No` |   \n  \nColumns: Select **Autofill** , to populate all the available columns and\nselect the following values:\n\n`displayName, regularMarketPrice, symbol`\n\n!9-13", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9726bc30-5961-431d-b58a-16f0023b6e76": {"__data__": {"id_": "9726bc30-5961-431d-b58a-16f0023b6e76", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "912a9072-7a67-4441-8ede-0931561f03de", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e774684d334f70c0edf395fd3a4235062f20d1ed2a5a4f8d000bcfef84b885ac", "class_name": "RelatedNodeInfo"}}, "text": "We will now read TRADER_PNL_TODAY from our previous transformation job.\n\n  1. Locate the **Table Input** component and place is underneath the previous Table Input. And update the properties as follows:\n\n| |   \n---|---|---  \nName: | `TRADER_PNL_TODAY` |   \nTarget Table: | `TRADER_PNL_TODAY` |   \nColumn Names: | Select all columns |   \n  \n!9-14", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8175030f-2acd-4a52-8dee-aa8dfc5954bb": {"__data__": {"id_": "8175030f-2acd-4a52-8dee-aa8dfc5954bb", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b7a554c-7339-420d-bc4a-9851472bd4ab", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cb0382f1735ced2f58f4bbf2946b72572866ee9f04ead70103483c486e143813", "class_name": "RelatedNodeInfo"}}, "text": "Now we will only filter for Cersei's trades by using the Filter component.\n\n  1. Locate the **Filter** component and connect it to the table input from the previous step. And update the properties as follows:\n\n| |   \n---|---|---  \nName: | `Cersei's Trades` |   \nInput Column: | `Trader` |   \nQualifier: | `Is` |   \nComparator: | `Value` |   \nValue: | `CERSEI` |   \n  \n!9-15", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ebc7e6f9-42dd-4c4f-8b97-c6aa52e92a18": {"__data__": {"id_": "ebc7e6f9-42dd-4c4f-8b97-c6aa52e92a18", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20f5fae9-e9ca-4a02-92c6-957811cc5480", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "33a1df7c942d582daa6fb413e5e3e16455748d8d3124209626f4ae73de7060e5", "class_name": "RelatedNodeInfo"}}, "text": "Now we will join Cersei's trades with the Yahoo API data using the Join\ncomponent.\n\n  1. Locate the **Join** component and connect to the Extract Nested Data and Cersei's Trades, and update the properties as follows:\n\n| |   \n---|---|---  \nName: | Join |   \nMain Table: | `Cersei's Trades` |   \nMain Table Alias: | `cersei` |   \nJoins: | `Joins Table - Extract Nested Data` |   \n| `Join Alias - trades` |   \n| `Join Type - Inner` |   \n  \n**Join Expressions:**\n\n| |   \n---|---|---  \ncersei_inner_trades | `\"cersei\".\"SYMBOL\" = \"trades\".\"symbol\"` |   \n  \n**Output Columns:**\n\n| |   \n---|---|---  \n`cersei.TRADER` | `TRADER` |   \n`cersei.SYMBOL` | `SYMBOL` |   \n`trades.regularMarketPrice` | `MARKETPRICE` |   \n`cersei.AVG_PRICE` | `AVG_PRICE` |   \n`cersei.NET_SHAERES` | `# SHARES` |   \n  \n!9-16", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f07d091-422a-470f-a382-75a192f7b20b": {"__data__": {"id_": "5f07d091-422a-470f-a382-75a192f7b20b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17a26717-b9ce-4080-93e4-d6d7b8327b1a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "0c0d8b1234c95b7d65b8300d9119dbf3cd3bb7b37189bc1ac61d4645f2f97b9f", "class_name": "RelatedNodeInfo"}}, "text": "Calculate the Win / Loss Logic\n\n  1. Drag and drop the Calculator component as the last step of the flow and update as follows:\n\n| |   \n---|---|---  \nName: | `Calculator` |   \nExpressions: |   \n---|---  \n`MARKET_VALUE` | `\"# SHARES\" * \"MARKETPLACE\"`  \n`PORTFOLIO_VALUE` | `\"AVG_PRICE\" * \"# SHARES\"`  \n`UNREALIZED_GAINS` | `(\"AVG_PRICE\" * \"# SHARES\") - (\"# SHARES\" * \"MARKETPLACE\")`  \n  \n!9-17\n\nFinally, we will write Cersei's profits back to Snowflake using the\n**Rewrite** component, and update as follows:\n\n|  \n---|---  \nName: | `CERSEI PROFITS`  \nTarget Table: | `CERSEI PROFITS`  \n  \nYour final flow should now look like this:\n\n!9-18\n\nWhat this shows is the stock, quantity, and the real time average price of\neach stock. The resulting table is how much realized gains Cersei can expect\nbased on the quantity of shares she owns. You can check the data in Snowflake\nto see that it was written correctly:\n\n!9-19", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "00b183b0-f5f8-4274-9c1c-7231d917af0c": {"__data__": {"id_": "00b183b0-f5f8-4274-9c1c-7231d917af0c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4431a0b2-ff50-474a-87be-251ac8adefdd", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1a7f23a1dd9ba87621705412adfa09263332b0468d0906c81fca30f87304af32", "class_name": "RelatedNodeInfo"}}, "text": "10\\. Conclusion\n\nCongrats! You have successfully developed a well-orchestrated data engineering\npipeline!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e148fd4-0955-4a37-8a25-8c8ce265aaf2": {"__data__": {"id_": "0e148fd4-0955-4a37-8a25-8c8ce265aaf2", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08c578ed-4907-4a25-9475-a0c0946a2e29", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "40eed7150f10060482aa7093ebc6baba0c43af220fff68719cc292afcb729f1c", "class_name": "RelatedNodeInfo"}}, "text": "What we have covered\n\n  * Source 3rd party data from Snowflake Marketplace\n  * Use Matillion's GUI to build end-to-end transformation pipeline\n  * Leverage Matillion scale up/down Snowflake's virtual warehouses\n\nUsing Matillion ETL for Snowflake we were able to easily extract data from S3,\nperform complex joins, filter and aggregate through an intuitive, browser\nbased, easy to use UI. If we were to have used traditional ETL tools, it would\nhave required a lot code, resources, and time to complete.\n\nMatillion ETL makes data engineering easier by allowing you to build your data\npipelines more efficiently with a low-code/no-code platform built for the Data\nCloud. We can build complex data pipelines to scale up and down within\nSnowflake based on your workload profile.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f730207f-749d-477b-af7a-3a81cc7c21bf": {"__data__": {"id_": "f730207f-749d-477b-af7a-3a81cc7c21bf", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89d38fdf-8098-4191-8d54-80f5a9088055", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7bc2654ef0f8d687de9a4528517b54955f06b95af8831dbbc6febb0f780c5dd3", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Snowflake Docs\n  * Matillion Docs\n\nBack\n\nNextDone\n\n!\n!\n\n![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=72fa2dc4-3c09-489b-bcf4-c721e7536ee6&sid=902d1760865a11efaab70706947e3446&vid=902d3020865a11ef9f9677c9c928fab9&vids=1&msclkid=N&pi=0&lg=en-\nUS&sw=1800&sh=1169&sc=30&nwd=1&tl=Cloud%20Native%20Data%20Engineering%20with%20Matillion%20and%20Snowflake&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fcloud_native_data_engineering_with_matillion_and_snowflake%2F%230&r=&lt=786&evt=pageLoad&sv=1&cdb=AQAQ&rn=130274)!\n\n!\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf05d258-264e-4544-aaff-749b1390b51d": {"__data__": {"id_": "bf05d258-264e-4544-aaff-749b1390b51d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22e42d19-9c1f-4972-bdf6-a919f6adb096", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9e6a8133cc801d45aa3b8a8e0567013c5ac578b2020d0b4828f0b6c0af527ee6", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Use of Cookies\n\nWe use cookies to enhance your experience and to analyze site traffic as\ndescribed in our Cookie Statement. By accepting, you consent to our use of\ncookies.[Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-\nstatement/)\n\nCookies Settings Reject All Accept All Cookies\n\n![Company\nLogo](https://cdn.cookielaw.org/logos/cb85e692-4053-4d0a-8dda-d24b5daa8b06/ff6c124b-1473-4861-9ca3-9eaf6debb37d/SNO-\nSnowflakeLogo_blue.png)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64b973f3-b332-4da1-b068-945986a9e3b6": {"__data__": {"id_": "64b973f3-b332-4da1-b068-945986a9e3b6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73f1c556-d756-498c-9334-4f18bb8743fd", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4c97db1032c391e2c0024a244b6f380e28404ec5c9dcf1f2921fadd9d7f4e0a9", "class_name": "RelatedNodeInfo"}}, "text": "Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58beacb6-5cdf-47c8-b231-9a646b86ebd4": {"__data__": {"id_": "58beacb6-5cdf-47c8-b231-9a646b86ebd4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f7fefea-5060-47f4-b742-7dcd757dc6ce", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c5696d8508a2da50475c0892b868cad3b571752bb28c261c7f46faf8c718feea", "class_name": "RelatedNodeInfo"}}, "text": "Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.  \nMore information", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "188baac2-b610-401a-aae4-c3b009c292ec": {"__data__": {"id_": "188baac2-b610-401a-aae4-c3b009c292ec", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "420c7222-1f01-4417-be2e-acbe12b5f0b6", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b2d13363bc920e1ab2025c01527624b7c909f35b6b6ee81f46a887e696770b15", "class_name": "RelatedNodeInfo"}}, "text": "Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff. They are usually only set in response to actions made by you which amount\nto a request for services, such as setting your privacy preferences, logging\nin or filling in forms. You can set your browser to block or alert you about\nthese cookies, but some parts of the site will not then work. These cookies do\nnot store any personally identifiable information.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "615c8611-8c13-4c60-be41-077e144055e2": {"__data__": {"id_": "615c8611-8c13-4c60-be41-077e144055e2", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d55814fa-8638-4141-8418-bfeb18d7ec28", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e7c3c7a872d2bcd2365b2a4d3973284fd6609001a8f9456e8fc3c8ded7183a7a", "class_name": "RelatedNodeInfo"}}, "text": "Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.    All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38f88f33-7cc5-456e-93c8-37d1881ff767": {"__data__": {"id_": "38f88f33-7cc5-456e-93c8-37d1881ff767", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "508ab91a-2f2e-4a50-a8b9-7c203c9c9e11", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "073932a32110b840684f155c55245a9945a646b17796afc6d2c8e1491065a373", "class_name": "RelatedNodeInfo"}}, "text": "Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages.    If you do not allow these cookies then\nsome or all of these services may not function properly.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f3b7b744-3f39-46c1-8c89-9109bbed095b": {"__data__": {"id_": "f3b7b744-3f39-46c1-8c89-9109bbed095b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "691f3498-b4b5-471b-9f41-1cdc3e0f13fd", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "15e0f5ee52599c93bb41e9e7e3e1c548492b7a286e579eee0503620fb11f2fa8", "class_name": "RelatedNodeInfo"}}, "text": "Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly identifiable\npersonal information, but are based on uniquely identifying your browser and\ninternet device. If you do not allow these cookies, you will experience less\ntargeted advertising.\n\nCookies Details\u200e\n\nBack Button", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c03dcf62-dfa5-42cc-98a3-b4f31122381c": {"__data__": {"id_": "c03dcf62-dfa5-42cc-98a3-b4f31122381c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a7ec057-f8cc-49aa-ae5c-5aecd2755502", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "36cce28e542a1809b74c9a28cbb1af2d69b3c10222cfa9227dc1896bf7509a10", "class_name": "RelatedNodeInfo"}}, "text": "Cookie List\n\nFilter Button\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[![Powered by\nOnetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-\nconsent/)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d4e0e90-6ce3-4474-84a8-8831602a5859": {"__data__": {"id_": "2d4e0e90-6ce3-4474-84a8-8831602a5859", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07c63003-cb6b-4d7d-88cd-8c8d85dd6a13", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d0ea7505cfeaa1f50694c704143374ef889549c733ce6544b7ad94fcb2dca54b", "class_name": "RelatedNodeInfo"}}, "text": "1. Overview\n  2. Quickstart Setup\n  3. Setup Snowflake\n  4. Load Raw\n  5. Load Weather\n  6. Create POS View\n  7. Fahrenheit to Celsius UDF\n  8. Orders Update Sproc\n  9. Daily City Metrics Update Sproc\n  10. Orchestrate Jobs\n  11. Process Incrementally\n  12. Deploy Via CI/CD\n  13. Teardown\n  14. Conclusion\n\n[ _bug_report_ Report a mistake](https://github.com/Snowflake-\nLabs/sfguides/issues)\n\n _close_ _menu_", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "52b8265f-1227-453b-9d58-fcd43121ef5f": {"__data__": {"id_": "52b8265f-1227-453b-9d58-fcd43121ef5f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43fae96a-eeea-4793-b587-dcfa3bd5d161", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2382b4beb3cfd8e70446b5b38de0d40ea815cbf0ab3edf61a5ebac2acc8c2b3f", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering Pipelines with Snowpark Python\n\n _access_time_ 130 mins remaining", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8040c4aa-f781-4c72-82bc-7c0a91f57eb0": {"__data__": {"id_": "8040c4aa-f781-4c72-82bc-7c0a91f57eb0", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f2de0fd-1545-464f-af39-77c2b309ef45", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4842b99877000675e49e6354f41902d9958f8ebf38314d971493c50e3b2328a8", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Overview\n\n\"Data engineers are focused primarily on building and maintaining data\npipelines that transport data through different steps and put it into a usable\nstate ... The data engineering process encompasses the overall effort required\nto create **data pipelines** that automate the transfer of data from place to\nplace and transform that data into a specific format for a certain type of\nanalysis. In that sense, data engineering isn't something you do once. It's an\nongoing practice that involves collecting, preparing, transforming, and\ndelivering data. A data pipeline helps automate these tasks so they can be\nreliably repeated. It's a practice more than a specific technology.\" (From\nCloud Data Engineering for Dummies, Snowflake Special Edition)\n\nAre you interested in unleashing the power of Snowpark Python to build data\nengineering pipelines? Well then, this Quickstart is for you! The focus here\nwill be on building data engineering pipelines with Python, and not on data\nscience. For examples of doing data science with Snowpark Python please check\nout our [Machine Learning with Snowpark Python: - Credit Card Approval\nPrediction](https://quickstarts.snowflake.com/guide/getting_started_snowpark_machine_learning/index.html?index=..%2F..index#0)\nQuickstart.\n\nThis Quickstart will cover a lot of ground, and by the end you will have built\na robust data engineering pipeline using Snowpark Python stored procedures.\nThat pipeline will process data incrementally, be orchestrated with Snowflake\ntasks, and be deployed via a CI/CD pipeline. You'll also learn how to use\nSnowflake's new developer CLI tool and Visual Studio Code extension! Here's a\nquick visual overview:\n\n!\n\nSo buckle up and get ready!\n\n**Note** \\- As of 3/7/2024, the [SnowCLI Tool](https://github.com/Snowflake-\nLabs/snowcli) is still in preview.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9ccbe0f-301e-479d-b543-2d6efc573db3": {"__data__": {"id_": "d9ccbe0f-301e-479d-b543-2d6efc573db3", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e2e1c9f-ba94-4652-8c35-1a29af359be3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "74f5629d7a1fe711f246d6deda6738239916d46a16cf6ac054f939bb5e7f9dfc", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\n  * Familiarity with Python\n  * Familiarity with the DataFrame API\n  * Familiarity with Snowflake\n  * Familiarity with Git repositories and GitHub", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ce5e8d4-78c2-417a-afa0-2001e6c46de6": {"__data__": {"id_": "6ce5e8d4-78c2-417a-afa0-2001e6c46de6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93f00707-9930-46d8-9f06-0fe4c663b71e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "8dbd2b787b3c301c391c41133f2f648c8a9c34ce8a33775accea47b47ce50a26", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\nYou will learn about the following Snowflake features during this Quickstart:\n\n  * Snowflake's Table Format\n  * Data ingestion with COPY\n  * Schema inference\n  * Data sharing/marketplace (instead of ETL)\n  * Streams for incremental processing (CDC)\n  * Streams on views\n  * Python UDFs (with third-party packages)\n  * Python Stored Procedures\n  * Snowpark DataFrame API\n  * Snowpark Python programmability\n  * Warehouse elasticity (dynamic scaling)\n  * Visual Studio Code Snowflake native extension (PuPr, Git integration)\n  * SnowCLI (PuPr)\n  * Tasks (with Stream triggers)\n  * Task Observability\n  * GitHub Actions (CI/CD) integration", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30731fcb-d003-4657-b818-4b4fb419f461": {"__data__": {"id_": "30731fcb-d003-4657-b818-4b4fb419f461", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14fbac6c-813b-4aa2-9bdd-bd28b6bf26de", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "619249cfe1506ba151f1252fdfadd66527959e9a24c9891d2d01b2529eede8da", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  * Snowflake account \n    * **A Snowflake Account**\n    * **A Snowflake user created with ACCOUNTADMIN permissions**. This user will be used to get things setup in Snowflake.\n    * **Anaconda Terms & Conditions accepted**. See Getting Started section in Third-Party Packages.\n  * GitHub account \n    * **A GitHub account**. If you don't already have a GitHub account you can create one for free. Visit the Join GitHub page to get started.\n\nNote: You can now run this entire quickstart as a Snowflake Notebook in your\nSnowsight UI. Download this [ipynb file](https://github.com/Snowflake-\nLabs/snowflake-demo-\nnotebooks/blob/main/Data%20Engineering%20Pipelines%20with%20Snowpark%20Python/Data%20Engineering%20Pipelines%20with%20Snowpark%20Python.ipynb)\nand import it to your Snowflake account as shown below.\n\n!notebook_callout", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "77c56b04-ca29-4acc-939d-d5d45197091c": {"__data__": {"id_": "77c56b04-ca29-4acc-939d-d5d45197091c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad700b0f-aa45-43ae-83a2-be28190729d3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "537dd3dec5470a8ba7dbe058a64d0ff57318d4a20feed640f929aa0ae424bc29", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\nDuring this Quickstart you will accomplish the following things:\n\n  * Load Parquet data to Snowflake using schema inference\n  * Setup access to Snowflake Marketplace data\n  * Create a Python UDF to convert temperature\n  * Create a data engineering pipeline with Python stored procedures to incrementally process data\n  * Orchestrate the pipelines with tasks\n  * Monitor the pipelines with Snowsight\n  * Deploy the Snowpark Python stored procedures via a CI/CD pipeline", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1f0517d6-21a0-4d6c-8925-be1034d8a24f": {"__data__": {"id_": "1f0517d6-21a0-4d6c-8925-be1034d8a24f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f5ad8b7-1624-4629-ace6-bcacee58fd7a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "87ce90e90ddcfee6fd32d3c357d68aa11b6a0ca779d10c257518961833ff69b9", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Quickstart Setup\n\nNote: You can now run this entire quickstart as a Snowflake Notebook in your\nSnowsight UI. Download this [ipynb file](https://github.com/Snowflake-\nLabs/snowflake-demo-\nnotebooks/blob/main/Data%20Engineering%20Pipelines%20with%20Snowpark%20Python/Data%20Engineering%20Pipelines%20with%20Snowpark%20Python.ipynb)\nand import it to your Snowflake account as shown below.\n\n!notebook_callout", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1335defa-aa12-4033-aeea-4a2305b37ce9": {"__data__": {"id_": "1335defa-aa12-4033-aeea-4a2305b37ce9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d573fc26-40e4-4053-8a73-065d4e7195a2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3137f1154f0abd265b35d1dbed3ba2487df2433619b3eddf52b271d8d659363b", "class_name": "RelatedNodeInfo"}}, "text": "Fork the Quickstart Repository and Enable GitHub Actions\n\nYou'll need to create a fork of the repository for this Quickstart in your\nGitHub account. Visit the [Data Engineering Pipelines with Snowpark Python\nassociated GitHub Repository](https://github.com/Snowflake-Labs/sfguide-data-\nengineering-with-snowpark-python) and click on the \"Fork\" button near the top\nright. Complete any required fields and click \"Create Fork\".\n\nBy default GitHub Actions disables any workflows (or CI/CD pipelines) defined\nin the forked repository. This repository contains a workflow to deploy your\nSnowpark Python UDF and stored procedures, which we'll use later on. So for\nnow enable this workflow by opening your forked repository in GitHub, clicking\non the `Actions` tab near the top middle of the page, and then clicking on the\n`I understand my workflows, go ahead and enable them` green button.\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e922d88a-e3d3-497b-a825-404210e20d1c": {"__data__": {"id_": "e922d88a-e3d3-497b-a825-404210e20d1c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f9b070-a014-4040-a651-7bdd7ceead4d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7392ccf5dec7adff94ddf5220d360df9b2412a9e4f99f29a7d3d7f2b392419d2", "class_name": "RelatedNodeInfo"}}, "text": "Create GitHub Codespace\n\nFor this Quickstart we will be using [GitHub\nCodespaces](https://docs.github.com/en/codespaces/overview) for our\ndevelopment environment. Codespaces offer a hosted development environment\nwith a hosted, web-based VS Code environment. GitHub currently offers [60\nhours for free each month](https://github.com/features/codespaces) when using\na 2 node environment, which should be more than enough for this lab.\n\nTo create a GitHub Codespace, click on the green ` Code` button from the\nGitHub repository homepage. In the Code popup, click on the `Codespaces` tab\nand then on the green `Create codespace on main`.\n\n!\n\nThis will open a new tab and begin setting up your codespace. This will take a\nfew minutes as it sets up the entire environment for this Quickstart. Here is\nwhat is being done for you:\n\n  * Creating a container for your environment\n  * Installing Anaconda (miniconda)\n  * Creating a directory and default config files\n  * Anaconda setup \n    * Creating the Anaconda environment\n    * Installing the Snowpark Python library\n    * Installing the SnowCLI Python CLI\n  * VS Code setup \n    * Installing VS Code\n    * Configuring VS Code for the Python Anaconda environment\n    * Installing the Snowflake VS Code extension\n  * Starting a hosted, web-based VS Code editor\n\nOnce the codepsace has been created and started you should see a hosted web-\nbased version of VS Code with your forked repository set up! Just a couple\nmore things and we're ready to start.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "418eb099-3ef3-44c1-bf97-c592e09e8579": {"__data__": {"id_": "418eb099-3ef3-44c1-bf97-c592e09e8579", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8abe616-ae9a-45ee-9812-de5e3ec23f53", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "8c2a589d61b93aa6a7caab054a09b14faa7c99ec57cb0168919e7b64fe42b98f", "class_name": "RelatedNodeInfo"}}, "text": "Configure Snowflake Credentials\n\nBoth the [Snowflake Connector for\nPython](https://docs.snowflake.com/en/developer-guide/python-connector/python-\nconnector) and the SnowCLI tool use the same configuration files, which can be\nfound in the `~/.snowflake` folder. Default configuration files were created\nfor you during the codespace setup.\n\nFor this Quickstart you'll only need to edit the\n`~/.snowflake/connections.toml` file. The easiest way to edit the default\n`~/.snowflake/connections.toml` file is directly from VS Code in your\ncodespace. Type `Command-P`, type (or paste) `~/.snowflake/connections.toml`\nand hit return. The config file should now be open. You just need to edit the\nfile and replace the `account`, `user`, and `password` with your values. Then\nsave and close the file.\n\n**Note:** The SnowCLI tool (and by extension this Quickstart) currently does\nnot work with Key Pair authentication. It simply grabs your username and\npassword details from the config file.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c54409ad-989e-49fd-ad89-ed4570c0a31d": {"__data__": {"id_": "c54409ad-989e-49fd-ad89-ed4570c0a31d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b20fb21f-1865-4b97-9360-ad5e7fe7887c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "da76843f3192eaa84aadb39437e6ae3833fc8a5c119fc673db2e4eb4f70d55e6", "class_name": "RelatedNodeInfo"}}, "text": "Verify Your Anaconda Environment is Activated\n\nDuring the codespace setup we created an Anaconda environment named\n`snowflake-demo`. And when VS Code started up it should have automatically\nactivated the environment in your terminal. You should see something like this\nin the terminal, and in particular you should see `(snowflake-demo)` before\nyour bash prompt.\n\n!\n\nIf for some reason it wasn't activiated simply run `conda activate snowflake-\ndemo` in your terminal.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca00c42d-0770-48b7-ae2b-4f1c781e8d4c": {"__data__": {"id_": "ca00c42d-0770-48b7-ae2b-4f1c781e8d4c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc8b1baa-3cfc-46c7-8ce5-2d2b3346356c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3c677c521ffbd0e01e45aebcc11d2a493e0953824c8d9bde044dbb5109573f1e", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Setup Snowflake", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 21, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f2dceb09-3e69-472c-bc53-e826db2be258": {"__data__": {"id_": "f2dceb09-3e69-472c-bc53-e826db2be258", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "503f0a38-5972-4d79-95ff-23d521f6cb1c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d5bdc39baedba82cbb0ceac4cebd7e747ab90b7e6bf41fe01bc31988ecee9134", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake Extensions for VS Code\n\nYou can run SQL queries against Snowflake in many different ways (through the\nSnowsight UI, SnowSQL, etc.) but for this Quickstart we'll be using the\nSnowflake extension for VS Code. For a brief overview of Snowflake's native\nextension for VS Code, please check out our [VS Code Marketplace Snowflake\nextension\npage](https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-\nvsc).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "71f5e318-a189-4fde-b39f-f947426ac059": {"__data__": {"id_": "71f5e318-a189-4fde-b39f-f947426ac059", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c423d2f-9155-4291-b662-d574bf78b36a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1b2fab49ceb859b8ed54186f94a192afdfbf7fa90867dc6439f92cae85ef1f17", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo set up all the objects we'll need in Snowflake for this Quickstart you'll\nneed to run the `steps/01_setup_snowflake.sql` script.\n\nStart by clicking on the Snowflake extension in the left navigation bar in VS\nCode. Then login to your Snowflake account with a user that has ACCOUNTADMIN\npermissions. Once logged in to Snowflake, open the\n`steps/01_setup_snowflake.sql` script in VS Code by going back to the file\nExplorer in the left navigation bar.\n\nTo run all the queries in this script, use the \"Execute All Statements\" button\nin the upper right corner of the editor window. Or, if you want to run them in\nchunks, you can highlight the ones you want to run and press CMD/CTRL+Enter.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63544937-a439-4096-8802-1ffefcfa1d9d": {"__data__": {"id_": "63544937-a439-4096-8802-1ffefcfa1d9d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "120f2c2a-99fd-417b-999d-3d57d3f141e1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a509ddc2bfc03e0ba975879478f9b287b67b33766e4244b394811368d3d1fefb", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Load Raw\n\nDuring this step we will be loading the raw Tasty Bytes POS and Customer\nloyalty data from raw Parquet files in `s3://sfquickstarts/data-engineering-\nwith-snowpark-python/` to our `RAW_POS` and `RAW_CUSTOMER` schemas in\nSnowflake. And you are going to be orchestrating this process from your laptop\nin Python using the Snowpark Python API. To put this in context, we are on\nstep **#2** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "edc709b9-24eb-40c5-856a-7a2346821c35": {"__data__": {"id_": "edc709b9-24eb-40c5-856a-7a2346821c35", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac24a6f5-5d32-40b9-aa87-38cc1e8388b4", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cf832ec8e466ae0c37319ea286dac1be71e5add0596e755b673c540825a39e8f", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo load the raw data, execute the `steps/02_load_raw.py` script. This can be\ndone a number of ways in VS Code, from a terminal or directly by VS Code. For\nthis demo you will need to execute the Python scripts from the terminal. So go\nback to the terminal in VS Code, make sure that your `snowflake-demo` conda\nenvironment is active, then run the following commands (which assume that your\nterminal has the root of your repository open):\n\n    \n    \n    cd steps\n    python 02_load_raw.py\n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30a4a5ec-6abd-40f1-9879-59d193f41e51": {"__data__": {"id_": "30a4a5ec-6abd-40f1-9879-59d193f41e51", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7137453-c90a-419f-9667-aeb74d5b922d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ee50dd863be091da1dfb89aa621a419f890f961d1e8b444ef1a026cd5110e090", "class_name": "RelatedNodeInfo"}}, "text": "Running Snowpark Python Locally\n\nIn this step you will be running the Snowpark Python code locally from your\nlaptop. At the bottom of the script is a block of code that is used for local\ndebugging (under the `if __name__ == \"__main__\":` block):\n\n    \n    \n    # For local debugging\n    if __name__ == \"__main__\":\n        # Create a local Snowpark session\n        with Session.builder.getOrCreate() as session:\n            load_all_raw_tables(session)\n    #        validate_raw_tables(session)\n\nA few things to point out here. First, the Snowpark session is being created\nwith the `Session.builder.getOrCreate()` statement. And the connection details\nused by the Snowpark library are pulled from the standard\n`~/.snowflake/connections.toml` configuration file (which you configured in\nstep 2).\n\nThen after getting the Snowpark session it calls the\n`load_all_raw_tables(session)` method which does the heavy lifting. The next\nfew sections will point out the key parts.\n\nFinally, almost all of the Python scripts in this Quickstart include a local\ndebugging block. Later on we will create Snowpark Python stored procedures and\nUDFs and those Python scripts will have a similar block. So this pattern is\nimportant to understand.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "591444bc-4af0-4aa7-af3a-b37dbdc7754f": {"__data__": {"id_": "591444bc-4af0-4aa7-af3a-b37dbdc7754f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f5b1576-4cb3-49fb-9f7a-b0f115c61c00", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "77ce969b6b4564ecee263ebd17de48d90c933dd25735d878f7a0734c6f1589c5", "class_name": "RelatedNodeInfo"}}, "text": "Viewing What Happened in Snowflake\n\nThe [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history) in Snowflake is a very power feature, that logs\nevery query run against your Snowflake account, no matter which tool or\nprocess initiated it. And this is especially helpful when working with client\ntools and APIs.\n\nThe Python script you just ran did a small amount of work locally, basically\njust orchestrating the process by looping through each table and issuing the\ncommand to Snowflake to load the data. But all of the heavy lifting ran inside\nSnowflake! This push-down is a hallmark of the Snowpark API and allows you to\nleverage the scalability and compute power of Snowflake!\n\nLog in to your Snowflake account and take a quick look at the SQL that was\ngenerated by the Snowpark API. This will help you better understand what the\nAPI is doing and will help you debug any issues you may run into.\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c24476fa-86b9-4ccd-a4d5-bbdfdc4d0891": {"__data__": {"id_": "c24476fa-86b9-4ccd-a4d5-bbdfdc4d0891", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f9fcb0b-9b01-4bcc-87f3-1f874bdd3beb", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c4d00b4bdb450bbefd535d696439aadcd1a6c4cbc212b1a1160f83792e51921f", "class_name": "RelatedNodeInfo"}}, "text": "Schema Inference\n\nOne very helpful feature in Snowflake is the ability to infer the schema of\nfiles in a stage that you wish to work with. This is accomplished in SQL with\nthe [`INFER_SCHEMA()`](https://docs.snowflake.com/en/sql-\nreference/functions/infer_schema.html) function. The Snowpark Python API does\nthis for you automatically when you call the `session.read()` method. Here is\nthe code snippet:\n\n    \n    \n        # we can infer schema using the parquet read option\n        df = session.read.option(\"compression\", \"snappy\") \\\n                                .parquet(location)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84f1bbea-95d9-47f7-894b-816ec83880f2": {"__data__": {"id_": "84f1bbea-95d9-47f7-894b-816ec83880f2", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3b26254-6fc9-4f1d-acb5-e0955092485a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6ad1ff3106a1b789ca8496f8372bc81bd1a54b66c2ada17b9005875f6601e775", "class_name": "RelatedNodeInfo"}}, "text": "Data Ingestion with COPY\n\nIn order to load the data into a Snowflake table we will use the\n`copy_into_table()` method on a DataFrame. This method will create the target\ntable in Snowflake using the inferred schema (if it doesn't exist), and then\ncall the highly optimized Snowflake [`COPY INTO `\nCommand](https://docs.snowflake.com/en/sql-reference/sql/copy-into-\ntable.html). Here is the code snippet:\n\n    \n    \n        df.copy_into_table(\"{}\".format(tname))", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "41715f74-6bb0-4894-bccc-873c3fbe9d05": {"__data__": {"id_": "41715f74-6bb0-4894-bccc-873c3fbe9d05", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "483d1b09-d12b-4cb5-89b0-41060cf45f71", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cd2be00bd9599e57d767373b60d03e45087d8dc67b6ce5f917e2748e1aa1c40a", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Table Format\n\nOne of the major advantages of Snowflake is being able to eliminate the need\nto manage a file-based data lake. And Snowflake was designed for this purpose\nfrom the beginning. In the step we are loading the raw data into a structured\nSnowflake managed table. But Snowflake tables can natively support structured\nand semi-structured data, and are stored in Snowflake's mature cloud table\nformat (which predates Hudi, Delta or Iceberg).\n\nOnce loaded into Snowflake the data will be securely stored and managed,\nwithout the need to worry about securing and managing raw files. Additionally\nthe data, whether raw or structured, can be transformed and queried in\nSnowflake using SQL or the language of your choice, without needing to manage\nseparate compute services like Spark.\n\nThis is a huge advantage for Snowflake customers.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c6ea3be-5f06-470c-8076-c639723823ce": {"__data__": {"id_": "7c6ea3be-5f06-470c-8076-c639723823ce", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3328c64-b8ea-4e21-a342-3f888c306283", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "904c648182bcd22140f2fcfe1a10e8039dec2e245a982d5e47bb31da519378a0", "class_name": "RelatedNodeInfo"}}, "text": "Warehouse Elasticity (Dynamic Scaling)\n\nWith Snowflake there is only one type of user defined compute cluster, the\nVirtual Warehouse,\nregardless of the language you use to process that data (SQL, Python, Java,\nScala, Javascript, etc.). This makes working with data much simpler in\nSnowflake. And governance of the data is completely separated from the compute\ncluster, in other words there is no way to get around Snowflake governance\nregardless of the warehouse settings or language being used.\n\nAnd these virtual warehouses can be dynamically scaled, in under a second for\nmost sized warehouses! This means that in your code you can dynamically resize\nthe compute environment to increase the amount of capacity to run a section of\ncode in a fraction of the time, and then dynamically resized again to reduce\nthe amount of capacity. And because of our per-second billing (with a sixty\nsecond minimum) you won't pay any extra to run that section of code in a\nfraction of the time!\n\nLet's see how easy that is done. Here is the code snippet:\n\n    \n    \n        _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n    \n        # Some data processing code\n    \n        _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n\nPlease also note that we included the `WAIT_FOR_COMPLETION` parameter in the\nfirst `ALTER WAREHOUSE` statement. Setting this parameter to `TRUE` will block\nthe return of the `ALTER WAREHOUSE` command until the resize has finished\nprovisioning all its compute resources. This way we make sure that the full\ncluster is available before processing any data with it.\n\nWe will use this pattern a few more times during this Quickstart, so it's\nimportant to understand.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7c1653b-bbfb-4578-ade0-c8eeabdbce73": {"__data__": {"id_": "a7c1653b-bbfb-4578-ade0-c8eeabdbce73", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6187b8ad-6bbc-485a-9982-605a29a083ff", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "828bebba62808aecfe7bc9a31933cf36792dfa4504422f1adaff40d9eae24652", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Load Weather\n\nDuring this step we will be \"loading\" the raw weather data to Snowflake. But\n\"loading\" is the really the wrong word here. Because we're using Snowflake's\nunique data sharing capability we don't actually need to copy the data to our\nSnowflake account with a custom ETL process. Instead we can directly access\nthe weather data shared by Weather Source in the Snowflake Marketplace. To put\nthis in context, we are on step **#3** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "901fc56b-8dc7-4027-a250-4c8eed419dbd": {"__data__": {"id_": "901fc56b-8dc7-4027-a250-4c8eed419dbd", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53787251-c893-45bc-9459-e85fed273463", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9893fec601833b91a8b44edab9569d3bbae712bcd19d89782deb90ec5de35514", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake Marketplace\n\nWeather Source is a leading provider of global weather and climate data and\ntheir OnPoint Product Suite provides businesses with the necessary weather and\nclimate data to quickly generate meaningful and actionable insights for a wide\nrange of use cases across industries. Let's connect to the `Weather Source\nLLC: frostbyte` feed from Weather Source in the Snowflake Marketplace by\nfollowing these steps:\n\n  * Login to Snowsight\n  * Click on the `Marketplace` link in the left navigation bar\n  * Enter \"Weather Source LLC: frostbyte\" in the search box and click return\n  * Click on the \"Weather Source LLC: frostbyte\" listing tile\n  * Click the blue \"Get\" button \n    * Expand the \"Options\" dialog\n    * Change the \"Database name\" to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n    * Select the \"HOL_ROLE\" role to have access to the new database\n  * Click on the blue \"Get\" button\n\nThat's it... we don't have to do anything from here to keep this data updated.\nThe provider will do that for us and data sharing means we are always seeing\nwhatever they have published. How amazing is that? Just think of all the\nthings you didn't have do here to get access to an always up-to-date, third-\nparty dataset!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43942304-f46d-4def-a0e0-30461f666486": {"__data__": {"id_": "43942304-f46d-4def-a0e0-30461f666486", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5a35427-a7b4-46a1-8687-c0191f6a1bb2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4557b8241ccaf140c2ab10859b63292e82c4b4d71148c703d2f431380b7fe08a", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nOpen the `steps/03_load_weather.sql` script in VS Code from the file Explorer\nin the left navigation bar, and run the script. Notice how easy it is to query\ndata shared through the Snowflake Marketplace! You access it just like any\nother table or view in Snowflake:\n\n    \n    \n    SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd3500c1-9674-4ef5-a9da-1c73a7db2d83": {"__data__": {"id_": "dd3500c1-9674-4ef5-a9da-1c73a7db2d83", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2d99ca6-f126-44d4-9b71-596df4d5d36c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "11cce9c615f779e06c00947b05a58e5af8519fc1235a578871ca85d211b5676f", "class_name": "RelatedNodeInfo"}}, "text": "6\\. Create POS View\n\nDuring this step we will be creating a view to simplify the raw POS schema by\njoining together 6 different tables and picking only the columns we need. But\nwhat's really cool is that we're going to define that view with the Snowpark\nDataFrame API! Then we're going to create a Snowflake stream on that view so\nthat we can incrementally process changes to any of the POS tables. To put\nthis in context, we are on step **#4** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "844227b3-6eee-4736-81e7-b501de1c62ff": {"__data__": {"id_": "844227b3-6eee-4736-81e7-b501de1c62ff", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3cc1582-c7b3-48e2-92e0-663037e43711", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7f3d1d34ae5127dbc03c10122eecbdca27c1908269e47a6fcd75e73ef478432f", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nTo create the view and stream, execute the `steps/04_create_pos_view.py`\nscript. Like we did in step 2, let's execute it from the terminal. So go back\nto the terminal in VS Code, make sure that your `snowflake-demo` conda\nenvironment is active, then run the following commands (which assume that your\nterminal has the root of your repository open):\n\n    \n    \n    cd steps\n    python 04_create_pos_view.py\n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "786a2024-7669-4e46-885d-e176b9c1f52c": {"__data__": {"id_": "786a2024-7669-4e46-885d-e176b9c1f52c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d2ee27e-3301-4b04-bc24-070959617957", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "8684b78fe2d18bda2c0dc9cc4786038ee557cd61cf4acc815167403fed7fff3e", "class_name": "RelatedNodeInfo"}}, "text": "Snowpark DataFrame API\n\nThe first thing you'll notice in the `create_pos_view()` function is that we\ndefine the Snowflake view using the Snowpark DataFrame API. After defining the\nfinal DataFrame, which captures all the logic we want in the view, we can\nsimply call the Snowpark `create_or_replace_view()` method. Here's the final\nline from the `create_pos_view()` function:\n\n    \n    \n        final_df.create_or_replace_view('POS_FLATTENED_V')\n\nFor more details about the Snowpark Python DataFrame API, please check out our\n[Working with DataFrames in Snowpark\nPython](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-\nwith-dataframes.html) page.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3c4e4c71-d3d4-40fc-8716-0ccf19496c08": {"__data__": {"id_": "3c4e4c71-d3d4-40fc-8716-0ccf19496c08", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b43d78fb-9919-4b37-80fb-47199f52e96a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "241626a7900e01de5aba7bdf35279e57668060af23ea6082b71ca42734885916", "class_name": "RelatedNodeInfo"}}, "text": "Streams for Incremental Processing (CDC)\n\nSnowflake makes processing data incrementally very easy. Traditionally the\ndata engineer had to keep track of a high watermark (usually a datetime\ncolumn) in order to process only new records in a table. This involved\ntracking and persisting that watermark somewhere and then using it in any\nquery against the source table. But with Snowflake streams all the heavy\nlifting is done for you by Snowflake. For more details please check out our\n[Change Tracking Using Table Streams](https://docs.snowflake.com/en/user-\nguide/streams.html) user guide.\n\nAll you need to do is create a [`STREAM`](https://docs.snowflake.com/en/sql-\nreference/sql/create-stream.html) object in Snowflake against your base table\nor view, then query that stream just like any table in Snowflake. The stream\nwill return only the changed records since the last DML option your performed.\nTo help you work with the changed records, Snowflake streams will supply the\nfollowing metadata columns along with the base table or view columns:\n\n  * METADATA$ACTION\n  * METADATA$ISUPDATE\n  * METADATA$ROW_ID\n\nFor more details about these stream metadata columns please check out the\n[Stream Columns](https://docs.snowflake.com/en/user-guide/streams-\nintro.html#stream-columns) section in our documentation.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9069655-a8af-4294-821b-9ebab8a39a7b": {"__data__": {"id_": "b9069655-a8af-4294-821b-9ebab8a39a7b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5d69d17-b9cc-4386-848f-0d5ce4bdc6b6", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2748d9430d37a20898a7d53df566db9e95136502f0a211d691c026680b941174", "class_name": "RelatedNodeInfo"}}, "text": "Streams on views\n\nWhat's really cool about Snowflake's incremental/CDC stream capability is the\nability to create a stream on a view! In this example we are creating a stream\non a view which joins together 6 of the raw POS tables. Here is the code to do\nthat:\n\n    \n    \n    def create_pos_view_stream(session):\n        session.use_schema('HARMONIZED')\n        _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                            ON VIEW POS_FLATTENED_V \\\n                            SHOW_INITIAL_ROWS = TRUE').collect()\n\nNow when we query the `POS_FLATTENED_V_STREAM` stream to find changed records,\nSnowflake is actually looking for changed records in any of the 6 tables\nincluded in the view. For those who have tried to build incremental/CDC\nprocesses around denormalized schemas like this, you will appreciate the\nincredibly powerful feature that Snowflake provides here.\n\nFor more details please check out the [Streams on\nViews](https://docs.snowflake.com/en/user-guide/streams-intro.html#streams-on-\nviews) section in our documentation.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61667a92-ebad-442d-a1ee-ece1402be1b3": {"__data__": {"id_": "61667a92-ebad-442d-a1ee-ece1402be1b3", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d2d79df-746d-497f-8113-2f8762d21247", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "23ce285fac4f0ce63b6baa5a7d00eac546174e2caa35bbfb6596ac03f195e548", "class_name": "RelatedNodeInfo"}}, "text": "7\\. Fahrenheit to Celsius UDF\n\nDuring this step we will be creating and deploying our first Snowpark Python\nobject to Snowflake, a user-defined function (or UDF). To begin with the UDF\nwill be very basic, but in a future step we'll update it to include a third-\nparty Python package. Also in this step you will be introduced to the new\nSnowCLI, a new developer command line tool. SnowCLI makes building and\ndeploying Snowpark Python objects to Snowflake a consistent experience for the\ndeveloper. More details below on SnowCLI. To put this in context, we are on\nstep **#5** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9fd86b9e-3224-4bdb-8d17-eb43ab2104a7": {"__data__": {"id_": "9fd86b9e-3224-4bdb-8d17-eb43ab2104a7", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18c40c4a-dcba-41f1-ad88-ad6af3c275b9", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "afd9fc74dcabb0e734d238197fbc0b707145b1b41acbbfc26abd4aca488f1d51", "class_name": "RelatedNodeInfo"}}, "text": "Running the UDF Locally\n\nTo test the UDF locally, you will execute the\n`steps/05_fahrenheit_to_celsius_udf/fahrenheit_to_celsius_udf/function.py`\nscript. Like we did in the previous steps, we'll execute it from the terminal.\nSo go back to the terminal in VS Code, make sure that your `snowflake-demo`\nconda environment is active, then run the following commands (which assume\nthat your terminal has the root of your repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    python fahrenheit_to_celsius_udf/function.py 35\n\nWhile you're developing the UDF you can simply run it locally in VS Code. And\nif your UDF doesn't need to query data from Snowflake, this process will be\nentirely local.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6a1accf-9f0e-48f5-a6f8-7e6752763ef4": {"__data__": {"id_": "d6a1accf-9f0e-48f5-a6f8-7e6752763ef4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c68b55c3-28af-4588-90f3-ef2a438c3974", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "95df5cde1af8551b693277f3d98f0cb87de7f3f2f82eed375533a1097d531cac", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the UDF to Snowflake\n\nTo deploy your UDF to Snowflake we will use the SnowCLI tool. The SnowCLI tool\nwill do all the heavy lifting of packaging up your application, copying it to\na Snowflake stage, and creating the object in Snowflake. Like we did in the\nprevious steps, we'll execute it from the terminal. So go back to the terminal\nin VS Code, make sure that your `snowflake-demo` conda environment is active,\nthen run the following commands (which assume that your terminal has the root\nof your repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    snow snowpark build\n    snow snowpark deploy\n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a53baff8-cd4e-4f34-a3ed-3ff460a6119d": {"__data__": {"id_": "a53baff8-cd4e-4f34-a3ed-3ff460a6119d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc362518-dce9-434d-be8c-82259f3d8849", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f8270b563332f8cf119d1bd0a6a58802debfc2637e350ebb8464a52e99f6bfde", "class_name": "RelatedNodeInfo"}}, "text": "Running the UDF in Snowflake\n\nIn order to run the UDF in Snowflake you have a few options. Any UDF in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    SELECT ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(35);\n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow snowpark execute function \"fahrenheit_to_celsius_udf(35)\"\n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "11a24237-4ed1-40bb-ba44-3f060ed53019": {"__data__": {"id_": "11a24237-4ed1-40bb-ba44-3f060ed53019", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbf3bb87-bb52-431b-8664-2f7fd39c79db", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "68bd493ef7441ab6a88410d0030e51c6a5b7ad24143b23be53316689d4ca52fc", "class_name": "RelatedNodeInfo"}}, "text": "Overview of the SnowCLI Tool\n\nThe SnowCLI tool is a command\nline tool for developers, and is executed as `snow` from the command line.\n\n**Note** \\- Do not confuse this with the\nSnowSQL command line\ntool which is a client for connecting to Snowflake to execute SQL queries and\nperform all DDL and DML operations, and is executed as `snowsql` from the\ncommand line.\n\nSnowCLI simplifies the development and deployment of the following Snowflake\nobjects:\n\n  * Snowpark Python UDFs\n  * Snowpark Python Stored Procedures\n  * Streamlit Applications\n\nFor this Quickstart we will be focused on the first two. And for Snowpark\nPython UDFs and sprocs in particular, the SnowCLI does all the heavy lifting\nof deploying the objects to Snowflake. Here's a brief summary of the steps the\nSnowCLI deploy command does for you:\n\n  * Dealing with third-party packages \n    * For packages that can be accessed directly from our Anaconda channel it will add them to the `PACKAGES` list in the `CREATE PROCEDURE` or `CREATE FUNCTION` SQL command\n    * For packages which are not currently available in our Anaconda channel it will download the code and include them in the project zip file\n  * Creating a zip file of everything in your project\n  * Copying that project zip file to your Snowflake stage\n  * Creating the Snowflake function or stored procedure object\n\nThis also allows you to develop and test your Python application without\nhaving to worry about wrapping it in a corresponding Snowflake database\nobject.\n\n**Note** \\- As of 3/7/2024, the [SnowCLI Tool](https://github.com/Snowflake-\nLabs/snowcli) is still in preview.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1932a9a-8359-441a-820f-c5b90cc71bbb": {"__data__": {"id_": "c1932a9a-8359-441a-820f-c5b90cc71bbb", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1eda91c2-f7e0-4241-aaf5-4cbbf5f70d56", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "8d8fee6e95541ad7a55081664346fce8aa6d17c4dce3dfaaa52664ff01e573d6", "class_name": "RelatedNodeInfo"}}, "text": "More on Snowpark Python UDFs\n\nIn this step we deployed a very simple Python UDF to Snowflake. In a future\nstep will update it to use a third-party package. And because we deployed it\nto Snowflake with the SnowCLI command you didn't have to worry about the SQL\nDDL Syntax to create the object in Snowflake. But for reference please check\nout our [Writing Python UDFs](https://docs.snowflake.com/en/developer-\nguide/udf/python/udf-python.html) developer guide.\n\nHere is the SQL query that the SnowCLI tool generated to deploy the function:\n\n    \n    \n    create or replace function HOL_DB.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(temp_f float)\n    returns float\n    language python\n    runtime_version=3.10\n    imports=('@HOL_DB.ANALYTICS.DEPLOYMENT/hol/fahrenheit_to_celsius_udf.zip')\n    handler='function.main'\n    packages=('snowflake-snowpark-python')", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "69f3e43a-1c26-49d0-a744-1c213352d812": {"__data__": {"id_": "69f3e43a-1c26-49d0-a744-1c213352d812", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b480a476-3b85-4054-ab87-6f772877103a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a3a843fbd6e634f210be5e15a1b132da03d2ba3b1b17c2ce50e4f300efe333df", "class_name": "RelatedNodeInfo"}}, "text": "8\\. Orders Update Sproc\n\nDuring this step we will be creating and deploying our first Snowpark Python\nstored procedure (or sproc) to Snowflake. This sproc will merge changes from\nthe `HARMONIZED.POS_FLATTENED_V_STREAM` stream into our target\n`HARMONIZED.ORDERS` table. To put this in context, we are on step **#6** in\nour data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1b88e21-fad4-40e1-a393-53890dcb9995": {"__data__": {"id_": "d1b88e21-fad4-40e1-a393-53890dcb9995", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "519557be-c726-43f5-9451-55eb94154174", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dbc19f1927f88afe31ae1a22dfb57c1a7f7c04bad3073157554d692587621297", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc Locally\n\nTo test the procedure locally, you will execute the\n`steps/06_orders_update_sp/orders_update_sp/procedure.py` script. Like we did\nin the previous steps, we'll execute it from the terminal. So go back to the\nterminal in VS Code, make sure that your `snowflake-demo` conda environment is\nactive, then run the following commands (which assume that your terminal has\nthe root of your repository open):\n\n    \n    \n    cd steps/06_orders_update_sp\n    python orders_update_sp/procedure.py\n\nWhile you're developing the sproc you can simply run it locally in VS Code.\nThe Python code will run locally on your laptop, but the Snowpark DataFrame\ncode will issue SQL queries to your Snowflake account.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf3677e0-c9ea-4344-9851-7530f2a73a9a": {"__data__": {"id_": "bf3677e0-c9ea-4344-9851-7530f2a73a9a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ac2473f-4d16-4a8e-8c20-a65723025631", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b6fc82a9f8cae881a8726ff5895fd2250d39a0e607b7cfd9da3b17e70e8057b2", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the Sproc to Snowflake\n\nTo deploy your sproc to Snowflake we will use the SnowCLI tool. Like we did in\nthe previous steps, we'll execute it from the terminal. So go back to the\nterminal in VS Code, make sure that your `snowflake-demo` conda environment is\nactive, then run the following commands (which assume that your terminal has\nthe root of your repository open):\n\n    \n    \n    cd steps/06_orders_update_sp\n    snow snowpark build\n    snow snowpark deploy\n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0717bf6-5865-4a1c-95c0-458d929dcc6d": {"__data__": {"id_": "e0717bf6-5865-4a1c-95c0-458d929dcc6d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae36a23c-fce4-4d72-9dab-e9de1f88bd75", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7bca1211146b21a13f5f45e6f62bfc9b9d29d5aa5dc4a23dcd8b27f7dd8c5a89", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc in Snowflake\n\nIn order to run the sproc in Snowflake you have a few options. Any sproc in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    CALL ORDERS_UPDATE_SP();\n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow snowpark execute procedure \"harmonized.orders_update_sp()\"\n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b726a81b-1aae-4f8f-9f23-1b1d81896f12": {"__data__": {"id_": "b726a81b-1aae-4f8f-9f23-1b1d81896f12", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e791a38f-9916-47d0-806c-cdcc8ac25756", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cfe386909fa13d26c97c3023a3802253489291d9cd6a2b44ca8939c2d72e1f60", "class_name": "RelatedNodeInfo"}}, "text": "More on Snowpark Python Sprocs\n\nIn this step we deployed a Python sproc to Snowflake. And because we deployed\nit to Snowflake with the SnowCLI command you didn't have to worry about the\nSQL DDL Syntax to create the object in Snowflake. But for reference please\ncheck out our [Writing Stored Procedures in Snowpark\n(Python)](https://docs.snowflake.com/en/sql-reference/stored-procedures-\npython.html) guide.\n\nHere is the SQL query that the SnowCLI tool generated to deploy the procedure:\n\n    \n    \n    create or replace procedure HOL_DB.HARMONIZED.ORDERS_UPDATE_SP()\n    returns string\n    language python\n    runtime_version=3.10\n    imports=('@HOL_DB.HARMONIZED.DEPLOYMENT/hol/orders_update_sp.zip')\n    handler='procedure.main'\n    packages=('snowflake-snowpark-python')", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f7b4cea-f295-457d-943e-29b7a4e589cf": {"__data__": {"id_": "5f7b4cea-f295-457d-943e-29b7a4e589cf", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f41d404-ece0-47fb-9dc4-af44212a0394", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f5302c12f7f09298ddd159b00cff932f3a93258ce603ed14022a2baf91f50734", "class_name": "RelatedNodeInfo"}}, "text": "More on the Snowpark API\n\nIn this step we're starting to really use the Snowpark DataFrame API for data\ntransformations. The Snowpark API provides the same functionality as the\n[Spark SQL\nAPI](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html).\nTo begin with you need to create a Snowpark session object. Like PySpark, this\nis accomplished with the `Session.builder.configs().create()` methods. When\nrunning locally, we use the `Session.builder.getOrCreate()` method to create\nthe session object for us. But when deployed to Snowflake, the session object\nis provisioned for you automatically by Snowflake. And when building a\nSnowpark Python sproc the contract is that the first argument to the entry\npoint (or handler) function is a Snowpark session.\n\nThe first thing you'll notice in the\n`steps/06_orders_update_sp/orders_update_sp/procedure.py` script is that we\nhave some functions which use SQL to create objects in Snowflake and to check\nobject status. To issue a SQL statement to Snowflake with the Snowpark API you\nuse the `session.sql()` function, like you'd expect. Here's one example:\n\n    \n    \n    def create_orders_stream(session):\n        _ = session.sql(\"CREATE STREAM IF NOT EXISTS HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS \\\n                        SHOW_INITIAL_ROWS = TRUE;\").collect()\n\nThe second thing to point out is how we're using DataFrames to merge changes\nfrom the source view to the target table. The Snowpark DataFrame API provides\na `merge()` method which will ultimately generate a `MERGE` command in\nSnowflake.\n\n    \n    \n        source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n        target = session.table('HARMONIZED.ORDERS')\n    \n        # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n        cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n        metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n        updates = {**cols_to_update, **metadata_col_to_update}\n    \n        # merge into DIM_CUSTOMER\n        target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                            [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\nAgain, for more details about the Snowpark Python DataFrame API, please check\nout our [Working with DataFrames in Snowpark\nPython](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-\nwith-dataframes.html) page.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e44431c4-52e8-4f72-91cc-2b7a5af611b6": {"__data__": {"id_": "e44431c4-52e8-4f72-91cc-2b7a5af611b6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bc41016-0ed5-46eb-a0ba-538cafc881c2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "93dd95138d130b79ce5c41aeab42fb37547fc6c0ebf071e927aabed4159111b5", "class_name": "RelatedNodeInfo"}}, "text": "9\\. Daily City Metrics Update Sproc\n\nDuring this step we will be creating and deploying our second Snowpark Python\nsproc to Snowflake. This sproc will join the `HARMONIZED.ORDERS` data with the\nWeather Source data to create a final, aggregated table for analysis named\n`ANALYTICS.DAILY_CITY_METRICS`. We will process the data incrementally from\nthe `HARMONIZED.ORDERS` table using another Snowflake Stream. And we will\nagain use the Snowpark DataFrame `merge()` method to merge/upsert the data. To\nput this in context, we are on step **#7** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1af41e5a-afbd-486a-9c77-f61c4187cbbe": {"__data__": {"id_": "1af41e5a-afbd-486a-9c77-f61c4187cbbe", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd73e4dd-2354-4d4a-bf6a-b8ab18138c44", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "253f661e26c6a5f494c0e3013cbb7ff3bba74d003334ecb7aae7953f1bdcdac0", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc Locally\n\nTo test the procedure locally, you will execute the\n`steps/07_daily_city_metrics_update_sp/daily_city_metrics_update_sp/procedure.py`\nscript. Like we did in the previous steps, we'll execute it from the terminal.\nSo go back to the terminal in VS Code, make sure that your `snowflake-demo`\nconda environment is active, then run the following commands (which assume\nthat your terminal has the root of your repository open):\n\n    \n    \n    cd steps/07_daily_city_metrics_update_sp\n    python daily_city_metrics_update_sp/procedure.py\n\nWhile you're developing the sproc you can simply run it locally in VS Code.\nThe Python code will run locally on your laptop, but the Snowpark DataFrame\ncode will issue SQL queries to your Snowflake account.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc0d0604-ead4-40d1-a9c9-bb61c3a70551": {"__data__": {"id_": "bc0d0604-ead4-40d1-a9c9-bb61c3a70551", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97c90058-8c5f-41b1-b0b3-4689824bd4b5", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "aeda5578d80f52bc117a4145402acf4b991c05e6b3bbc782ca7646c0cfec2778", "class_name": "RelatedNodeInfo"}}, "text": "Deploying the Sproc to Snowflake\n\nTo deploy your sproc to Snowflake we will use the SnowCLI tool. Like we did in\nthe previous steps, we'll execute it from the terminal. So go back to the\nterminal in VS Code, make sure that your `snowflake-demo` conda environment is\nactive, then run the following commands (which assume that your terminal has\nthe root of your repository open):\n\n    \n    \n    cd steps/07_daily_city_metrics_update_sp\n    snow snowpark build\n    snow snowpark deploy\n\nWhile that is running, please open the script in VS Code and continue on this\npage to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b2ab42e-2b31-4d8b-90e2-fd5a7eb5c847": {"__data__": {"id_": "9b2ab42e-2b31-4d8b-90e2-fd5a7eb5c847", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ccd05586-212f-4737-b792-275a5a88886f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3b7d2f321d65b1eceefb316cd295268d660c7f3c0ab85c1e2fa32be76b2fe2a5", "class_name": "RelatedNodeInfo"}}, "text": "Running the Sproc in Snowflake\n\nIn order to run the sproc in Snowflake you have a few options. Any sproc in\nSnowflake can be invoked through SQL as follows:\n\n    \n    \n    CALL DAILY_CITY_METRICS_UPDATE_SP();\n\nAnd with the SnowCLI utility you can also invoke the UDF from the terminal in\nVS Code as follows:\n\n    \n    \n    snow snowpark execute procedure \"daily_city_metrics_update_sp()\"\n\nThat will result in the SnowCLI tool generating the SQL query above and\nrunning it against your Snowflake account.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68640ba3-5b21-4f9d-8bbc-70f4cb0aea44": {"__data__": {"id_": "68640ba3-5b21-4f9d-8bbc-70f4cb0aea44", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b6d9dc6-154a-4247-9bd6-cf4b482b002f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1781ff2b6b16eea9689d0f120d69c589a068a837f255a2fa413e52cff81ec7f2", "class_name": "RelatedNodeInfo"}}, "text": "Data Modeling Best Practice\n\nWhen modeling data for analysis a best practice has been to clearly define and\nmanage the schema of the table. In step 2, when we loaded raw data from\nParquet we took advantage of Snowflake's schema detection feature to create a\ntable with the same schema as the Parquet files. In this step we are\nexplicitly defining the schema in DataFrame syntax and using that to create\nthe table.\n\n    \n    \n    def create_daily_city_metrics_table(session):\n        SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                            T.StructField(\"CITY_NAME\", T.StringType()),\n                                            T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                            T.StructField(\"DAILY_SALES\", T.StringType()),\n                                            T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                            T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                            T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                            T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                            T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                        ]\n        DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n        DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n    \n        dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                            .na.drop() \\\n                            .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n        dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0683b035-25be-412d-b37e-b721ebfaa012": {"__data__": {"id_": "0683b035-25be-412d-b37e-b721ebfaa012", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bc124b2-cbb0-4133-9bbd-d964a0304bd0", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "180f272fa63a79c33553f6f12e538bde0bf8bd467fa885fa19a4646089a8e518", "class_name": "RelatedNodeInfo"}}, "text": "Complex Aggregation Query\n\nThe `merge_daily_city_metrics()` function contains a complex aggregation query\nwhich is used to join together and aggregate the data from our POS and Weather\nSource. Take a look at the series of complex series of joins and aggregations\nthat are expressed, and how we're even leveraging the Snowpark UDF we created\nin step #5!\n\nThe complex aggregation query is then merged into the final analytics table\nusing the Snowpark `merge()` method. If you haven't already, check out your\nSnowflake Query history and see which queries were generated by the Snowpark\nAPI. In this case you will see that the Snowpark API took all the complex\nlogic, including the merge and created a single Snowflake query to execute!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c438f145-78c5-45b0-8a87-80151b5e1c0d": {"__data__": {"id_": "c438f145-78c5-45b0-8a87-80151b5e1c0d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5df8b2c7-12a6-4784-a1a3-d954ab6c2ac7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d8b5bf7201eca7bf89217932a66e28de93d5bc1c5c9050e9da2b0aec7c26f0a9", "class_name": "RelatedNodeInfo"}}, "text": "10\\. Orchestrate Jobs\n\nDuring this step we will be orchestrating our new Snowpark pipelines with\nSnowflake's native orchestration feature named Tasks. We will create two\ntasks, one for each stored procedure, and chain them together. We will then\nrun the tasks. To put this in context, we are on step **#8** in our data flow\noverview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "28fd5933-2014-4bff-830e-3185e3e03cc7": {"__data__": {"id_": "28fd5933-2014-4bff-830e-3185e3e03cc7", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64466d9f-fd2e-4f5e-b572-c45223a0592c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "82478e193eebd3d4216d14b0033f508649dd6df6eef31bad9f61ce3fb78f28d8", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nSince this is a SQL script we will be using our native VS Code extension to\nexecute it. So simply open the `steps/08_orchestrate_jobs.sql` script in VS\nCode and run the whole thing using the \"Execute All Statements\" button in the\nupper right corner of the editor window.\n\nWhile that is running, please read through the script in VS Code and continue\non this page to understand what is happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a34e35d0-1275-4cc5-8849-9ac9f5eec6c5": {"__data__": {"id_": "a34e35d0-1275-4cc5-8849-9ac9f5eec6c5", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "034d36c7-152e-4f6a-afef-dc4ce449c2d3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e427dd6a297838975681f2d32f0392050d68af7d2162186b1e79de0610c8dbe1", "class_name": "RelatedNodeInfo"}}, "text": "Running the Tasks\n\nIn this step we did not create a schedule for our task DAG, so it will not run\non its own at this point. So in this script you will notice that we manually\nexecute the DAG, like this:\n\n    \n    \n    EXECUTE TASK ORDERS_UPDATE_TASK;\n\nTo see what happened when you ran this task just now, highlight and run (using\nCMD/CTRL+Enter) this commented query in the script:\n\n    \n    \n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n\nYou will notice in the task history output that it skipped our task\n`ORDERS_UPDATE_TASK`. This is correct, because our\n`HARMONIZED.POS_FLATTENED_V_STREAM` stream doesn't have any data. We'll add\nsome new data and run them again in the next step.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5b6bbf0d-b5d1-4eaf-82de-b440c8de0ad5": {"__data__": {"id_": "5b6bbf0d-b5d1-4eaf-82de-b440c8de0ad5", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f5c7ad8-94b4-467a-85d5-7168af917740", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a6f15999ea9116672bfb2beaba0b772ac9ad5f8137fc8a61103c1e595dd16a39", "class_name": "RelatedNodeInfo"}}, "text": "More on Tasks\n\nTasks are Snowflake's native scheduling/orchestration feature. With a task you\ncan execute any one of the following types of SQL code:\n\n  * Single SQL statement\n  * Call to a stored procedure\n  * Procedural logic using Snowflake Scripting Developer Guide\n\nFor this Quickstart we'll call our Snowpark stored procedures. Here is the SQL\nDDL code to create the second task:\n\n    \n    \n    CREATE OR REPLACE TASK DAILY_CITY_METRICS_UPDATE_TASK\n    WAREHOUSE = HOL_WH\n    AFTER ORDERS_UPDATE_TASK\n    WHEN\n      SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\n    AS\n    CALL ANALYTICS.DAILY_CITY_METRICS_UPDATE_SP();\n\nA few things to point out. First you specify which Snowflake virtual warehouse\nto use when running the task with the `WAREHOUSE` clause. The `AFTER` clause\nlets you define the relationship between tasks, and the structure of this\nrelationship is a Directed Acyclic Graph (or DAG) like most orchestration\ntools provide. The `AS` clause let's you define what the task should do when\nit runs, in this case to call our stored procedure.\n\nThe `WHEN` clause is really cool. We've already seen how streams work in\nSnowflake by allowing you to incrementally process data. We've even seen how\nyou can create a stream on a view (which joins many tables together) and\ncreate a stream on that view to process its data incrementally! Here in the\n`WHEN` clause we're calling a system function `SYSTEM$STREAM_HAS_DATA()` which\nreturns true if the specified stream has new data. With the `WHEN` clause in\nplace the virtual warehouse will only be started up when the stream has new\ndata. So if there's no new data when the task runs then your warehouse won't\nbe started up and you won't be charged. You will only be charged when there's\nnew data to process. Pretty cool, huh?\n\nAs mentioned above we did not define a `SCHEDULE` for the root task, so this\nDAG will not run on its own. That's fine for this Quickstart, but in a real\nsituation you would define a schedule. See [CREATE\nTASK](https://docs.snowflake.com/en/sql-reference/sql/create-task.html) for\nthe details.\n\nAnd for more details on Tasks see [Introduction to\nTasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1df25d21-5bb4-42a9-9b25-ff87ef0f2dde": {"__data__": {"id_": "1df25d21-5bb4-42a9-9b25-ff87ef0f2dde", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d88415e-8318-4ddb-b469-dc9ff55175d4", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "003e21d56b32d6cef55c11684b0139438978a0ba462c9764fd4cc677413f668d", "class_name": "RelatedNodeInfo"}}, "text": "Task Metadata\n\nSnowflake keeps metadata for almost everything you do, and makes that metadata\navailable for you to query (and to create any type of process around). Tasks\nare no different, Snowflake maintains rich metadata to help you monitor your\ntask runs. Here are a few sample SQL queries you can use to monitor your tasks\nruns:\n\n    \n    \n    -- Get a list of tasks\n    SHOW TASKS;\n    \n    -- Task execution history in the past day\n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n    \n    -- Scheduled task runs\n    SELECT\n        TIMESTAMPDIFF(SECOND, CURRENT_TIMESTAMP, SCHEDULED_TIME) NEXT_RUN,\n        SCHEDULED_TIME,\n        NAME,\n        STATE\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\n    WHERE STATE = 'SCHEDULED'\n    ORDER BY COMPLETED_TIME DESC;", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e0ae116-320d-4bac-a6db-269d8a9b8eac": {"__data__": {"id_": "7e0ae116-320d-4bac-a6db-269d8a9b8eac", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77b98518-12d7-458e-acf9-a9b44ee63255", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a36b3b67305c23a3eeb6dbe2ce65a5cf1322dea22c9cbdee25d9b7455951951e", "class_name": "RelatedNodeInfo"}}, "text": "Monitoring Tasks\n\nSo while you're free to create any operational or monitoring process you wish,\nSnowflake provides some rich task observability features in our Snowsight UI.\nTry it out for yourself by following these steps:\n\n  1. In the Snowsight navigation menu, click **Data** \u00bb **Databases**.\n  2. In the right pane, using the object explorer, navigate to a database and schema.\n  3. For the selected schema, select and expand **Tasks**.\n  4. Select a task. Task information is displayed, including **Task Details** , **Graph** , and **Run History** sub-tabs.\n  5. Select the **Graph** tab. The task graph appears, displaying a hierarchy of child tasks.\n  6. Select a task to view its details.\n\nHere's what the task graph looks like:\n\n!\n\nAnd here's an example of the task run history:\n\n!\n\nFor more details, and to learn about viewing account level task history,\nplease check out our [Viewing Task\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-tasks.html)\ndocumentation.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf260c36-2e62-490f-a348-cd6b6a705735": {"__data__": {"id_": "cf260c36-2e62-490f-a348-cd6b6a705735", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09b082f7-50be-4097-a775-7ff5b1174ca4", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "aa3a18b1f3ffdbdfdbb7cca7a835956947f6006e93fe7482b4fd207f1e6bf911", "class_name": "RelatedNodeInfo"}}, "text": "11\\. Process Incrementally\n\nDuring this step we will be adding new data to our POS order tables and then\nrunning our entire end-to-end pipeline to process the new data. And this\nentire pipeline will be processing data incrementally thanks to Snowflake's\nadvanced stream/CDC capabilities. To put this in context, we are on step\n**#9** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43c06e1b-66b7-4e1d-a6d3-bdb79b299565": {"__data__": {"id_": "43c06e1b-66b7-4e1d-a6d3-bdb79b299565", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2e2b7ec-fa03-4daf-bb58-038b18c00c08", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "bd778265753ed9e6d540d7d560751f87c01a692321525815b725b7e8ac85a7d6", "class_name": "RelatedNodeInfo"}}, "text": "Run the Script\n\nSince this is a SQL script we will be using our native VS Code extension to\nexecute it. So simply open the `steps/09_process_incrementally.sql` script in\nVS Code and run the whole thing using the \"Execute All Statements\" button in\nthe upper right corner of the editor window.\n\nWhile that is running, let's briefly discuss what's happening. As in step #2,\nwe're going to load data from Parquet into our raw POS tables. In step #2 we\nloaded all the order data except for the 2022 data for `ORDER_HEADER` and\n`ORDER_DETAIL`. So now we're going to load the remaining data.\n\nThis time we will be doing the data loading through SQL instead of Python, but\nthe process is the same. We'll resize the warehouse, scaling up so that we can\nload the data faster and then scaling back down after when we're done. After\nthe new data is loaded we will also run the task DAG again. And this time both\ntasks will run and process the new data.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7f362c69-2ce7-4699-8cdd-cc2d1e39b77b": {"__data__": {"id_": "7f362c69-2ce7-4699-8cdd-cc2d1e39b77b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d1609f8-c946-4208-ad59-6d85e89286b1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "8ec8457dded9bd46c8d17ff698c7d8592f4cdf4f2c62a75744ec999d0f460271", "class_name": "RelatedNodeInfo"}}, "text": "Viewing the Task History\n\nLike the in the previous step, to see what happened when you ran this task\nDAG, highlight and run (using CMD/CTRL+Enter) this commented query in the\nscript:\n\n    \n    \n    SELECT *\n    FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n        SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n        RESULT_LIMIT => 100))\n    ORDER BY SCHEDULED_TIME DESC\n    ;\n\nThis time you will notice that the `ORDERS_UPDATE_TASK` task will not be\nskipped, since the `HARMONIZED.POS_FLATTENED_V_STREAM` stream has new data. In\na few minutes you should see that both the `ORDERS_UPDATE_TASK` task and the\n`DAILY_CITY_METRICS_UPDATE_TASK` task completed successfully.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf950538-c27d-4a43-b571-3725bcba7b16": {"__data__": {"id_": "bf950538-c27d-4a43-b571-3725bcba7b16", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78d6d00c-3aba-4352-8a81-297fe1e48080", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "77a292b8ab2b5a217f144609f5467ebed33f8940b5f66f01280be98aa786363e", "class_name": "RelatedNodeInfo"}}, "text": "Query History for Tasks\n\nOne important thing to understand about tasks, is that the queries which get\nexecuted by the task won't show up with the default Query History UI settings.\nIn order to see the queries that just ran you need to do the following:\n\n  * Remove filters at the top of this table, including your username, as later scheduled tasks will run as \"System\":\n\n!\n\n  * Click \"Filter\", and add filter option \u2018Queries executed by user tasks' and click \"Apply Filters\":\n\n!\n\nYou should now see all the queries run by your tasks! Take a look at each of\nthe MERGE commands in the Query History to see how many records were processed\nby each task. And don't forget to notice that we processed the whole pipeline\njust now, and did so incrementally!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d18a704c-b7f6-4ac9-9c8c-9efbcbd87e32": {"__data__": {"id_": "d18a704c-b7f6-4ac9-9c8c-9efbcbd87e32", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ef0aaf4-f004-4d9d-b4a4-4a699e880b0c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d5d949c3ac21b84d7be414838a5eb14deb3ea50824a4e97f53bd38fa992c0957", "class_name": "RelatedNodeInfo"}}, "text": "12\\. Deploy Via CI/CD\n\nDuring this step we will be making a change to our\n`FAHRENHEIT_TO_CELSIUS_UDF()` UDF and then deploying it via a CI/CD pipeline.\nWe will be updating the `FAHRENHEIT_TO_CELSIUS_UDF()` UDF to use a third-party\nPython package, pushing it to your forked GitHub repo, and finally deploying\nit using the SnowCLI in a GitHub Actions workflow! To put this in context, we\nare on step **#10** in our data flow overview:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3260cbd3-00a1-4e35-b98e-8a69a61dc99f": {"__data__": {"id_": "3260cbd3-00a1-4e35-b98e-8a69a61dc99f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43462acc-a4e5-4aa4-9726-7f2cdca2ea0b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d46fa2b437b4be071b408bd433d5f23fbd8318931e088d7eb0ea1a2baefb9249", "class_name": "RelatedNodeInfo"}}, "text": "Update the Fahrenheit to Celsius UDF\n\nWe will be replacing our hard-coded temperature conversion with a package from\n`scipy`. First we will make a few changes to the\n`steps/05_fahrenheit_to_celsius_udf/fahrenheit_to_celsius_udf/function.py`\nscript. In this file we will be adding an `import` command and replacing the\nbody of the `main()` function. So open the\n`steps/05_fahrenheit_to_celsius_udf/fahrenheit_to_celsius_udf/function.py`\nscript in VS Code and replace this section:\n\n    \n    \n    import sys\n    \n    def main(temp_f: float) -> float:\n        return (float(temp_f) - 32) * (5/9)\n\nWith this:\n\n    \n    \n    import sys\n    from scipy.constants import convert_temperature\n    \n    def main(temp_f: float) -> float:\n        return convert_temperature(float(temp_f), 'F', 'C')\n\nDon't forget to save your changes.\n\nThe second change we need to make is to add `scipy` to our `requirements.txt`\nfile. Open the `steps/05_fahrenheit_to_celsius_udf/requirements.txt` file in\nVS Code, add a newline with `scipy` on it and save it.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "743616f4-5b01-4dde-9dd7-96cb495b478d": {"__data__": {"id_": "743616f4-5b01-4dde-9dd7-96cb495b478d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38ddfa0d-00e4-4f26-8860-1efe4b85eb91", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d06e323014d80ae064486855aee3842eac44ee94f685a71385b473131219e10b", "class_name": "RelatedNodeInfo"}}, "text": "Test your Changes Locally\n\nTo test the UDF locally, you will execute the\n`steps/05_fahrenheit_to_celsius_udf/fahrenheit_to_celsius_udf/function.py`\nscript. Like we did in previous steps, we'll execute it from the terminal. So\ngo back to the terminal in VS Code, make sure that your `snowflake-demo` conda\nenvironment is active, then run the following commands (which assume that your\nterminal has the root of your repository open):\n\n    \n    \n    cd steps/05_fahrenheit_to_celsius_udf\n    pip install -r requirements.txt\n    python fahrenheit_to_celsius_udf/function.py 35\n\nNotice that this time we're also running pip install to make sure that our\ndependent packages are installed. Once your function runs successfully we'll\nbe ready to deploy it via CI/CD!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "53e52ceb-d20e-4cd3-b675-21823768669d": {"__data__": {"id_": "53e52ceb-d20e-4cd3-b675-21823768669d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11567799-1cda-4c36-a5b1-7e53d7a7a5bd", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e014a5992737095922dafbb7a50318b8097e35eb370e7a7b880c641a01a968ee", "class_name": "RelatedNodeInfo"}}, "text": "Configuring Your Forked GitHub Project\n\nIn order for your GitHub Actions workflow to be able to connect to your\nSnowflake account you will need to store your Snowflake credentials in GitHub.\nAction Secrets in GitHub are used to securely store values/variables which\nwill be used in your CI/CD pipelines. In this step we will create secrets for\neach of the parameters used by SnowCLI.\n\nFrom the repository, click on the `Settings` tab near the top of the page.\nFrom the Settings page, click on the `Secrets and variables` then `Actions`\ntab in the left hand navigation. The `Actions` secrets should be selected. For\neach secret listed below click on `New repository secret` near the top right\nand enter the name given below along with the appropriate value (adjusting as\nappropriate).\n\nSecret name | Secret value  \n---|---  \nSNOWFLAKE_ACCOUNT | myaccount  \nSNOWFLAKE_USER | myusername  \nSNOWFLAKE_PASSWORD | mypassword  \nSNOWFLAKE_ROLE | HOL_ROLE  \nSNOWFLAKE_WAREHOUSE | HOL_WH  \nSNOWFLAKE_DATABASE | HOL_DB  \n  \n**Tip** \\- For more details on how to structure the account name in\nSNOWFLAKE_ACCOUNT, see the account name discussion in [the Snowflake Python\nConnector install guide](https://docs.snowflake.com/en/user-guide/python-\nconnector-install.html#step-2-verify-your-installation).\n\nWhen you're finished adding all the secrets, the page should look like this:\n\n!\n\n**Tip** \\- For an even better solution to managing your secrets, you can\nleverage [GitHub Actions\nEnvironments](https://docs.github.com/en/actions/reference/environments).\nEnvironments allow you to group secrets together and define protection rules\nfor each of your environments.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a42142e9-583a-4cd2-9596-9c8d9863ea98": {"__data__": {"id_": "a42142e9-583a-4cd2-9596-9c8d9863ea98", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2400f6aa-f12f-47ab-9ec0-ba379edd013c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9ff05633d8721aff411f8853db0825becd68943bea02d0d01649c456989a7021", "class_name": "RelatedNodeInfo"}}, "text": "Push Changes to Forked Repository\n\nNow that we have a changes ready and tested, and our Snowflake credentials\nstored in GitHub, let's commit them to our local repository and then push them\nto your forked repository. This can certainly be done from the command line,\nbut in this step we'll do so through VS Code to make it easy.\n\nStart by opening the \"Source Control\" extension in the left hand nav bar, you\nshould see two files with changes. Click the `+` (plus) sign at the right of\neach file name to stage the changes. Then enter a message in the \"Message\" box\nand click the blue `Commit` button to commit the changes locally. Here's what\nit should look like before you click the button:\n\n!\n\nAt this point those changes are only committed locally and have not yet been\npushed to your forked repository in GitHub. To do that, simply click the blue\n`Sync Changes` button to push these commits to GitHub. Here's what it should\nlook like before you click the button:\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4a3eba8-9828-42b7-bb04-feec0b4de226": {"__data__": {"id_": "d4a3eba8-9828-42b7-bb04-feec0b4de226", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed01398f-a4d6-4179-9219-161d9510fc80", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e3e5bc3d5600ce00014a81a36b1bf8f298b4240c7c0083f610bb0a50e2aa247d", "class_name": "RelatedNodeInfo"}}, "text": "Viewing GitHub Actions Workflow\n\nThis repository is already set up with a very simple GitHub Actions CI/CD\npipeline. You can review the code for the workflow by opening the\n`.github/workflows/build_and_deploy.yaml` file in VS Code.\n\nAs soon as you pushed the changes to your GitHub forked repo the workflow\nkicked off. To view the results go back to the homepage for your GitHub\nrepository and do the following:\n\n  * From the repository, click on the `Actions` tab near the top middle of the page\n  * In the left navigation bar click on the name of the workflow `Deploy Snowpark Apps`\n  * Click on the name of most recent specific run (which should match the comment you entered)\n  * From the run overview page click on the `deploy` job and then browse through the output from the various steps. In particular you might want to review the output from the `Deploy Snowpark apps` step.\n\n!\n\nThe output of the `Deploy Snowpark apps` step should be familiar to you by\nnow, and should be what you saw in the terminal in VS Code when you ran\nSnowCLI in previous steps. The one thing that may be different is the order of\nthe output, but you should be able to see what's happening.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8789a67-460d-47a6-9fc6-02c0db3428ad": {"__data__": {"id_": "b8789a67-460d-47a6-9fc6-02c0db3428ad", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a7dd1f0-b8bb-48c8-9830-397c055d758f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "0ce10a4b20d99de74b895ab26bf13162abf864e1d2efee92dab5c34a6379eee1", "class_name": "RelatedNodeInfo"}}, "text": "13\\. Teardown\n\nOnce you're finished with the Quickstart and want to clean things up, you can\nsimply run the `steps/11_teardown.sql` script. Since this is a SQL script we\nwill be using our native VS Code extension to execute it. So simply open the\n`steps/11_teardown.sql` script in VS Code and run the whole thing using the\n\"Execute All Statements\" button in the upper right corner of the editor\nwindow.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "292e71ce-f384-46e4-bf32-78ba5bbc6d47": {"__data__": {"id_": "292e71ce-f384-46e4-bf32-78ba5bbc6d47", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0e6b4e5-595e-4ea1-add2-34c110f5c71c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c0cccc2e16869723c466f0897b1e242823fe070d45786c6bdb5c8689762c0395", "class_name": "RelatedNodeInfo"}}, "text": "14\\. Conclusion\n\nWow, we have covered a lot of ground during this Quickstart! By now you have\nbuilt a robust data engineering pipeline using Snowpark Python stored\nprocedures. This pipeline processes data incrementally, is orchestrated with\nSnowflake tasks, and is deployed via a CI/CD pipeline. You also learned how to\nuse Snowflake's new developer CLI tool and Visual Studio Code extension!\nHere's a quick visual recap:\n\n!\n\nBut we've really only just scratched the surface of what's possible with\nSnowpark. Hopefully you now have the building blocks, and examples, you need\nto get started building your own data engineering pipeline with Snowpark\nPython. So, what will you build now?", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c58c5f80-1598-4aa2-976c-4bfdaffa4498": {"__data__": {"id_": "c58c5f80-1598-4aa2-976c-4bfdaffa4498", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ed0ff19-5549-4533-95b0-0277a217e31b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "99010224d3dcf14fd4b8428272c28579f03107aece921fe08a1542a5c6e74529", "class_name": "RelatedNodeInfo"}}, "text": "What we've covered\n\nWe've covered a ton in this Quickstart, and here are the highlights:\n\n  * Snowflake's Table Format\n  * Data ingestion with COPY\n  * Schema inference\n  * Data sharing/marketplace (instead of ETL)\n  * Streams for incremental processing (CDC)\n  * Streams on views\n  * Python UDFs (with third-party packages)\n  * Python Stored Procedures\n  * Snowpark DataFrame API\n  * Snowpark Python programmability\n  * Warehouse elasticity (dynamic scaling)\n  * Visual Studio Code Snowflake native extension (PuPr, Git integration)\n  * SnowCLI (PuPr)\n  * Tasks (with Stream triggers)\n  * Task Observability\n  * GitHub Actions (CI/CD) integration", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "445e167a-4689-4331-a409-1e370b452599": {"__data__": {"id_": "445e167a-4689-4331-a409-1e370b452599", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89baa835-38a8-4904-87d5-102bcb0e7361", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3f2616914394f4b43b45b85e8fa3c3ce7bef8d341b0dc126082132fcb4fa3e80", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\nAnd finally, here's a quick recap of related resources:\n\n  * Full Demo on Snowflake Demo Hub\n  * Source Code on GitHub\n  * Snowpark Developer Guide for Python\n    * Writing Python UDFs\n    * Writing Stored Procedures in Snowpark (Python)\n    * Working with DataFrames in Snowpark Python\n  * Related Tools \n    * Snowflake Visual Studio Code Extension\n    * SnowCLI Tool\n\nBack\n\nNextDone\n\n!\n!\n\n![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=3a0c5fde-\nbf4e-4324-aef4-dd0fbb5fd28d&sid=8ddf0340865a11efb10f357f1b6c021b&vid=8ddf20c0865a11ef8a2e991feff7f571&vids=1&msclkid=N&pi=0&lg=en-\nUS&sw=1800&sh=1169&sc=30&nwd=1&tl=Data%20Engineering%20Pipelines%20with%20Snowpark%20Python&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fdata_engineering_pipelines_with_snowpark_python%2F%230&r=&lt=956&evt=pageLoad&sv=1&cdb=AQAQ&rn=236835)![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=3a0c5fde-\nbf4e-4324-aef4-dd0fbb5fd28d&sid=8ddf0340865a11efb10f357f1b6c021b&vid=8ddf20c0865a11ef8a2e991feff7f571&vids=0&msclkid=N&ec=quickstarts&gc=USD&tpp=1&ea=quickstarts&en=Y&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fdata_engineering_pipelines_with_snowpark_python%2F%230&sw=1800&sh=1169&sc=30&nwd=1&evt=custom&cdb=AQAQ&rn=87490)\n\n!\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60de9efb-6aa1-4b4e-9756-6dad713abea2": {"__data__": {"id_": "60de9efb-6aa1-4b4e-9756-6dad713abea2", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b31014a-eef2-41c3-9719-6f34d12a09e2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "204297a776128693f4240e183f0f58c7b0fd2d18c0e997f49baf548c733c6473", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Use of Cookies\n\nWe use cookies to enhance your experience and to analyze site traffic as\ndescribed in our Cookie Statement. By accepting, you consent to our use of\ncookies.[Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-\nstatement/)\n\nCookies Settings Reject All Accept All Cookies\n\n![Company\nLogo](https://cdn.cookielaw.org/logos/cb85e692-4053-4d0a-8dda-d24b5daa8b06/ff6c124b-1473-4861-9ca3-9eaf6debb37d/SNO-\nSnowflakeLogo_blue.png)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e3e983b-ea5d-4cfe-a39c-3c2a214cfd76": {"__data__": {"id_": "4e3e983b-ea5d-4cfe-a39c-3c2a214cfd76", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ca2c4c3-5820-4c8b-bbb7-1ab56dc84eb1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2d689eb4aa87ac17adba2c29148984c95dbca029b44b2374bd60c45cf8e5371c", "class_name": "RelatedNodeInfo"}}, "text": "Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e72652f-02a6-43d9-bb43-49bc513ba4f4": {"__data__": {"id_": "3e72652f-02a6-43d9-bb43-49bc513ba4f4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cedd2731-34a2-494a-90c5-1bc11c9b9fb9", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4cce9e886afd56b4a06c2b1e833118ac4ade5fb3b117fddb7be539bfc59c82fd", "class_name": "RelatedNodeInfo"}}, "text": "Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.  \nMore information", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bf7ccc9-7c8e-4621-9344-a672aec1be2c": {"__data__": {"id_": "7bf7ccc9-7c8e-4621-9344-a672aec1be2c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3c6d656-037e-4bd2-9e97-1df1c58225e1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e809e14f27c4f2805398fa38537d04be40bbc4b0ec695e92055896b717d1ff22", "class_name": "RelatedNodeInfo"}}, "text": "Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff. They are usually only set in response to actions made by you which amount\nto a request for services, such as setting your privacy preferences, logging\nin or filling in forms. You can set your browser to block or alert you about\nthese cookies, but some parts of the site will not then work. These cookies do\nnot store any personally identifiable information.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f82ed1b4-935a-4ffb-a3f3-5306607decba": {"__data__": {"id_": "f82ed1b4-935a-4ffb-a3f3-5306607decba", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b09d4df-7047-4119-8caf-61645c291808", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "78b0dad73f015b07c66b6656832857835e5b193130f7cbd2a7e1c448a4d95c30", "class_name": "RelatedNodeInfo"}}, "text": "Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.    All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e8ada223-9db8-40e7-91e9-2eae49f34c17": {"__data__": {"id_": "e8ada223-9db8-40e7-91e9-2eae49f34c17", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d321286-2cbe-46fd-ac0b-5b8f5d721643", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "17eac5a22dbe7b5378927e620c95d36e2c1ee1533d67bde2677025cefd921fb1", "class_name": "RelatedNodeInfo"}}, "text": "Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages.    If you do not allow these cookies then\nsome or all of these services may not function properly.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c2154083-8137-4184-9cbd-d8cda3dc56a0": {"__data__": {"id_": "c2154083-8137-4184-9cbd-d8cda3dc56a0", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "768217be-95f5-4827-93d1-0fce03320ca7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d7fadec146d8924460f78bee8f4984339c4e71caac12fda6f1a2ce181441240c", "class_name": "RelatedNodeInfo"}}, "text": "Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly identifiable\npersonal information, but are based on uniquely identifying your browser and\ninternet device. If you do not allow these cookies, you will experience less\ntargeted advertising.\n\nCookies Details\u200e\n\nBack Button", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "90a6c7e9-4b00-4e51-9c4d-512bf5d639b6": {"__data__": {"id_": "90a6c7e9-4b00-4e51-9c4d-512bf5d639b6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1070558e-1571-4897-840b-14d0f7aaf36c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cc0ad6d7d2e7433ec10af457afbd06507a6fc66b9ef9bc4127a3a275532da65a", "class_name": "RelatedNodeInfo"}}, "text": "Cookie List\n\nFilter Button\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[![Powered by\nOnetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-\nconsent/)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c7707de-f394-4264-aa7a-2d470ee00f17": {"__data__": {"id_": "1c7707de-f394-4264-aa7a-2d470ee00f17", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e123543-6967-4d54-b0bc-43260d86f3ee", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c034020c681c34a0c7784c91abf45794b5b2393bdc5542807ad9766ad5d481fd", "class_name": "RelatedNodeInfo"}}, "text": "1. Overview\n  2. Set up of environment\n  3. Setting up our dbt Project\n  4. Creating our CSV data files in dbt\n  5. Creating our dbt models in models folder\n  6. Preparing our Airflow Environment\n  7. Building Our dbt DAG\n  8. Starting Airflow Environment & Adding Connections\n  9. Running our docker-compose file for Airflow\n  10. Incorporating Snowpark\n  11. Creating a DAG with Cosmos and Snowpark\n  12. Running our new DAG\n  13. Conclusion\n\n[ _bug_report_ Report a mistake](https://github.com/Snowflake-\nLabs/sfguides/issues)\n\n _close_ _menu_", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aeffc19e-b955-4967-ae15-f84aaecd67ca": {"__data__": {"id_": "aeffc19e-b955-4967-ae15-f84aaecd67ca", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d84de500-3e2b-4d48-a8eb-4043be4cb958", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "952cbe4da904bc59366f4d7519f7c7816a781a0671bc8a05962c1acb631b0e0e", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering with Apache Airflow, Snowflake, Snowpark, dbt & Cosmos\n\n _access_time_ 46 mins remaining", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d475db3-41dc-4bad-b9d7-ac51a7a43026": {"__data__": {"id_": "0d475db3-41dc-4bad-b9d7-ac51a7a43026", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f196f29c-9898-42c6-a88b-bf11d66c381b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9c932a48adf151f3e3d6a52d7267e0ba415d95d3651130c843763588803cec7c", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Overview\n\n!architecture\n\nNumerous business are looking at modern data strategy built on platforms that\ncould support agility, growth and operational efficiency. Snowflake is Data\nCloud, a future proof solution that can simplify data pipelines for all your\nbusinesses so you can focus on your data and analytics instead of\ninfrastructure management and maintenance.\n\nApache Airflow is an open-source workflow management platform that can be used\nto author and manage data pipelines. Airflow uses workflows made of directed\nacyclic graphs (DAGs) of tasks. The [Astro\nCLI](https://docs.astronomer.io/astro/cli/overview) is a command line\ninterface for Airflow developed by Astronomer. It's the easiest way to get\nstarted with running Apache Airflow locally\n\ndbt is a modern data engineering framework\nmaintained by dbt Labs that is becoming very\npopular in modern data architectures, leveraging cloud data platforms like\nSnowflake. dbt CLI is the\ncommand line interface for running dbt projects. The CLI is free to use and\nopen source.\n\ncosmos is an\nOpen-Source project that enables you to run your dbt Core projects as Apache\nAirflow DAGs and Task Groups with a few lines of code.\n\nSnowflake's Snowpark is a\ndeveloper experience feature introduced by Snowflake to allow data engineers,\ndata scientists, and developers to write code in familiar programming\nlanguages, such as Python, and execute it directly within the Snowflake Data\nCloud. Snowpark provides a set of native libraries that make it easier to\nbuild complex data transformations, UDFs (User-Defined Functions), and data\npipelines without having to rely heavily on SQL. This not only makes it more\napproachable for those who aren't SQL experts but also enables leveraging the\nfull power and scalability of Snowflake's platform.\n\nIn this virtual hands-on lab, you will follow a step-by-step guide to using\nAirflow with dbt to create scheduled data transformation jobs. Then, you'll\nlearn how you can make use of this data within Snowpark for further analysis\nvia Python and Pandas transformations.\n\nLet's get started.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0cf1b229-5b3d-442b-93ed-7bfb11cc2aab": {"__data__": {"id_": "0cf1b229-5b3d-442b-93ed-7bfb11cc2aab", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf8c9660-008d-4b09-84fe-d3503830c223", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "0077447c34841884bf4716dfaca02e600179d31216137ec72c26a71209cc5f28", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\nThis guide assumes you have a basic working knowledge of Python, SQL and dbt", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c393514-7a78-4fa6-b116-e710527cb20a": {"__data__": {"id_": "7c393514-7a78-4fa6-b116-e710527cb20a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8ae9318-28d9-45d5-8281-af39698cf6c1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1743a4cdfc69dc44cb096233a9829545da1449d0b507ed2f3828f34a17547d45", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * how to use an opensource tool like Airflow to create a data scheduler\n  * how do we write a DAG and upload it onto Airflow\n  * how to build scalable pipelines using dbt, Airflow and Snowflake\n  * How to use Snowpark to interact with your Snowflake data using Python", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb5cac62-8a8e-4b0f-84cb-602f79eae657": {"__data__": {"id_": "cb5cac62-8a8e-4b0f-84cb-602f79eae657", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef20831b-1347-4943-88b0-20c8feaeb95b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "16db2c4686707854683ea297b5df7c27b111f109e410ec568db0c91558e55d43", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  1. Snowflake\n  2. **A Snowflake Account.**\n  3. **A Snowflake User created with appropriate permissions.** This user will need permission to create objects in the DEMO_DB database.\n  4. **Snowpark Enabled**\n  5. GitHub\n  6. **A GitHub Account.** If you don't already have a GitHub account you can create one for free. Visit the Join GitHub page to get started.\n  7. Integrated Development Environment (IDE)\n  8. **Your favorite IDE with Git integration.** If you don't already have a favorite IDE that integrates with Git I would recommend the great, free, open-source Visual Studio Code.\n  9. Docker Desktop\n  10. **Docker Desktop on your laptop.** We will be running Airflow as a container. Please install Docker Desktop on your desired OS by following the Docker setup instructions.\n  11. OpenAI API key\n  12. **Optional Step to Enable Chatbot Functionality**\n  13. Astro CLI\n  14. **The Astro CLI Installed.** We will be using the Astro CLI to create our Airflow environments. Please install the Astro CLI on your desired OS by following the Astro CLI setup instructions", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5aa29ed7-fa53-4131-b69d-02503a6d9968": {"__data__": {"id_": "5aa29ed7-fa53-4131-b69d-02503a6d9968", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f0920b8-0f84-45b1-bf24-f9c08140496c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "bda32b5aae5dd96bb08b79e2e4327f2b54034bb1528d1147bada8197cc67796e", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * A simple working Airflow pipeline with dbt and Snowflake\n  * A slightly more complex Airflow pipeline that incorporates Snowpark to analyze your data with Python", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ee0c215b-8152-48fb-b5ad-4d7f0c56fe7c": {"__data__": {"id_": "ee0c215b-8152-48fb-b5ad-4d7f0c56fe7c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d2987ff-b8b2-43de-8580-453697c32c65", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e4af435a25667f963bbe73e86d1b6ed1b8b3ae8e02e7e1131f5ca5f68dd3107d", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Set up of environment\n\nFirst, let us create a folder by running the command below\n\n    \n    \n    mkdir dbt_airflow && cd dbt_airflow\n\nNext, we will use the Astro CLI to create a new Astro project by running the\nfollowing command. An Astro project contains the set of files necessary to run\nAirflow, including dedicated folders for your DAG files, plugins, and\ndependencies.\n\n    \n    \n    astro dev init\n\nNow, navigate into the DAG's folder that the Astro CLI created, and create a\nnew folder called dbt by running the following command.\n\n    \n    \n    mkdir dbt && cd dbt\n\nNext, run the following command to install dbt and create all the necessary\nfolders for your project. It will prompt you for a name for your project,\nenter \u2018cosmosproject'.\n\n    \n    \n    dbt init\n\nYour tree repository should look like this\n\n!Folderstructure", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7145d89d-b33d-47af-a6a6-6a8b5d9435e8": {"__data__": {"id_": "7145d89d-b33d-47af-a6a6-6a8b5d9435e8", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ee05085-cccc-4bae-b76a-17e5e282cf37", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dba8c2af61ad3308c54f6bb0b64a5baff487cdcd8e8cba3b7655463b20901d96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f1c7b4-8fe1-4035-8ce5-5897e3bcf694", "node_type": "1", "metadata": {}, "hash": "7dffb824114b61aa716e2339e6dcbd5896d6202789ab2fde459f070b1defd54f", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Setting up our dbt Project\n\nNow that we have gotten our repo up, it is time to configure and set up our\ndbt project.\n\nBefore we begin, let's take some time to understand what we are going to do\nfor our dbt project.\n\nAs can be seen in the diagram below, we have 3 csv files `bookings_1`,\n`bookings_2` and `customers` . We are going to seed these csv files into\nSnowflake as tables. This will be detailed later.\n\nFollowing this, we are going to use dbt to merge `bookings_1` and `bookings_2`\ntables into `combined_bookings`. Then, we are going to join the\n`combined_bookings` and `customer` table on customer_id to form the\n`prepped_data` table.\n\nFinally, we are going to perform our analysis and transformation on the\n`prepped_data` by creating 2 views.\n\n  1. `hotel_count_by_day.sql`: This will create a hotel_count_by_day view in the ANALYSIS schema in which we will count the number of hotel bookings by day.\n  2. `thirty_day_avg_cost.sql`: This will create a thirty_day_avg_cost view in the ANALYSIS schema in which we will do a average cost of booking for the last 30 days.\n\n!dbt_structure\n\nFirst, let's go to the Snowflake console and run the script below after\nreplacing the field with your password of choice. What this does is create a\ndbt_user and a dbt_dev_role and after which we set up a database for dbt_user.\n\n    \n    \n    USE ROLE SECURITYADMIN;\n    \n    CREATE OR REPLACE ROLE dbt_DEV_ROLE COMMENT='dbt_DEV_ROLE';\n    GRANT ROLE dbt_DEV_ROLE TO ROLE SYSADMIN;\n    \n    CREATE OR REPLACE USER dbt_USER PASSWORD=''\n    \tDEFAULT_ROLE=dbt_DEV_ROLE\n    \tDEFAULT_WAREHOUSE=dbt_WH\n    \tCOMMENT='dbt User';\n        \n    GRANT ROLE dbt_DEV_ROLE TO USER dbt_USER;\n    \n    -- Grant privileges to role\n    USE ROLE ACCOUNTADMIN;\n    \n    GRANT CREATE DATABASE ON ACCOUNT TO ROLE dbt_DEV_ROLE;\n    \n    /*---------------------------------------------------------------------------\n    Next we will create a virtual warehouse that will be used\n    ---------------------------------------------------------------------------*/\n    USE ROLE SYSADMIN;\n    \n    --Create Warehouse for dbt work\n    CREATE OR REPLACE WAREHOUSE dbt_DEV_WH\n      WITH WAREHOUSE_SIZE = 'XSMALL'\n      AUTO_SUSPEND = 120\n      AUTO_RESUME = true\n      INITIALLY_SUSPENDED = TRUE;\n    \n    GRANT ALL ON WAREHOUSE dbt_DEV_WH TO ROLE dbt_DEV_ROLE;\n    \n\nLet's login with the `dbt_user` and create the database `DEMO_dbt` by running\nthe command\n\n    \n    \n    CREATE OR REPLACE DATABASE DEMO_dbt\n    \n\nThen, in the new `Demo_dbt` database, copy and paste the following sql\nstatements to create our `bookings_1`, `bookings_2` and `customers` tables\nwithin Snowflake\n\n    \n    \n    CREATE TABLE bookings_1 (\n        id INTEGER,\n        booking_reference INTEGER,\n        hotel STRING,\n        booking_date DATE,\n        cost INTEGER\n    );\n    CREATE TABLE bookings_2 (\n        id INTEGER,\n        booking_reference INTEGER,\n        hotel STRING,\n        booking_date DATE,\n        cost INTEGER\n    );\n    CREATE TABLE customers (\n        id INTEGER,\n        first_name STRING,\n        last_name STRING,\n        birthdate DATE,\n        membership_no INTEGER\n    );\n    \n\nAfter you're done, you should have a folder structure that looks like the\nbelow:\n\n!airflow\n\nNow, let's go back to our project `cosmosproject` > `dbt`that we set up\npreviously.\n\nWe will set up a couple configurations for the respective files below. Please\nnote for the `dbt_project.yml` you just need to replace the models section\n\npackages.yml (Create in `cosmosproject` folder if not already present)\n\n    \n    \n    packages:\n      - package: dbt-labs/dbt_utils\n        version: [\">=1.0.0\", \"<2.0.0\"]\n\ndbt_project.yml\n\n    \n    \n    models:\n      my_new_project:\n          # Applies to all files under models/example/\n          transform:\n              schema: transform\n              materialized: view\n          analysis:\n              schema: analysis\n              materialized: view\n\nNext, we will install the `dbt-labs/dbt_utils` that we had placed inside\n`packages.yml`. This can be done by running the command `dbt deps` from the\n`cosmosproject` folder.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 4111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "69f1c7b4-8fe1-4035-8ce5-5897e3bcf694": {"__data__": {"id_": "69f1c7b4-8fe1-4035-8ce5-5897e3bcf694", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ee05085-cccc-4bae-b76a-17e5e282cf37", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dba8c2af61ad3308c54f6bb0b64a5baff487cdcd8e8cba3b7655463b20901d96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7145d89d-b33d-47af-a6a6-6a8b5d9435e8", "node_type": "1", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ab5c1418291278b24bd79af61f0c54554472721691385caa0dcdf9a90c8ab243", "class_name": "RelatedNodeInfo"}}, "text": "We will set up a couple configurations for the respective files below. Please\nnote for the `dbt_project.yml` you just need to replace the models section\n\npackages.yml (Create in `cosmosproject` folder if not already present)\n\n    \n    \n    packages:\n      - package: dbt-labs/dbt_utils\n        version: [\">=1.0.0\", \"<2.0.0\"]\n\ndbt_project.yml\n\n    \n    \n    models:\n      my_new_project:\n          # Applies to all files under models/example/\n          transform:\n              schema: transform\n              materialized: view\n          analysis:\n              schema: analysis\n              materialized: view\n\nNext, we will install the `dbt-labs/dbt_utils` that we had placed inside\n`packages.yml`. This can be done by running the command `dbt deps` from the\n`cosmosproject` folder.\n\nWe will now create a file called `custom_demo_macros.sql` under the `macros`\nfolder and input the below sql\n\n    \n    \n    {% macro generate_schema_name(custom_schema_name, node) -%}\n        {%- set default_schema = target.schema -%}\n        {%- if custom_schema_name is none -%}\n            {{ default_schema }}\n        {%- else -%}\n            {{ custom_schema_name | trim }}\n        {%- endif -%}\n    {%- endmacro %}\n    \n    \n    {% macro set_query_tag() -%}\n      {% set new_query_tag = model.name %} {# always use model name #}\n      {% if new_query_tag %}\n        {% set original_query_tag = get_current_query_tag() %}\n        {{ log(\"Setting query_tag to '\" ~ new_query_tag ~ \"'. Will reset to '\" ~ original_query_tag ~ \"' after materialization.\") }}\n        {% do run_query(\"alter session set query_tag = '{}'\".format(new_query_tag)) %}\n        {{ return(original_query_tag)}}\n      {% endif %}\n      {{ return(none)}}\n    {% endmacro %}\n\nNow we are done setting up our dbt environment. Your file structure should\nlook like the below screenshot:\n\n!airflow", "mimetype": "text/plain", "start_char_idx": 3326, "end_char_idx": 5177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c5a452c5-b0f9-4ea2-9953-8d5cf5f8c1ed": {"__data__": {"id_": "c5a452c5-b0f9-4ea2-9953-8d5cf5f8c1ed", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "828605ba-5131-4e7e-8acb-6f58ee4f9e30", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "65514bfc2cfb8337bc68bc3adeaaa9bd20b3ba042ae82cc7c6e82cbf74df5b43", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Creating our CSV data files in dbt\n\nIn this section, we will be prepping our sample csv data files alongside the\nassociated sql models.\n\nTo start, let us first create 3 excel files under the folder `data` inside the\ndbt folder.\n\nbookings_1.csv\n\n    \n    \n    id,booking_reference,hotel,booking_date,cost\n    1,232323231,Pan Pacific,2021-03-19,100\n    1,232323232,Fullerton,2021-03-20,200\n    1,232323233,Fullerton,2021-04-20,300\n    1,232323234,Jackson Square,2021-03-21,400\n    1,232323235,Mayflower,2021-06-20,500\n    1,232323236,Suncity,2021-03-19,600\n    1,232323237,Fullerton,2021-08-20,700\n\nbookings_2.csv\n\n    \n    \n    id,booking_reference,hotel,booking_date,cost\n    2,332323231,Fullerton,2021-03-19,100\n    2,332323232,Jackson Square,2021-03-20,300\n    2,332323233,Suncity,2021-03-20,300\n    2,332323234,Jackson Square,2021-03-21,300\n    2,332323235,Fullerton,2021-06-20,300\n    2,332323236,Suncity,2021-03-19,300\n    2,332323237,Berkly,2021-05-20,200\n\ncustomers.csv\n\n    \n    \n    id,first_name,last_name,birthdate,membership_no\n    1,jim,jone,1989-03-19,12334\n    2,adrian,lee,1990-03-10,12323\n\nOur folder structure should be like as below\n\n!airflow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "762461c3-1ef7-4dfc-84fa-600fe234ec74": {"__data__": {"id_": "762461c3-1ef7-4dfc-84fa-600fe234ec74", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d08dfede-3afb-4489-822a-7baa362c62b7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "33e12071bdb83f37ed646dc6f5a7495b2670c28c5724da885106efc142eff5a5", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Creating our dbt models in models folder\n\nCreate 2 folders `analysis` and `transform` in the models folder. Please\nfollow the sections below for analysis and transform respectively.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4288bf7b-dcc8-40c8-b1de-d8b47c34c282": {"__data__": {"id_": "4288bf7b-dcc8-40c8-b1de-d8b47c34c282", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b7c521c-b322-4871-b154-cb009d64055d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "fdd8450f112f89b47abb09fc32dc32ab60f155070d2a4fee02863bdf5d805473", "class_name": "RelatedNodeInfo"}}, "text": "dbt models for transform folder\n\nInside the `transform` folder, we will have 3 SQL files\n\n  1. `combined_bookings.sql`: This will combine the 2 bookings CSV files we had above and create the `COMBINED_BOOKINGS` view in the `TRANSFORM` schema.\n\ncombined_bookings.sql\n\n    \n    \n    {{ dbt_utils.union_relations(\n        relations=[ref('bookings_1'), ref('bookings_2')]\n    ) }}\n\n  2. `customer.sql`: This will create a `CUSTOMER` view in the `TRANSFORM` schema.\n\ncustomer.sql\n\n    \n    \n    SELECT ID \n        , FIRST_NAME\n        , LAST_NAME\n        , birthdate\n    FROM {{ ref('customers') }}\n\n  3. `prepped_data.sql`: This will create a `PREPPED_DATA` view in the `TRANSFORM` schema in which it will perform an inner join on the `CUSTOMER` and `COMBINED_BOOKINGS` views from the steps above.\n\nprepped_data.sql\n\n    \n    \n    SELECT A.ID \n        , FIRST_NAME\n        , LAST_NAME\n        , birthdate\n        , BOOKING_REFERENCE\n        , HOTEL\n        , BOOKING_DATE\n        , COST\n    FROM {{ref('customer')}}  A\n    JOIN {{ref('combined_bookings')}} B\n    on A.ID = B.ID", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2ecec2c-ffa6-4aaf-b106-5dff918770d4": {"__data__": {"id_": "a2ecec2c-ffa6-4aaf-b106-5dff918770d4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6712de-db0a-4ce5-b7af-044b7e734d8d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5900798a5778691084edce538bd235c5e6881dfbeef61162a73716b0bdf881af", "class_name": "RelatedNodeInfo"}}, "text": "dbt models for analysis folder\n\nNow let's move on to the `analysis` folder. Change to the `analysis` folder\nand create these 2 SQL files\n\n  1. `hotel_count_by_day.sql`: This will create a hotel_count_by_day view in the `ANALYSIS` schema in which we will count the number of hotel bookings by day.\n\n    \n    \n    SELECT\n      BOOKING_DATE,\n      HOTEL,\n      COUNT(ID) as count_bookings\n    FROM {{ ref('prepped_data') }}\n    GROUP BY\n      BOOKING_DATE,\n      HOTEL\n\n  2. `thirty_day_avg_cost.sql`: This will create a thirty_day_avg_cost view in the `ANALYSIS` schema in which we will do a average cost of booking for the last 30 days.\n\n    \n    \n    SELECT\n      BOOKING_DATE,\n      HOTEL,\n      COST,\n      AVG(COST) OVER (\n        ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n      ) as \"30_DAY_AVG_COST\",\n      COST -   AVG(COST) OVER (\n        ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n      ) as \"DIFF_BTW_ACTUAL_AVG\"\n    FROM {{ ref('prepped_data') }}\n\nYour file structure should look like the below screenshot. We have now\nfinished our dbt models and can proceed to working on using Airflow to manage\nthem.\n\n!airflow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dcd8b469-9fb2-481d-978d-575e744d615c": {"__data__": {"id_": "dcd8b469-9fb2-481d-978d-575e744d615c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a2633dc-9e5e-4263-bc05-80db13c174d2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b1bdb3dfbdbeecf649a6a7278f3c4a21d6460ea550440d7c355085dd94574f4c", "class_name": "RelatedNodeInfo"}}, "text": "6\\. Preparing our Airflow Environment\n\nNow going back to your Airflow directory, open up the requirements.txt file\nthat the Astro CLI created. Copy and paste the following text block to install\nthe Cosmos and\nSnowflake libraries for Airflow. Cosmos will be used to turn each dbt model\ninto a task/task group complete with retries, alerting, etc.\n\n    \n    \n    astronomer-cosmos\n    apache-airflow-providers-snowflake\n\nNext, open up the Dockerfile in your Airflow folder and copy and paste the\nfollowing code block to overwrite your existing Dockerfile. These changes will\ncreate a virtual environment for dbt along with the adapter to connect to\nSnowflake. It's recommended to use a virtual environment because dbt and\nAirflow can have conflicting dependencies.\n\n    \n    \n    # syntax=quay.io/astronomer/airflow-extensions:latest\n    \n    FROM quay.io/astronomer/astro-runtime:9.1.0-python-3.9-base\n    \n    RUN python -m venv dbt_venv && source dbt_venv/bin/activate && pip install --no-cache-dir dbt-snowflake && deactivate", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "498666a1-d7b0-4fe2-acfd-5de631d924dd": {"__data__": {"id_": "498666a1-d7b0-4fe2-acfd-5de631d924dd", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75839b32-9fad-4693-925b-b66c76b23b3c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6ca5996aefc16e245dc7f840c8269fe7dea8c85f02d0ead18c5a3ecec6192229", "class_name": "RelatedNodeInfo"}}, "text": "7\\. Building Our dbt DAG\n\nNow that our Airflow environment is set up, lets create our DAG! Instead of\nusing the conventional DAG definition methods, we'll be using Cosmos' dbtDAG\nclass to create a DAG based on our dbt models. This allows us to turn our dbt\nprojects into Apache Airflow DAGs and Task Groups with a few lines of code. To\ndo so, create a new file in the `dags` folder called `my_cosmos_dag.py` and\ncopy and paste the following code block into the file.\n\n    \n    \n    from datetime import datetime\n    import os\n    from cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig\n    from cosmos.profiles import SnowflakeUserPasswordProfileMapping\n    from pathlib import Path\n    \n    dbt_project_path = Path(\"/usr/local/airflow/dags/dbt/cosmosproject\")\n    \n    profile_config = ProfileConfig(profile_name=\"default\",\n                                   target_name=\"dev\",\n                                   profile_mapping=SnowflakeUserPasswordProfileMapping(conn_id=\"snowflake_default\", \n                                                        profile_args={\n                                                            \"database\": \"demo_dbt\",\n                                                            \"schema\": \"public\"\n                                                            },\n                                                        ))\n    \n    \n    dbt_snowflake_dag = DbtDag(project_config=ProjectConfig(dbt_project_path,),\n                        operator_args={\"install_deps\": True},\n                        profile_config=profile_config,\n                        execution_config=ExecutionConfig(dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",),\n                        schedule_interval=\"@daily\",\n                        start_date=datetime(2023, 9, 10),\n                        catchup=False,\n                        dag_id=\"dbt_snowflake_dag\",)\n    \n\nFirst, we import the various Cosmos libraries\n\n\u2022 DbtDag: This is a class that allows you to create an Apache Airflow Directed\nAcyclic Graph (DAG) for a dbt (Data Build Tool) project. The DAG will execute\nthe dbt project according to the specified configuration.\n\n\u2022 ProjectConfig: This class is used to specify the configuration for the dbt\nproject that the DbtDag will execute by pointing it to the path tfor your dbt\nproject.\n\n\u2022 ProfileConfig: This class is used to specify the configuration for the\ndatabase profile that dbt will use when executing the project. This includes\nthe profile name, target name, and any necessary mapping to Airflow\nconnections.\n\n\u2022 ExecutionConfig: This class is used to specify any additional configuration\nfor executing the dbt project. We'll be pointing it to the virtual environment\nwe created at `{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt` in our\n`Dockerfile` to execute in.\n\n\u2022 PostgresUserPasswordProfileMapping and SnowflakeUserPasswordProfileMapping:\nThese classes are used to map Airflow connections to dbt profiles for\nPostgreSQL and Snowflake databases, respectively. This allows you to manage\nyour database credentials in Airflow and use them in dbt.\n\nAfter the imports, a ProfileConfig object is created, which is used to define\nthe configuration for the Snowflake connection. The\nSnowflakeUserPasswordProfileMapping class is used to map the Snowflake\nconnection in Airflow to a dbt profile. The DbtDag object is then created.\nThis object represents an Airflow DAG that will execute a dbt project. It\ntakes several parameters:\n\n`project_config` specifies the path to the dbt project we created at\n`/usr/local/airflow/dags/dbt/cosmosprojectoperator_args` is used to pass\narguments to the dbt operator. Here, it's specifying that dependencies should\nbe installed. `profile_config` is the profile configuration defined earlier,\nwhich will be used to execute the dbt models in Snowflake. `execution_config`\nspecifies the path to our virtual environment to executue our dbt code in.\n`schedule_interval`, `start_date`, `catchup`, and `dag_id` are standard\nAirflow DAG parameters. You can use any of the standard Airflow DAG parameters\nin a dbtDAG as well.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 4111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e42307c-1e89-4fd3-a115-838fdd8283b6": {"__data__": {"id_": "2e42307c-1e89-4fd3-a115-838fdd8283b6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41154eb6-1efe-48ef-a419-bfc90bf9147d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c2be901d927b539ce1930f40f7a0280b931ea4eb80ca951782c8362780d31e22", "class_name": "RelatedNodeInfo"}}, "text": "8\\. Starting Airflow Environment & Adding Connections\n\nWithin your Airflow `dbt_airflow` directory, enter the below command to start\nyour Airflow environment\n\n    \n    \n    astro dev start", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "916dcd27-48d7-4aa6-b5e3-2fbc2daef0bd": {"__data__": {"id_": "916dcd27-48d7-4aa6-b5e3-2fbc2daef0bd", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca7a12fd-763a-4702-a99c-571c4d379705", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5fb6563cbd32d0c084a1482dd29e47362b9269759b25bb96165ac24d30d836b1", "class_name": "RelatedNodeInfo"}}, "text": "9\\. Running our docker-compose file for Airflow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 49, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db39e369-5f2d-4e16-8262-2785cb7290f4": {"__data__": {"id_": "db39e369-5f2d-4e16-8262-2785cb7290f4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40463d32-8daf-4a36-8770-499d5ccf226d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "0abc2930521b6950819d1d6dbfee0eb204f318bedb489e0f1ea574070a19a078", "class_name": "RelatedNodeInfo"}}, "text": "Running our cosmos_dag!\n\nWe will now run our DAG `cosmos_dag` to see our dbt models in action! If you\nclick the big blue play button on the top left of the screen, you'll see your\ntasks start to run your dbt transformations within your Snowflake database. If\neverything goes smoothly, your Snowflake environment should look like the\nfollowing screenshot:\n\n!airflow\n\nOur `Transform` and `Analysis` views have been created successfully! Open them\nto see the results of our analysis, and check out the other tables to see how\ndata was transformed using dbt.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "04e1b5a9-60bb-49e1-9736-5dd07f82ca8d": {"__data__": {"id_": "04e1b5a9-60bb-49e1-9736-5dd07f82ca8d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d096a7b-6ece-4607-bbb1-bd33aec2bceb", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6c562c2a12410e0ca72fc456f413cfe0755c8947f0217a6aec710f255d4cfc70", "class_name": "RelatedNodeInfo"}}, "text": "10\\. Incorporating Snowpark\n\nNow that we've gotten our dbt DAG set up, lets extend it by adding Snowpark\nfor some data analysis with Python. To do this, we'll need to change some\nexisting files and add new requirements to our local airflow environment.\nWhile we do this, lets stop our Airflow environment by running the following\ncommand so we can restart it later with our changes incorporated.\n\n    \n    \n    astro dev stop\n\nFirst, go to your `packages.txt` file in your root directory and add `build-\nessential` to it, then save. The build-essential package in Linux systems is a\nreference for all the packages needed to compile a Debian package. We'll be\nusing it to create a Python 3.8 Virtual Environment to run our Snowpark code\nin, since Snowpark uses Python 3.8, and Airflow only supports Python versions\n3.9 and above.\n\nNext, we'll need to import the Snowpark provider to create our Snowpark task.\nWhile in development the provider package is not yet in pypi. For this demo,\ndownload the `astro_provider_snowflake-0.0.0-py3-none-any.whl` file from the\nthis [link](https://github.com/astronomer/airflow-snowpark-\ndemo/tree/main/include). Then, copy the downloaded file into your include\ndirectory in your `DBT_Airflow` folder. In the future, this will be a part of\nthe base Snowflake provider, but in the meantime you can use this .whl file in\nany other projects that require it.\n\nAfter that, we'll need to add the .whl file to our `requirements.txt`. Copy\nand paste the following line into your `requirements.txt` file to do so.\n\n    \n    \n    /tmp/astro_provider_snowflake-0.0.0-py3-none-any.whl\n\nFinally, we'll need to create a `requirements-snowpark.txt` to install some\nnecessary packages into the Python VirtualEnv we'll be creating. To do so,\ncreate a file called `requirements-snowpark.txt` in your root Airflow\ndirectory and copy and paste the following code block into it:\n\n    \n    \n    psycopg2-binary\n    snowflake_snowpark_python[pandas]==1.5.1\n    virtualenv\n    /tmp/astro_provider_snowflake-0.0.0-py3-none-any.whl\n\nThese packages will allow us to interact with Snowpark through the virtual\nenvironment we're creating.\n\nNow that we've got our Snowpark provider present, we'll need to edit our\nDockerfile to install it, and spin up the Snowpark Python VirtualEnv. Copy and\npaste the following code block into your Dockerfile to add the necessary\ncommands.\n\n    \n    \n    # syntax=quay.io/astronomer/airflow-extensions:latest\n    \n    FROM quay.io/astronomer/astro-runtime:9.1.0-python-3.9-base\n    \n    COPY include/astro_provider_snowflake-0.0.0-py3-none-any.whl /tmp\n    \n    # Create the virtual environment\n    PYENV 3.8 snowpark requirements-snowpark.txt\n    \n    # Install packages into the virtual environment\n    COPY requirements-snowpark.txt /tmp\n    RUN python3.8 -m pip install -r /tmp/requirements-snowpark.txt\n    \n    \n    RUN python -m venv dbt_venv && source dbt_venv/bin/activate && pip install --no-cache-dir dbt-snowflake && pip install --no-cache-dir dbt-postgres && deactivate\n\nThe first line tells Docker to use the Astronomer provided BuildKit that\nenables us to create virtual environments with the PYENV command. Then we COPY\nin the `.whl` file and use it to create a Python 3.8 VirtualEnv called\nsnowpark.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8e90324-ec46-48a8-a092-55eddabd02ed": {"__data__": {"id_": "b8e90324-ec46-48a8-a092-55eddabd02ed", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb39b745-b644-41a1-872e-94da3cc1e42c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f4f7b47a08a1703005bb14149da0cde8c618c49780bad1cdb07a1f5211986438", "class_name": "RelatedNodeInfo"}}, "text": "11\\. Creating a DAG with Cosmos and Snowpark\n\nIn order to use Cosmos and Snowpark together, we'll need to use Cosmos's\n`dbtTaskGroup` with a normal Airflow DAG instead of a `dbtDAG`. The definition\nfor this is almost identical to the `dbtDAG` approach, and allows us to add\nadditional tasks up or downstream of our dbt workflows. Instead of editing our\nexisting DAG, create a new file called `cosmosandsnowflake.py` in your DAG's\nfolder, and copy the following code into it:\n\n    \n    \n    from airflow.operators.dummy_operator import DummyOperator\n    from astro import sql as aql\n    from astro.files import File\n    from astro.sql.table import Table\n    from cosmos import DbtTaskGroup, ProjectConfig, ProfileConfig, ExecutionConfig\n    from cosmos.profiles import SnowflakeUserPasswordProfileMapping\n    from astronomer.providers.snowflake.utils.snowpark_helpers import SnowparkTable\n    from pathlib import Path\n    \n    dbt_project_path = Path(\"/usr/local/airflow/dags/dbt/cosmosproject\")\n    snowflake_objects = {'demo_database': 'DEMO',\n                         'demo_schema': 'DEMO',\n                         'demo_warehouse': 'COMPUTE_WH',\n                         'demo_xcom_stage': 'XCOM_STAGE',\n                         'demo_xcom_table': 'XCOM_TABLE',\n                         'demo_snowpark_wh': 'SNOWPARK_WH'\n    }\n    _SNOWFLAKE_CONN_ID = \"snowflake_default\"\n    \n    profile_config = ProfileConfig(\n        profile_name=\"default\",\n        target_name=\"dev\",\n        profile_mapping=SnowflakeUserPasswordProfileMapping(\n            conn_id=\"snowflake_default\",\n            profile_args={\n              \"database\": \"demo_dbt\",\n                \"schema\": \"public\"\n            },\n        )\n    )\n    \n    @dag(default_args={\n             \"snowflake_conn_id\": _SNOWFLAKE_CONN_ID,\n             \"temp_data_output\": \"table\",\n             \"temp_data_db\": snowflake_objects['demo_database'],\n             \"temp_data_schema\": snowflake_objects['demo_schema'],\n             \"temp_data_overwrite\": True,\n             \"database\": snowflake_objects['demo_database'],\n             \"schema\": snowflake_objects['demo_schema']\n             },\n        schedule_interval=\"@daily\",\n        start_date=datetime(2023, 9, 10),\n        catchup=False,\n        dag_id=\"dbt_snowpark\",\n    )\n    def dbt_snowpark_dag():\n        transform_data = DbtTaskGroup(\n            group_id=\"transform_data\",\n            project_config=ProjectConfig(dbt_project_path),\n            profile_config=profile_config,\n            execution_config=ExecutionConfig(dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\"),\n            operator_args={\"install_deps\": True},\n        )\n    \n        intermediate = DummyOperator(task_id='intermediate')\n    \n        @task.snowpark_virtualenv(python_version='3.8', requirements=['snowflake-ml-python==1.0.9'])\n        def findbesthotel(snowflake_objects:dict): \n            \n            df = snowpark_session.sql(\"\"\"\n                SELECT *\n                FROM DEMO_DBT.PUBLIC.THIRTY_DAY_AVG_COST\n            \"\"\").to_pandas()\n            highest_cost_hotel = df[df['COST'] == df['COST'].max()]['HOTEL']\n    \n            highest_cost_hotel_str = str(highest_cost_hotel)\n            print(highest_cost_hotel)\n    \n            return highest_cost_hotel_str\n        \n    \n        besthotel = findbesthotel(snowflake_objects)\n        transform_data >> intermediate >> besthotel\n    \n    dbt_snowpark_dag = dbt_snowpark_dag()\n\nThis DAG adds a new Snowpark task called `findbesthotel`, which means it is\nexecuted in a virtual environment with Snowpark and other specified\ndependencies installed. In this case, we installed the `snowflake-ml-\npython==1.0.9` package to install pandas and other popular data science\npackages for data analysis. The `findbesthotel` task connects to a Snowflake\ndatabase and fetches data from the `THIRTY_DAY_AVG_COST` table in the `PUBLIC`\nschema of the `DEMO_DBT` database. It then converts this data into a pandas\nDataFrame and finds the hotel with the highest cost. The name of this hotel is\nconverted to a string, printed, and then returned by the task. All of our dbt\ntransformation set up stays almost identical, aside from changing from using\ndbtDAG to dbtTaskGroup.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 4226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bbbc5c48-7fd2-4ed7-a6aa-5919688f154d": {"__data__": {"id_": "bbbc5c48-7fd2-4ed7-a6aa-5919688f154d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9c0f9cb-6fec-44bc-8ed1-0f2f6b6ab300", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "92c4d9a5a66b1a615fa762cb3b9d405eb9ad8e372cd325d610d26353c3a93d89", "class_name": "RelatedNodeInfo"}}, "text": "12\\. Running our new DAG\n\nNow that we've add Snowpark to our environment and written our DAG, it's time\nto restart our Airflow environment and run it! Restart your Airflow\nenvironment with the following terminal command\n\n    \n    \n    astro dev start\n\nLogin to the Airflow UI the same way as before, and you should see a new dag\ncalled `dbt_snowpark`. Since we already set up our Snowflake connection\nbefore, we can just run this new DAG immediately by clicking its blue play\nbutton. Then, click on the DAG and open up its graph view to watch it run. It\nshould look like the example below:\n\n!snowparkdag\n\nAfter the DAG has finished running, select the `findbesthotel` and open its\nlog file. If all has gone well, you'll see the most expensive hotel to stay at\nprinted out for your convenience!\n\n!airflow", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f59e818b-28d6-497f-9218-8ef20a3b9f08": {"__data__": {"id_": "f59e818b-28d6-497f-9218-8ef20a3b9f08", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b59bd34-a1ea-4d8e-b737-b22ac7698046", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dd7abb5ae62bfff0363e50e6785d0f1936b1f74e605e5190979b2771e27074e8", "class_name": "RelatedNodeInfo"}}, "text": "View Streamlit Dashboard\n\nWe can now view our analyzed data on a Streamlit\ndashboard. To do this, go to terminal and enter the following bash command to\nconnect into the Airflow webserver container.\n\n    \n    \n    astro dev bash -w\n\nThen, run the following command to start a streamlit application.\n\n    \n    \n    cd include/streamlit/src\n    python -m streamlit run ./streamlit_app.py\n\nAfter you've done so, you can view your data dashboard by navigating to\nhttp://localhost:8501/ in your browser! If you'd like to enable the ability to\nask questions about your data, you'll need to add an OpenAI API key in the\n.env file and restart your Airflow environment.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7aa2eff4-307a-43c4-97de-e0173d4f3cc1": {"__data__": {"id_": "7aa2eff4-307a-43c4-97de-e0173d4f3cc1", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ccdc480-3dc8-482d-9c6f-7abe0d886db8", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dd7abb5ae62bfff0363e50e6785d0f1936b1f74e605e5190979b2771e27074e8", "class_name": "RelatedNodeInfo"}}, "text": "View Streamlit Dashboard\n\nWe can now view our analyzed data on a Streamlit\ndashboard. To do this, go to terminal and enter the following bash command to\nconnect into the Airflow webserver container.\n\n    \n    \n    astro dev bash -w\n\nThen, run the following command to start a streamlit application.\n\n    \n    \n    cd include/streamlit/src\n    python -m streamlit run ./streamlit_app.py\n\nAfter you've done so, you can view your data dashboard by navigating to\nhttp://localhost:8501/ in your browser! If you'd like to enable the ability to\nask questions about your data, you'll need to add an OpenAI API key in the\n.env file and restart your Airflow environment.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8386241a-f297-47b1-8f02-3fe4471a473d": {"__data__": {"id_": "8386241a-f297-47b1-8f02-3fe4471a473d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98294a4b-8e60-4209-9d18-6a97ca746522", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "af25dcfd96bb17ee72053eeb4e704def04ebdbf1da67a939fe33dfadeb5624c5", "class_name": "RelatedNodeInfo"}}, "text": "13\\. Conclusion\n\nCongratulations! You have created your first Apache Airflow DAG with dbt,\nCosmos, Snowflake, and Snowpark! We encourage you to continue with your free\ntrial by loading your own sample or production data and by using some of the\nmore advanced capabilities of Airflow and Snowflake not covered in this lab.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f830c562-8ba3-44cd-a4fc-0304d76ac599": {"__data__": {"id_": "f830c562-8ba3-44cd-a4fc-0304d76ac599", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd6b45a7-b792-4530-a976-dae2d96a08b9", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5fbb484464c7629906e74c4dce4cbb1b2eeabb3d1880ce2a9790256e3966538c", "class_name": "RelatedNodeInfo"}}, "text": "Additional Resources:\n\n  * Join our dbt community Slack which contains more than 18,000 data practitioners today. We have a dedicated slack channel #db-snowflake to Snowflake related content.\n  * Quick tutorial on how to write a simple Airflow DAG\n  * Documentation on how to use Cosmos to render dbt workflows in Airflow Cosmos", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5b45e197-453b-4179-ae28-61f5741c8d48": {"__data__": {"id_": "5b45e197-453b-4179-ae28-61f5741c8d48", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fcc786f0-f412-41fc-88d7-7af3ff617a73", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9226f9a9e30f03ce2934fa94f66a05d8f9faab4f973cd9080dcce30a74848854", "class_name": "RelatedNodeInfo"}}, "text": "What we've covered:\n\n  * How to set up Airflow, dbt & Snowflake\n  * How to create a dbt DAG using Cosmos to run dbt models using Airflow\n\nBack\n\nNextDone\n\n!\n!\n\n![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=17e17d3f-d0ad-4846-b7db-84ad6b5759fa&sid=91c2da30865a11efb27035fd67b05c34&vid=91c302e0865a11ef97873b231b35e853&vids=1&msclkid=N&pi=0&lg=en-\nUS&sw=1800&sh=1169&sc=30&nwd=1&tl=Data%20Engineering%20with%20Apache%20Airflow,%20Snowflake,%20Snowpark,%20dbt%20%26%20Cosmos&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fdata_engineering_with_apache_airflow%2F%230&r=&lt=647&evt=pageLoad&sv=1&cdb=AQAQ&rn=730848)!\n\n!\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af77e483-1b0f-4f46-a24f-a6d1a9ca54b0": {"__data__": {"id_": "af77e483-1b0f-4f46-a24f-a6d1a9ca54b0", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfb3ed91-2fff-4c1c-a57b-07476bc54a56", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6f68b32e0d79e10258d637bf4e64742543096a526f8d859799e59080fd214e2b", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Use of Cookies\n\nWe use cookies to enhance your experience and to analyze site traffic as\ndescribed in our Cookie Statement. By accepting, you consent to our use of\ncookies.[Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-\nstatement/)\n\nCookies Settings Reject All Accept All Cookies\n\n![Company\nLogo](https://cdn.cookielaw.org/logos/cb85e692-4053-4d0a-8dda-d24b5daa8b06/ff6c124b-1473-4861-9ca3-9eaf6debb37d/SNO-\nSnowflakeLogo_blue.png)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "882434e1-92d6-489a-99a7-67aadb38ae59": {"__data__": {"id_": "882434e1-92d6-489a-99a7-67aadb38ae59", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6dd8cb00-c892-41df-a601-7d9ecdd8c329", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9d9ba4da7309f0ba29b00109c83d5bbf33a617847fd1622aea5e84fed0b7c0e8", "class_name": "RelatedNodeInfo"}}, "text": "Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f807e6a9-6f78-4400-836b-451cb960375b": {"__data__": {"id_": "f807e6a9-6f78-4400-836b-451cb960375b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38d78a61-473a-4827-b6c4-da6dcf9941f9", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5391a9ffe1c3ca52c9ae99ee1c0088830beb2f0af106d9a2a1a4feaa4b6b5888", "class_name": "RelatedNodeInfo"}}, "text": "Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.  \nMore information", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c3a62d4-a515-4616-a8c9-9b121e0a8954": {"__data__": {"id_": "4c3a62d4-a515-4616-a8c9-9b121e0a8954", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad594b28-ca2a-4efe-ab33-34d74644e6fe", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ed6be61649a10838ad645f5513441acfd2e0e16baf1926a4d91d31a2ff73097a", "class_name": "RelatedNodeInfo"}}, "text": "Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff. They are usually only set in response to actions made by you which amount\nto a request for services, such as setting your privacy preferences, logging\nin or filling in forms. You can set your browser to block or alert you about\nthese cookies, but some parts of the site will not then work. These cookies do\nnot store any personally identifiable information.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d18e74c6-aed3-4ecb-bfab-28292bd17697": {"__data__": {"id_": "d18e74c6-aed3-4ecb-bfab-28292bd17697", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4a63ea7-31af-4e4e-b525-98b9f6562d2e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7c6c59433e19e3021c2db4ecf442baff9b7f79b4b23a24a7d0959da1a6f4e70c", "class_name": "RelatedNodeInfo"}}, "text": "Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.    All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "29039804-9cfd-4cc5-943f-2a817869666b": {"__data__": {"id_": "29039804-9cfd-4cc5-943f-2a817869666b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d269188-7a16-434c-8bde-f23e8896f10d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "eedec6a06bcb2bcd94a04b09ac778df96fc5af7abdb0572a2d734cdfec204f20", "class_name": "RelatedNodeInfo"}}, "text": "Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages.    If you do not allow these cookies then\nsome or all of these services may not function properly.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8d2c07b-9d4e-4d1d-8216-e5a36d512466": {"__data__": {"id_": "b8d2c07b-9d4e-4d1d-8216-e5a36d512466", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9b8fa70-5582-4dc6-8999-b43d6b85d217", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2562bcda001cc05fa0610c60089a93d8386c8aa6c8f73e5f4edcb113ede81504", "class_name": "RelatedNodeInfo"}}, "text": "Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly identifiable\npersonal information, but are based on uniquely identifying your browser and\ninternet device. If you do not allow these cookies, you will experience less\ntargeted advertising.\n\nCookies Details\u200e\n\nBack Button", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3c58786-4fcb-41a7-92e6-1c26c23e5f49": {"__data__": {"id_": "c3c58786-4fcb-41a7-92e6-1c26c23e5f49", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9d9002a-756c-48f4-9bcb-3107519b31a2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "7225181edbcff9f956969ca7c43674f42679df2198cc4857579aa08b2082ab40", "class_name": "RelatedNodeInfo"}}, "text": "Cookie List\n\nFilter Button\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[![Powered by\nOnetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-\nconsent/)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43bc2fcd-08a3-4eea-ab8b-62811f76b31d": {"__data__": {"id_": "43bc2fcd-08a3-4eea-ab8b-62811f76b31d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "738bf1a2-eaa3-4dde-91da-dee0de861383", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6a9c210216190b3ed95c51332ed2077f67b0adad2c9503a97168702ad534deb2", "class_name": "RelatedNodeInfo"}}, "text": "1. Overview\n  2. Create the dbt Project\n  3. Create a Simple Python Model\n  4. Understand How the Simple dbt Python Model Works\n  5. Create a Python Model with a UDF\n  6. Understand How the dbt Python Model with a UDF Works\n  7. Conclusion & Next Steps\n\n[ _bug_report_ Report a mistake](https://github.com/Snowflake-\nLabs/sfguides/issues)\n\n _close_ _menu_", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27c6a234-3a46-493d-83cf-98070463b42e": {"__data__": {"id_": "27c6a234-3a46-493d-83cf-98070463b42e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "808c1c41-6d28-441d-81cb-b6120b8f9dab", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6e37f75e1b0a950853771e46699ec9c02422601543c07e31f212503edd846a66", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering with Snowpark Python and dbt\n\n _access_time_ 54 mins remaining", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e0f8ecd-4e52-48e6-b294-5f8afef9b61f": {"__data__": {"id_": "8e0f8ecd-4e52-48e6-b294-5f8afef9b61f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af78da87-a865-428e-b2d3-940b2483d507", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3a07da1fcee4fb72b93f7810bfa50f9cc12d2b3032b3e5e95590d7a53de204ff", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Overview\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 17, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f833fc80-f5b2-43c2-b0c6-0ea92d6c935e": {"__data__": {"id_": "f833fc80-f5b2-43c2-b0c6-0ea92d6c935e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b1ded5d-f7ae-48c5-9d8d-fe2bc4859c33", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "bbe2094a5bc830f0099925d95b0c997dea61bb5196a09f53b5feb0967f9be532", "class_name": "RelatedNodeInfo"}}, "text": "Background\n\nData engineering is a broad discipline which includes data ingestion, data\ntransformation, and data consumption, along with the accompanying SDLC best\npractices (i.e. DevOps). Data engineers employ different tools and approaches\ndepending on the phase. For this Quickstart we will focus on the data\ntransformation phase in particular.\n\nData transformation involves taking source data which has been ingested into\nyour data platform and cleansing it, combining it, and modeling it for\ndownstream use. Historically the most popular way to transform data has been\nwith the SQL language and data engineers have built data transformation\npipelines using SQL often with the help of ETL/ELT tools. But recently many\nfolks have also begun adopting the DataFrame API in languages like Python for\nthis task. For the most part a data engineer can accomplish the same data\ntransformations with either approach, and deciding between the two is mostly a\nmatter of preference and particular use cases. That being said, there are use\ncases where a particular data transform can't be expressed in SQL and a\ndifferent approach is needed. The most popular approach for these use cases is\nPython along with a DataFrame API.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35e7b24e-6c4c-4254-9756-0d4129057640": {"__data__": {"id_": "35e7b24e-6c4c-4254-9756-0d4129057640", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95437d44-1e4a-4f50-a06b-a70eca0d15b0", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dd604fbb223a9195e4ae61975cff9bd6219d6947e7a492b77c6ec665db4dcb4b", "class_name": "RelatedNodeInfo"}}, "text": "dbt\n\nEnter dbt. dbt is one of the most popular data transformation tools today. And\nuntil now dbt has been entirely a SQL-based transformation tool. But with the\nrelease of dbt version 1.3, it's now possible to create both SQL and Python\nbased models in dbt! Here's how dbt explains it:\n\ndbt Python (\"dbt-py\") models will help you solve use cases that can't be\nsolved with SQL. You can perform analyses using tools available in the open\nsource Python ecosystem, including state-of-the-art packages for data science\nand statistics. Before, you would have needed separate infrastructure and\norchestration to run Python transformations in production. By defining your\nPython transformations in dbt, they're just models in your project, with all\nthe same capabilities around testing, documentation, and lineage. ([dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models))", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "baa07f0e-84d8-46a7-abb5-d323e0cb8ccc": {"__data__": {"id_": "baa07f0e-84d8-46a7-abb5-d323e0cb8ccc", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed1a9377-cdfc-4f41-b2b7-d2d1c1a66073", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dc1eee4a7c078471069983d385ffd23572076b59d08dc41fc0a24f13ae155b70", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake\n\nPython based dbt models are made possible by [Snowflake's new native Python\nsupport and Snowpark API for Python](https://www.snowflake.com/blog/snowpark-\npython-innovation-available-all-snowflake-customers/) (Snowpark Python for\nshort). Snowpark Python includes the following exciting capabilities:\n\n  * Python (DataFrame) API\n  * Python Scalar User Defined Functions (UDFs)\n  * Python UDF Batch API (Vectorized UDFs)\n  * Python Table Functions (UDTFs)\n  * Python Stored Procedures\n  * Integration with Anaconda\n\nWith Snowflake's Snowpark Python capabilities, you no longer need to maintain,\nsecure and pay for separate infrastructure/services to run Python code as it\ncan now be run directly within Snowflake's Enterprise grade data platform! For\nmore details check out [the Snowpark Developer Guide for\nPython](ttps://docs.snowflake.com/en/developer-\nguide/snowpark/python/index.html).\n\nThis guide will provide step-by-step instructions for how to get started with\nSnowflake Snowpark Python and dbt's new Python-based models.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6b4b736-3119-464a-bfdc-f2dfeb90012e": {"__data__": {"id_": "d6b4b736-3119-464a-bfdc-f2dfeb90012e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b0cadaf-a01a-4018-b59b-bc92ec0d4da0", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "02b12088a5a95540ee2c6fe16a55be110cb4b4ca7cefc3e1cd4bd88bc4038de0", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\nThis guide assumes that you have a basic working knowledge of dbt, Python, and\nAnaconda.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33fe586e-3838-44aa-8b6e-03619939c9d6": {"__data__": {"id_": "33fe586e-3838-44aa-8b6e-03619939c9d6", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "070616f9-0c5f-42b5-adac-e3751842b2e7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "bafd1b4ee6c57749f39b9a9c530fc7fb8ad026625e9cc2d88e5cb9dd249f3596", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Learn\n\n  * The basics of Snowpark Python\n  * How to create Python-based models in dbt\n  * How to create and use Python UDFs in your dbt Python model\n  * How the integration between Snowpark Python and dbt's Python models works", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4362f1ee-fdf4-4261-bc07-a512169ece67": {"__data__": {"id_": "4362f1ee-fdf4-4261-bc07-a512169ece67", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0228951c-1a5a-4a45-bde8-20e01de18878", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "73abba5f9354fb4315873cdfb5c8bcd67162cdcc3cc79e6775d98e9549abaa46", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Need\n\nYou will need the following things before beginning:\n\n  1. Snowflake \n    1. **A Snowflake Account.**\n    2. **A Snowflake Database named DEMO_DB.**\n    3. **A Snowflake User created with appropriate permissions.** This user will need permission to create objects in the DEMO_DB database.\n  2. Anaconda \n    1. **Anaconda installed on your computer.** Check out the Anaconda Installation instructions for the details.\n  3. dbt \n    1. **dbt installed on your computer.** Python models were first introduced in dbt version 1.3, so make sure you install version 1.3 or newer of dbt. Please follow these steps (where `` is any name you want for the Anaconda environment): \n      1. `conda create -n  python=3.8`\n      2. `conda activate `\n      3. `pip install dbt-core dbt-snowflake` (or `pip install --upgrade dbt-core dbt-snowflake` if upgrading)\n  4. Integrated Development Environment (IDE) \n    1. **Your favorite IDE installed on your computer.** If you don't already have a favorite IDE I would recommend the great, free, open-source Visual Studio Code.\n\nThe following are optional but will help you debug your Python dbt models:\n\n  1. **Python extension installed in your IDE.** For VS Code, install the Python extension from Microsoft.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7967424c-25c1-4b7c-a598-acbe39882de9": {"__data__": {"id_": "7967424c-25c1-4b7c-a598-acbe39882de9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af3d69e9-ff54-431e-8eac-f06dd4eb3e76", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6ae1938c5443ffe1bdbb3d8bdadb1d837170d5b68c9cdfd0002b3dd2d335f416", "class_name": "RelatedNodeInfo"}}, "text": "What You'll Build\n\n  * A simple dbt project with Python-based models!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 71, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4400a42d-b508-4e26-a90f-8bddc97f9446": {"__data__": {"id_": "4400a42d-b508-4e26-a90f-8bddc97f9446", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86d2c220-0a8b-4337-ac14-86b64db7f6e1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1385313cfa3a92b642f82ab6cf70ce9038ff7c61e2e7b571ecfd681695731369", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Create the dbt Project\n\nThe easiest way to get started with dbt, and ensure you have the most up-to-\ndate dbt configurations, is to the run the `dbt init` command. The `dbt init`\nprocess will create two folders in the directory you run it from, a `logs`\nfolder and a folder with the same name as your project. So in a terminal\nchange to the directory where you want the new dbt project folder created and\nexecute `dbt init`. Follow the prompts to create your new project. For most of\nthe prompts enter the value appropriate to your environment, but for\n**database** and **schema** enter the values below:\n\n  * Enter a name for your project (letters, digits, underscore): ``\n  * Which database would you like to use? ... Enter a number: `[1] snowflake`\n  * account (https://.snowflakecomputing.com): ``\n  * user (dev username): ``\n  * Desired authentication type option (enter a number): `[1] password`\n  * password (dev password): ``\n  * role (dev role): ``\n  * warehouse (warehouse name): ``\n  * database (default database that dbt will build objects in): `DEMO_DB`\n  * schema (default schema that dbt will build objects in): `DEMO_SCHEMA`\n  * threads (1 or more) [1]: `1`\n\nSee the [dbt init\ndocumentation](https://docs.getdbt.com/reference/commands/init) for more\ndetails on what just happened, but after `dbt init` finished you will have a\nfunctional dbt project and the following default SQL models in the `models`\nfolder:\n\n    \n    \n    models\n    |-- example\n    |--|-- my_first_dbt_model.sql\n    |--|-- my_second_dbt_model.sql\n    |--|-- schema.yml\n\n**Note** \\- The connection details you just entered have been stored in a dbt\nconnection profile in the default location: `~/.dbt/profiles.yml`. To learn\nmore about managing connection details with dbt profiles please see\n[configuring your profile](https://docs.getdbt.com/dbt-cli/configure-your-\nprofile).\n\nTo verify that everything is configured properly, open a terminal, cd to the\nfolder that `dbt init` created and then execute `dbt run`. dbt should execute\nsuccessfully and you should now have the following objects created in\nSnowflake in your `DEMO_DB.DEMO_SCHEMA` schema:\n\n  * A table named `my_first_dbt_model`\n  * A view named `my_second_dbt_model`\n\n**Note** \\- If the `dbt run` command did not complete successfully, it's most\nlikely something wrong with your connection details. Please review and update\nthose details in your dbt connection profile saved here:\n`~/.dbt/profiles.yml`. Then retry.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c97fbcc8-5500-4b11-8660-2e0fbf202b79": {"__data__": {"id_": "c97fbcc8-5500-4b11-8660-2e0fbf202b79", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66dafcac-9034-400d-b7d1-6cb021583260", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f87a870b0708d950028bbd7ef4c307023af6ff4d1c8a6a3d82dc370486e2fe30", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Create a Simple Python Model", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 34, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "721f456c-7bf4-4a24-85e4-6e45ce3e6574": {"__data__": {"id_": "721f456c-7bf4-4a24-85e4-6e45ce3e6574", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24d4f034-b9e4-4369-9c49-3119b310b548", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ac8d848ab3de9cf3e06741cea251fd421506fce9b2b5d3d5c8502abdac9ce643", "class_name": "RelatedNodeInfo"}}, "text": "Overview\n\nNow that you've created your dbt project and run it once successfully, it's\ntime to create our first Python model! But before we do, here's a brief\noverview of how to create a Python model in dbt (from [dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models)):\n\nEach Python model lives in a `.py` file in your `models/` folder. It defines a\nfunction named **`model()`** , which takes two parameters:\n\n  * `dbt`: A class compiled by dbt Core, unique to each model, that enables you to run your Python code in the context of your dbt project and DAG.\n  * `session`: A class representing the connection to the Python backend on your data platform. The session is needed to read in tables as DataFrames, and to write DataFrames back to tables. In PySpark, by convention, the `SparkSession` is named `spark`, and available globally. For consistency across platforms, we always pass it into the `model` function as an explicit argument named `session`.\n\nThe `model()` function must return a single DataFrame.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b5b19af8-2842-4176-91bd-3ddacab95a4f": {"__data__": {"id_": "b5b19af8-2842-4176-91bd-3ddacab95a4f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc87abf2-5f82-4618-a906-f262c7e32818", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1b282df3abc787a6e021930f0d7377bcca7a9d3ab82e3cf1a1da898229a44cf0", "class_name": "RelatedNodeInfo"}}, "text": "Create my_first_python_model\n\nIn the `models/example` folder, create a new file named\n`my_first_python_model.py` and copy & paste the following content to the new\nfile:\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n\nFinally, save the file and execute `dbt run` again. If everything ran\nsuccessfully you just ran your very first Python model in dbt! It's that\nsimple. Here are a few things to note at this point:\n\n  * No Jinja! dbt Python models don't use Jinja to render compiled code.\n  * You don't have to explicitly import the Snowflake Snowpark Python library, dbt will do that for you. More on this in the next step.\n  * As mentioned above, every dbt Python model must define a method named `model` that has the following signature: `model(dbt, session)`.\n  * As of 10/17/2022 only `table` or `incremental` materializations are supported, which is why we configured it explicitly here.\n  * You can use `dbt.ref()` and `dbt.source()` just the same as their Jinja equivalents in SQL models. And you can refer to either Python or SQL models interchangeably!\n\n**Note** \\- For more details on accessing dbt project contexts from your\nPython models, please check out [Accessing project\ncontext](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models#accessing-project-context).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5cd927f3-7c2b-4720-9431-321cdb1ae55b": {"__data__": {"id_": "5cd927f3-7c2b-4720-9431-321cdb1ae55b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e170d7e-0219-44ef-90ac-680024e4a38f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c2f03cf7e14bb2dbad234e890ab370706302418fd1dad5288b3f456e6fc16ffa", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Understand How the Simple dbt Python Model Works\n\nSo what just happened you ran your dbt Python model? The single best thing to\nhelp you debug and understand what's happening is to look at your [Query\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history) in Snowflake. Please take a minute now to review\nwhat happened in your Snowflake account, by reviewing your recent query\nhistory.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eefab3a0-fb96-4be7-83c3-dd085ce1c263": {"__data__": {"id_": "eefab3a0-fb96-4be7-83c3-dd085ce1c263", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a3b5954-007d-43a5-904d-29a6a9052e82", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2354a8da4dae7118e7cd6712491761dd059b3eef2a0af8a1558e96bc1223f3c7", "class_name": "RelatedNodeInfo"}}, "text": "Overview of dbt Executed Queries\n\nHere are the queries that dbt executed when you ran the\n`my_first_python_model` model. I've omitted the content of the stored\nprocedure in this section so that it's easier to see what's happening at a\nhigh level. In the next section we'll discuss what's happening inside the\nstored procedure.\n\n  1. List schemas\n    \n         show terse schemas in database DEMO_DB\n\n  2. List objects\n    \n         show terse objects in DEMO_DB.DEMO_SCHEMA\n\n  3. Create stored procedure\n    \n         CREATE OR REPLACE PROCEDURE DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp ()\n     RETURNS STRING\n     LANGUAGE PYTHON\n     RUNTIME_VERSION = '3.8' -- TODO should this be configurable?\n     PACKAGES = ('snowflake-snowpark-python')\n     HANDLER = 'main'\n     EXECUTE AS CALLER\n     AS\n     $$\n    \n     \n    \n     $$;\n\n  4. Call stored procedure\n    \n         CALL DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp()\n\n    1. Materialize model\n        \n                 CREATE  OR  REPLACE TABLE DEMO_DB.DEMO_SCHEMA.my_first_python_model AS  SELECT  *  FROM ( SELECT  *  FROM (DEMO_DB.DEMO_SCHEMA.my_first_dbt_model)\n\n  5. Drop stored procedure\n    \n         drop procedure if exists DEMO_DB.DEMO_SCHEMA.my_first_python_model__dbt_sp()", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "65f6b9ef-98c2-4030-a7cb-368f752b212d": {"__data__": {"id_": "65f6b9ef-98c2-4030-a7cb-368f752b212d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fcca436e-32b6-4131-9965-551529181e79", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f7e1640517e2df2261059758144f98618f723dc136f7446d7a96edca8989cfec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee4a0c98-3a55-4f71-a0c3-d5fefc536e44", "node_type": "1", "metadata": {}, "hash": "b5fa2e40df41632d876cb1bb35bdf6c50df941aef1b53bb37d8e194e70b0a302", "class_name": "RelatedNodeInfo"}}, "text": "Overview of Stored Procedure Code Generated by dbt\n\nSo what exactly is happening in the stored procedure? This is really the\ncritical part to focus on in order to understand what dbt is doing with Python\nmodels. As we go through this, keep in mind that a core design principal for\ndbt Python models is that all of the Python code will be executed in\nSnowflake, and none will be run locally. Here's how dbt's documentation puts\nit:\n\nThe prerequisites for dbt Python models include using an adapter for a data\nplatform that supports a fully featured Python runtime. In a dbt Python model,\nall Python code is executed remotely on the platform. None of it is run by dbt\nlocally. We believe in clearly separating model definition from model\nexecution. In this and many other ways, you'll find that dbt's approach to\nPython models mirrors its longstanding approach to modeling data in SQL. (from\n[dbt Python models](https://docs.getdbt.com/docs/building-a-dbt-\nproject/building-models/python-models))\n\nWith that, let's look at our first Python dbt model again:\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n\nSo in order to run that in Snowflake we need a few things. First it needs to\nrun in the context of a stored procedure, and second the stored procedure\nneeds to contain everything necessary to run the model. This means that dbt\nneeds to generate Python code to handle the following things:\n\n  1. Provide a `dbt` and `session` object to our `model()` method. \n    1. The `dbt` object needs to provide access to dbt configuration and dag context with methods like `ref()`, `source()`, `config()`, `this()`, etc.\n    2. The `session` object is the Snowpark session\n  2. Provide the logic to materialize the resulting DataFrame returned by the `model()` method\n  3. Provide the overall orchestration of these pieces\n\nHere's the full content of the stored procedure generated by dbt (which was\nremoved in the overview above and replaced with the `<dbt compiled Python\ncode>` placeholder):\n\n    \n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n     \n        return df\n    \n    \n    # This part is user provided model code\n    # you will need to copy the next section to run the code\n    # COMMAND ----------\n    # this part is dbt logic for get ref work, do not modify\n    \n    def ref(*args,dbt_load_df_function):\n        refs = {\"my_first_dbt_model\": \"DEMO_DB.DEMO_SCHEMA.my_first_dbt_model\"}\n        key = \".\".join(args)\n        return dbt_load_df_function(refs[key])\n    \n    \n    def source(*args, dbt_load_df_function):\n        sources = {}\n        key = \".", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ee4a0c98-3a55-4f71-a0c3-d5fefc536e44": {"__data__": {"id_": "ee4a0c98-3a55-4f71-a0c3-d5fefc536e44", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fcca436e-32b6-4131-9965-551529181e79", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f7e1640517e2df2261059758144f98618f723dc136f7446d7a96edca8989cfec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65f6b9ef-98c2-4030-a7cb-368f752b212d", "node_type": "1", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "041a93b237a7a8343cc92d3917378d5bd740860f0dd3b3085ba1f9bde53c4293", "class_name": "RelatedNodeInfo"}}, "text": "\".join(args)\n        return dbt_load_df_function(refs[key])\n    \n    \n    def source(*args, dbt_load_df_function):\n        sources = {}\n        key = \".\".join(args)\n        return dbt_load_df_function(sources[key])\n    \n    \n    config_dict = {}\n    \n    \n    class config:\n        def __init__(self, *args, **kwargs):\n            pass\n    \n        @staticmethod\n        def get(key):\n            return config_dict.get(key)\n    \n    class this:\n        \"\"\"dbt.this() or dbt.this.identifier\"\"\"\n        database = 'DEMO_DB'\n        schema = 'DEMO_SCHEMA'\n        identifier = 'my_first_python_model'\n        def __repr__(self):\n            return 'DEMO_DB.DEMO_SCHEMA.my_first_python_model'\n    \n    \n    class dbtObj:\n        def __init__(self, load_df_function) -> None:\n            self.source = lambda x: source(x, dbt_load_df_function=load_df_function)\n            self.ref = lambda x: ref(x, dbt_load_df_function=load_df_function)\n            self.config = config\n            self.this = this()\n            self.is_incremental = False\n    \n    # COMMAND ----------\n    \n    # To run this in snowsight, you need to select entry point to be main\n    # And you may have to modify the return type to text to get the result back\n    # def main(session):\n    #     dbt = dbtObj(session.table)\n    #     df = model(dbt, session)\n    #     return df.collect()\n    \n    # to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python\n    # then you can do the following to run model\n    # dbt = dbtObj(session.table)\n    # df = model(dbt, session)\n    \n    \n    def materialize(session, df, target_relation):\n        # we have to make sure pandas is imported\n        import pandas\n        if isinstance(df, pandas.core.frame.DataFrame):\n            # session.write_pandas does not have overwrite function\n            df = session.createDataFrame(df)\n        df.write.mode(\"overwrite\").save_as_table(\"DEMO_DB.DEMO_SCHEMA.my_first_python_model\", create_temp_table=False)\n    \n    def main(session):\n        dbt = dbtObj(session.table)\n        df = model(dbt, session)\n        materialize(session, df, dbt.this)\n        return \"OK\"\n\n**Note** \\- When building and debugging your dbt Python models, you can find\nthis Python code in the compiled version of the model by running `dbt compile`\n(or `dbt run`). The compiled files are written to the `target-path` folder,\nwhich by default is a folder named `target` in your dbt project folder.", "mimetype": "text/plain", "start_char_idx": 2856, "end_char_idx": 5376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "631cac94-e2cd-472d-886b-9b3f38e5ba32": {"__data__": {"id_": "631cac94-e2cd-472d-886b-9b3f38e5ba32", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "541021e3-62fe-4a44-af91-cbc6f109976a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "60f6ec35f95680ce2e26eb1ecd183a58f02d260cae3cdee9c77cce5e8f837520", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Create a Python Model with a UDF\n\nNow that you understand the basics of dbt Python models, let's add in another\nconcept, the user-defined function (or UDF for short). \"A UDF (user-defined\nfunction) is a user-written function that can be called from Snowflake in the\nsame way that a built-in function can be called. Snowflake supports UDFs\nwritten in multiple languages, including Python.\" (see [Introduction to Python\nUDFs](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-\nintroduction.html) for more details). By creating and registering a UDF it can\nbe used in your DataFrame (or SQL) expression just like a built-in function.\n\nSo let's create a second dbt Python model, this time with a UDF. In the\n`models/example` folder, create a new file named `my_second_python_model.py`\nand copy & paste the following content to the new file:\n\n    \n    \n    from snowflake.snowpark.functions import udf\n    \n    def model(dbt, session):\n        # Must be either table or incremental (view is not currently supported)\n        dbt.config(materialized = \"table\")\n    \n        # User defined function\n        @udf\n        def add_one(x: int) -> int:\n            x = 0 if not x else x\n            return x + 1\n    \n        # DataFrame representing an upstream model\n        df = dbt.ref(\"my_first_dbt_model\")\n    \n        # Add a new column containing the id incremented by one\n        df = df.withColumn(\"id_plus_one\", add_one(df[\"id\"]))\n    \n        return df\n\nAs you can see from the code above we're creating a very simple UDF named\n`add_one()` which adds one to the number passed to the function. Finally, save\nthe file and execute `dbt run` again (or just run this new model by executing\n`dbt run --model my_second_python_model`). If everything ran successfully you\njust ran your second Python model with a UDF in dbt!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2ab02b58-f0f0-44de-8328-9afe8253a9fa": {"__data__": {"id_": "2ab02b58-f0f0-44de-8328-9afe8253a9fa", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16ac756b-c246-4cff-b668-49cda942e4b3", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dd4b4ee526c13a426fa1b5757818637032d532bf0a5dc2316187b881a8b3b40b", "class_name": "RelatedNodeInfo"}}, "text": "Aside: Where to Define the UDF\n\nRemember in the last section how we saw that your dbt Python model code is\nsupplemented with dbt's compiled code and then wrapped in a Snowflake stored\nprocedure? Well that's important to note as it has implications for where/how\nyou define a UDF in your Python model. In the example above we defined and\nregistered the function inside the `main()` handler function, using the `@udf`\ndecorator.\n\nIf you try and define and register the function outside of the `main()`\nhandler function you will get the following error: `Query outside the handler\nis not allowed in Stored Procedure. Please move query related functions into\nthe handler in function`. In order to move the function definition outside of\nthe `main()` handler function you must only define it there and then register\nit inside. Here is an example from dbt's documentation ([dbt Python\nmodels](https://docs.getdbt.com/docs/building-a-dbt-project/building-\nmodels/python-models)):\n\n    \n    \n    import snowflake.snowpark.types as T\n    import snowflake.snowpark.functions as F\n    import numpy\n    \n    def register_udf_add_random():\n        add_random = F.udf(\n            # use 'lambda' syntax, for simple functional behavior\n            lambda x: x + numpy.random.normal(),\n            return_type=T.FloatType(),\n            input_types=[T.FloatType()]\n        )\n        return add_random\n    \n    def model(dbt, session):\n    \n        dbt.config(\n            materialized = \"table\",\n            packages = [\"numpy\"]\n        )\n    \n        temps_df = dbt.ref(\"temperatures\")\n    \n        add_random = register_udf_add_random()\n    \n        # warm things up, who knows by how much\n        df = temps_df.withColumn(\"degree_plus_random\", add_random(\"degree\"))\n        return df\n\nNotice that the function is defined outside the `main()` handler function and\nthen registered (and used) inside.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2775871e-a372-4724-af90-b61a4d694032": {"__data__": {"id_": "2775871e-a372-4724-af90-b61a4d694032", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "319c2a65-fcff-4bdb-bc01-11ae4ad79a6e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "54e3e13a5a5d4530230d86153e45bf1eed390661ab12a95bda12f3b0940f643a", "class_name": "RelatedNodeInfo"}}, "text": "6\\. Understand How the dbt Python Model with a UDF Works\n\nNow let's take another deep dive into what happened when we run the second dbt\nmodel with a UDF. As before, please take a minute now to review what happened\nin your Snowflake account, by reviewing your recent query history ([Query\nHistory](https://docs.snowflake.com/en/user-guide/ui-snowsight-\nactivity.html#query-history)).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6827eb2b-db43-4da4-ad8d-d08429642a91": {"__data__": {"id_": "6827eb2b-db43-4da4-ad8d-d08429642a91", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba434c09-b641-4a78-b697-b9e3ea4e85f8", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "ea901055725528abae1717d9318ecbfbeb11250cc187a67ccaa4f17b2fc525ad", "class_name": "RelatedNodeInfo"}}, "text": "Overivew of dbt Executed Queries\n\nHere are the queries that dbt executed when you ran the\n`my_second_python_model` model. You will notice that it's similar to the first\nmodel, but with some more queries to deal with creating the UDF. I've again\nomitted the content of the stored procedure in this section so that it's\neasier to see what's happening at a high level. But I won't walk through it\nthis time as it's very similar to the previous example, the only real\ndifference being the difference in model code.\n\n  1. List schemas\n    \n         show terse schemas in database DEMO_DB\n\n  2. List objects\n    \n         show terse objects in DEMO_DB.DEMO_SCHEMA\n\n  3. Create stored procedure\n    \n         CREATE OR REPLACE PROCEDURE DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp ()\n     RETURNS STRING\n     LANGUAGE PYTHON\n     RUNTIME_VERSION = '3.8' -- TODO should this be configurable?\n     PACKAGES = ('snowflake-snowpark-python')\n     HANDLER = 'main'\n     EXECUTE AS CALLER\n     AS\n     $$\n    \n     \n    \n     $$;\n\n  4. Call stored procedure\n    \n         CALL DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp()\n\n    1. Use the correct database\n        \n                 SELECT CURRENT_DATABASE()\n\n    2. Create a temporary stage\n        \n                 create temporary stage if not exists DEMO_DB.DEMO_SCHEMA.SNOWPARK_TEMP_STAGE_4LBT18DLR8\n\n    3. List the contents of the stage\n        \n                 ls '@\"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_STAGE_4LBT18DLR8'\n\n    4. Get the list of files in the stage\n        \n                 SELECT \"name\" FROM ( SELECT  *  FROM  TABLE ( RESULT_SCAN('')))\n\n    5. Create the temporary function\n        \n                 CREATE TEMPORARY FUNCTION \"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_FUNCTION_1Z5V5PPNVH(arg1 BIGINT)\n         RETURNS BIGINT\n         LANGUAGE PYTHON\n         RUNTIME_VERSION=3.8\n         PACKAGES=('cloudpickle==2.0.0')\n         HANDLER='compute'\n        \n         AS $$\n        \n         \n        \n         $$;\n\n    6. Materialize the table\n        \n                 CREATE  OR  REPLACE TABLE  DEMO_DB.DEMO_SCHEMA.my_second_python_model AS  SELECT  *  FROM ( SELECT \"ID\", \"DEMO_DB\".\"DEMO_SCHEMA\".SNOWPARK_TEMP_FUNCTION_1Z5V5PPNVH(\"ID\") AS \"ID_PLUS_ONE\" FROM ( SELECT  *  FROM (DEMO_DB.DEMO_SCHEMA.my_first_dbt_model)))\n\n  5. Drop stored procedure\n    \n         drop procedure if exists DEMO_DB.DEMO_SCHEMA.my_second_python_model__dbt_sp()\n\nThe overall steps are the same, but notice that what happens during the stored\nprocedure execution (step #4) is different and more complex. This is because\nthe Snowpark library, running inside the Snowflake Python stored procedure has\nmore work to do with UDFs. It has to pickle the UDF content, create a\ntemporary stage, and finally create the UDF in Snowflake.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4ab0e8f-c2f6-4718-aea7-b62eabab107d": {"__data__": {"id_": "c4ab0e8f-c2f6-4718-aea7-b62eabab107d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59d99ede-5036-4de8-9cbd-9c179de7dd1d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "17b9c01a6fac053e2719ed6c4a18fb9f289bf576acf45fffd9d0314d7a35876e", "class_name": "RelatedNodeInfo"}}, "text": "7\\. Conclusion & Next Steps", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 29, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "14ad0550-d114-4ef8-b9c5-6a684abb6126": {"__data__": {"id_": "14ad0550-d114-4ef8-b9c5-6a684abb6126", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2dfbf854-899d-4813-9a03-9ac74644979f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d8791a055c9f63aad8928dd27afce07641f002787404ac632b3f0dff3a90460b", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\n\nHopefully you've seen how powerful the combination of dbt Python models and\nthe Snowflake platform can be! By supporting both SQL and Python based\ntransformations in dbt, data engineers can take advantage of both while\nbuilding robust data pipelines. While most of the time the choice between SQL\nor DataFrames is simply a matter of preference, as we discussed in the\nintroduction there are use cases where data transformations can't be expressed\nin SQL. In these cases data engineers can make use of tools available in the\nopen source Python ecosystem, including state-of-the-art packages for data\nscience and statistics.\n\nAnd best of all, the Snowflake platform enables this with native Python\nsupport and rich Snowpark API for Python. This eliminates the need for data\nengineers to maintain and pay for separate infrastructure/services to run\nPython code. Snowflake manages all of it for you with the ease of use that you\nwould expect from Snowflake!\n\nData engineering with Snowflake and dbt just got easier!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d81971b1-5ece-45cf-b76d-9dd7155fd164": {"__data__": {"id_": "d81971b1-5ece-45cf-b76d-9dd7155fd164", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56ddda2a-f3a5-4595-a525-a6234ecdd8c2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "300df847f63a2d96ca1e33d91cef273061c0d2ee40b3d4c9e5ad6367f721b686", "class_name": "RelatedNodeInfo"}}, "text": "Some Tips for Debugging dbt Python Models\n\nThere are a few ways to debug your dbt Python models.\n\nThe first tip is to build and debug via dbt. This is probably the default for\nmost people. With this approach you edit the dbt Python model code directly in\nthe dbt model file and execute it via the `dbt run` command (and it's often\nbest to use the `dbt run --model ` syntax to only run the model in\nconsideration). But this can be a bit time consuming and require you to flip\nback and forth between your dbt and Snowflake.\n\nThe second tip is to build and debug the model code directly in Python. Like I\nmentioned in Section 1, I personally like Visual Studio Code and the [the\nPython extension from\nMicrosoft](https://marketplace.visualstudio.com/items?itemName=ms-\npython.python), but you can use whatever Python IDE you prefer. Regardless,\nthe idea here is to copy/paste the contents of the dbt generated Python code\ninto a temporary Python script as a starting point then edit/debug until it's\nright and finally paste just the contents of the model back into your dbt\nmodel file. As we saw in section 4 of this Quickstart you can get the dbt\ngenerated Python code by compiling the model and looking at the compiled\nscript. There is even commented out code in the dbt generated Python code to\nhelp get you started.\n\nHope that helps!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eff7c45b-8876-4552-9c0a-4f48e5331e39": {"__data__": {"id_": "eff7c45b-8876-4552-9c0a-4f48e5331e39", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ab73d7-371e-4eb4-86d0-dac99b341355", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9d785d94a4e86bb3df943bc96abd38905c565fef60eeb8dc50366a86616f87f2", "class_name": "RelatedNodeInfo"}}, "text": "What We've Covered\n\n  * The basics of Snowpark Python\n  * How to create Python-based models in dbt\n  * How to create and use Python UDFs in your dbt Python model\n  * How the integration between Snowpark Python and dbt's Python models works", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2444b2a1-fa23-4023-ad27-72bbd98c297e": {"__data__": {"id_": "2444b2a1-fa23-4023-ad27-72bbd98c297e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "826696e2-85bc-4f46-a80e-0281e76111bb", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "eae7aa8ce16630441991a1ff96cbe6fd64cf6d6bb0b70f55bb4d9d4c03934bd4", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Snowpark Developer Guide for Python\n  * Snowpark API Reference (Python)\n  * dbt Python models\n\nBack\n\nNextDone\n\n!\n!\n\n![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=191d57cb-9fca-48cc-a747-ef72f27f46a8&sid=95062c40865a11ef9df0974bc0b630cc&vid=95065fc0865a11ef8b5bd9527109a496&vids=1&msclkid=N&pi=0&lg=en-\nUS&sw=1800&sh=1169&sc=30&nwd=1&tl=Data%20Engineering%20with%20Snowpark%20Python%20and%20dbt&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fdata_engineering_with_snowpark_python_and_dbt%2F%230&r=&lt=599&evt=pageLoad&sv=1&cdb=AQAQ&rn=921167)!\n\n!\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7fa9b4a3-008e-49a5-849b-55cb1b58aa4e": {"__data__": {"id_": "7fa9b4a3-008e-49a5-849b-55cb1b58aa4e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95f884ae-7871-4dd0-8826-34b4a1b8892f", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9f9646995ce4eca7ffea86b43f1eb0164c318b54941c1afdc7733fa9d45186c2", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Use of Cookies\n\nWe use cookies to enhance your experience and to analyze site traffic as\ndescribed in our Cookie Statement. By accepting, you consent to our use of\ncookies.[Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-\nstatement/)\n\nCookies Settings Reject All Accept All Cookies\n\n![Company\nLogo](https://cdn.cookielaw.org/logos/cb85e692-4053-4d0a-8dda-d24b5daa8b06/ff6c124b-1473-4861-9ca3-9eaf6debb37d/SNO-\nSnowflakeLogo_blue.png)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f9fe088-82a8-4953-b649-3d801781ac02": {"__data__": {"id_": "3f9fe088-82a8-4953-b649-3d801781ac02", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84ea060b-8647-46f1-b040-ff3ba0bac1e2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b48043687f61b9e31c1a2a3b4709d291253a3cf4b693803bd9c61e1a6320d9fa", "class_name": "RelatedNodeInfo"}}, "text": "Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "393670cc-d306-4499-84f0-ca41247fb5de": {"__data__": {"id_": "393670cc-d306-4499-84f0-ca41247fb5de", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c092a1d-422d-4aaa-b04d-d595eebe7a2e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6398340aee318d0882b13f59a01d4de8c65f3051a14137b0e48404121e310241", "class_name": "RelatedNodeInfo"}}, "text": "Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.  \nMore information", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "348009a3-6d66-4603-813f-31143d202cc9": {"__data__": {"id_": "348009a3-6d66-4603-813f-31143d202cc9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "608aafea-5510-4f18-b150-b146f3027a75", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "15ed8b57925f72e3041b9e5c874ca7d12e34ed4996cf10939f9dd7275088e05f", "class_name": "RelatedNodeInfo"}}, "text": "Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff. They are usually only set in response to actions made by you which amount\nto a request for services, such as setting your privacy preferences, logging\nin or filling in forms. You can set your browser to block or alert you about\nthese cookies, but some parts of the site will not then work. These cookies do\nnot store any personally identifiable information.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c497fa09-6589-4b07-b9f2-27798f0857d9": {"__data__": {"id_": "c497fa09-6589-4b07-b9f2-27798f0857d9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b49ef040-ef86-4025-834e-5632d2352d13", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "dddb8a566927221dbf892de9ea7a2f47e7a462655fb98be1f1972c28f32dbc19", "class_name": "RelatedNodeInfo"}}, "text": "Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.    All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "79e31419-4df2-48d1-8527-70a0e2caee0a": {"__data__": {"id_": "79e31419-4df2-48d1-8527-70a0e2caee0a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "258ec2ca-b291-4034-8c40-8bc47c56f7d1", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "acf2334abae025b608e0a3bb4fea38586325388be2f9e7e090b5ebf06ae8c5b1", "class_name": "RelatedNodeInfo"}}, "text": "Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages.    If you do not allow these cookies then\nsome or all of these services may not function properly.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a728d1c7-b7d2-40cd-bbe7-6814eaa3c218": {"__data__": {"id_": "a728d1c7-b7d2-40cd-bbe7-6814eaa3c218", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffc11a9c-7ce3-4d0b-b52c-d14b46f8c798", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "f89e6056ad5749a2f32e62b9836f5afa8aad6f6971aa1e6be66811fc8f82d11c", "class_name": "RelatedNodeInfo"}}, "text": "Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly identifiable\npersonal information, but are based on uniquely identifying your browser and\ninternet device. If you do not allow these cookies, you will experience less\ntargeted advertising.\n\nCookies Details\u200e\n\nBack Button", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b235cb7f-a146-46c7-bc27-1cb248e26112": {"__data__": {"id_": "b235cb7f-a146-46c7-bc27-1cb248e26112", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5fd03f5-cc00-47d8-ba88-1c4574a3f31c", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "eb2560679a1155ec8b4f8ab2aa988cf73acb5440f1a0f7772a1d2d2672d4e6ac", "class_name": "RelatedNodeInfo"}}, "text": "Cookie List\n\nFilter Button\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[![Powered by\nOnetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-\nconsent/)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25acd883-9570-4d9e-8863-6a6007877b53": {"__data__": {"id_": "25acd883-9570-4d9e-8863-6a6007877b53", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e19d7283-f1f9-4a4b-8aa2-3edd8c2b6448", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a1ef686ad98beb9e88522251ab0e99618be0c4b90649968d0b3cfb4838133de5", "class_name": "RelatedNodeInfo"}}, "text": "1. Overview\n  2. Setup Environment\n  3. Data Engineering\n  4. Data Pipelines\n  5. Machine Learning\n  6. Streamlit Application\n  7. Cleanup\n  8. Conclusion And Resources\n\n _close_ _menu_", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6fc1c36-c758-469e-8b64-9b93b55412ba": {"__data__": {"id_": "c6fc1c36-c758-469e-8b64-9b93b55412ba", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b55bbe6f-bb9d-49a8-a54d-bc137e07bfca", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5fbcb1c448b993047b1af2bf244dd55cd1054218ad32cfa909f8e8b8d882113d", "class_name": "RelatedNodeInfo"}}, "text": "Getting Started with Data Engineering and ML using Snowpark for Python and\nSnowflake Notebooks\n\n _access_time_ 48 mins remaining", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f3a7eacc-d017-4ab0-a460-2d24f50a3e95": {"__data__": {"id_": "f3a7eacc-d017-4ab0-a460-2d24f50a3e95", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2d9ce3b-0ce5-4437-b8f4-e7b2a73f6326", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "86407d933bcf049528ec0d56a470e95e285618511e916eaf16b8882144ed7bd5", "class_name": "RelatedNodeInfo"}}, "text": "1\\. Overview\n\nBy completing this guide, you will be able to go from raw data to an\ninteractive application that can help organization optimize their advertising\nbudget allocation.\n\nHere is a summary of what you will be able to learn in each step by following\nthis quickstart:\n\n  * **Setup Environment** : Use stages and tables to ingest and organize raw data from S3 into Snowflake\n  * **Data Engineering** : Leverage Snowpark for Python DataFrames in Snowflake Notebook to perform data transformations such as group by, aggregate, pivot, and join to prep the data for downstream applications\n  * **Data Pipelines** : Use Snowflake Tasks to turn your data pipeline code into operational pipelines with integrated monitoring\n  * **Machine Learning** : Process data and run training job in Snowflake Notebook using the Snowpark ML library, and register ML model and use it for inference from Snowflake Model Registry\n  * **Streamlit** : Build an interactive Streamlit application using Python (no web development experience required) to help visualize the ROI of different advertising spend budgets", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "14fbc204-f12a-42c8-ae50-38ee75535741": {"__data__": {"id_": "14fbc204-f12a-42c8-ae50-38ee75535741", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a57f718-94cc-4f09-9700-3543e95c4e50", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "467ed2693ce4845dfa98bfe2223b547496d4cacdc8a8dcbd33fc30e853f48a60", "class_name": "RelatedNodeInfo"}}, "text": "What is Snowpark?\n\nSnowpark is the set of libraries and code execution environments that run\nPython and other programming languages next to your data in Snowflake.\nSnowpark can be used to build data pipelines, ML models, apps, and other data\nprocessing tasks.\n\n!Snowpark\n\n**Client Side Libraries** \\- Snowpark libraries can be installed and\ndownloaded from any client-side notebook or IDE and are used for code\ndevelopment and deployment. Libraries include the Snowpark API for data\npipelines and apps and the Snowpark ML API for end to end machine learning.\n\n**Elastic Compute Runtimes** \\- Snowpark provides elastic compute runtimes for\nsecure execution of your code in Snowflake. Runtime options include: Python,\nJava, and Scala in warehouses, container runtimes for out-of-the-box\ndistributed processing with CPUs or GPUs using any Python framework, or custom\nruntimes brought in from Snowpark Container Services to execute any language\nof choice with CPU or GPU compute.\n\nLearn more about Snowpark.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d044ef7b-4eb4-45f2-bdb4-cf9801ee3b5f": {"__data__": {"id_": "d044ef7b-4eb4-45f2-bdb4-cf9801ee3b5f", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc6917de-9329-4048-9f16-c251e2d1863d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cd8872aa2c84b1b41b96fa0897f4de2e8c181c4863a04a6fcd17cc778dd8d691", "class_name": "RelatedNodeInfo"}}, "text": "What is Snowflake ML?\n\nSnowflake ML is the integrated set of capabilities for end-to-end machine\nlearning in a single platform on top of your governed data. Snowflake ML can\nbe used for fully custom and out-of-the-box workflows. For ready-to-use ML,\nanalysts can use ML Functions to shorten development time or democratize ML\nacross your organization with SQL from Studio, our no-code user interface. For\ncustom ML, data scientists and ML engineers can easily and securely develop\nand productionize scalable features and models without any data movement,\nsilos or governance tradeoffs.\n\nTo get started with Snowflake ML, developers can use the Python APIs from the\n[Snowpark ML library](https://docs.snowflake.com/en/developer-guide/snowpark-\nml/index), directly from Snowflake Notebooks (public preview) or downloaded\nand installed into any IDE of choice, including Jupyter or Hex.\n\n!Snowpark\n\nThis quickstart will focus on\n\n  * Snowpark ML Modeling API, which enables the use of popular Python ML frameworks, such as scikit-learn and XGBoost, for feature engineering and model training without the need to move data out of Snowflake.\n  * Snowflake Model Registry, which provides scalable and secure model management of ML models in Snowflake, regardless of origin. Using these features, you can build and operationalize a complete ML workflow, taking advantage of Snowflake's scale and security features.\n\n**Feature Engineering and Preprocessing** \\- Improve performance and\nscalability with distributed execution for common scikit-learn preprocessing\nfunctions.\n\n**Model Training** \\- Accelerate model training for scikit-learn, XGBoost and\nLightGBM models without the need to manually create stored procedures or user-\ndefined functions (UDFs), and leverage distributed hyperparameter\noptimization.\n\n!Snowpark\n\n**Model Management and Batch Inference** \\- Manage several types of ML models\ncreated both within and outside Snowflake and execute batch inference.\n\n!Snowpark", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57060b29-5d65-4dc2-af2b-82ef79c2238c": {"__data__": {"id_": "57060b29-5d65-4dc2-af2b-82ef79c2238c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25ce4b9f-9800-435a-97aa-6f0425f75594", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d18c92d03103be64ede3ca8cb2e9a5165c3943465fd77b3dcabd8e0bc7e5625b", "class_name": "RelatedNodeInfo"}}, "text": "What is Streamlit?\n\nStreamlit enables data scientists and Python developers to combine Streamlit's\ncomponent-rich, open-source Python library with the scale, performance, and\nsecurity of the Snowflake platform.\n\nLearn more about [Streamlit](https://www.snowflake.com/en/data-\ncloud/overview/streamlit-in-snowflake/).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "411371eb-a753-463c-86ea-933296b29622": {"__data__": {"id_": "411371eb-a753-463c-86ea-933296b29622", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0a67858-64ea-4182-a21f-07453bc5a663", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "b93baba5f136c880bef0578cf40600e73b81af2a73fa4931b850bd757889b5e0", "class_name": "RelatedNodeInfo"}}, "text": "What You Will Learn\n\n  * How to analyze data and perform data engineering tasks using Snowpark DataFrames and APIs\n  * How to use open-source Python libraries from curated Snowflake Anaconda channel\n  * How to create Snowflake Tasks to automate data pipelines\n  * How to train ML model using Snowpark ML in Snowflake\n  * How to register ML model and use it for inference from Snowpark ML Model Registry\n  * How to create Streamlit application that uses the ML Model for inference based on user input", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "62de375d-817f-445d-af47-285008aafb00": {"__data__": {"id_": "62de375d-817f-445d-af47-285008aafb00", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5582b301-a846-491a-b4f9-88eb44b26f81", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e7b96246c25229aff1db2d4d419e97706abb6f1bcffa3253c9242128071772cf", "class_name": "RelatedNodeInfo"}}, "text": "What You Will Build\n\n  * A data engineering pipeline\n  * A machine learning model\n  * A Streamlit application", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33f630c9-c9d7-4305-911a-0960e42bb20b": {"__data__": {"id_": "33f630c9-c9d7-4305-911a-0960e42bb20b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec540a68-1ac1-4c09-a70d-d55fcaaf2d6a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "df944e2163827d837ab0c6df7b0a46a742b30489423d2a54b2a5d290fefc3a03", "class_name": "RelatedNodeInfo"}}, "text": "Prerequisites\n\n  * A Snowflake account with Anaconda Packages enabled by ORGADMIN. If you do not have a Snowflake account, you can register for a free trial account.\n  * A Snowflake account login with ACCOUNTADMIN role. If you have this role in your environment, you may choose to use it. If not, you will need to 1) Register for a free trial, 2) Use a different role that has the ability to create database, schema, tables, stages, tasks, user-defined functions, and stored procedures OR 3) Use an existing database and schema in which you are able to create the mentioned objects.\n\nIMPORTANT: Before proceeding, make sure you have a Snowflake account with\nAnaconda packages enabled by ORGADMIN as described\n[here](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-\npackages#getting-started).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17b89515-8c0e-4643-913e-49f9149af623": {"__data__": {"id_": "17b89515-8c0e-4643-913e-49f9149af623", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab18cadd-350e-48ee-afdb-a0751b03d4ed", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "43c5933553bc6ac8e2c4bf5c925c2a9ceb4e4695303e787c6ccb9765f5879079", "class_name": "RelatedNodeInfo"}}, "text": "2\\. Setup Environment", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 23, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "386e8c27-bcb2-4af4-b520-268b22193dca": {"__data__": {"id_": "386e8c27-bcb2-4af4-b520-268b22193dca", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2c71e46-5c60-4cf4-974e-891020707f46", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "73f8ee5f151feec99aef070fcdc97417e9a822236010b60f6feba41563e2f958", "class_name": "RelatedNodeInfo"}}, "text": "Create Tables and Load Data\n\nLog into [Snowsight](https://docs.snowflake.com/en/user-guide/ui-\nsnowsight.html#) using your credentials to create tables and load data from\nAmazon S3.\n\nIMPORTANT:\n\n  * If you use different names for objects created in this section, be sure to update scripts and code in the following sections accordingly.\n  * For each SQL script block below, select all the statements in the block and execute them top to bottom.\n\nIn a new SQL worksheet, run the following SQL commands to create the\n[warehouse](https://docs.snowflake.com/en/sql-reference/sql/create-\nwarehouse.html), [database](https://docs.snowflake.com/en/sql-\nreference/sql/create-database.html) and\nschema.\n\n    \n    \n    USE ROLE ACCOUNTADMIN;\n    \n    CREATE WAREHOUSE IF NOT EXISTS DASH_S WAREHOUSE_SIZE=SMALL;\n    CREATE DATABASE IF NOT EXISTS DASH_DB;\n    CREATE SCHEMA IF NOT EXISTS DASH_SCHEMA;\n    \n    USE DASH_DB.DASH_SCHEMA;\n    USE WAREHOUSE DASH_S;\n\nIn the same SQL worksheet, run the following SQL commands to create table\n**CAMPAIGN_SPEND** from data hosted on publicly accessible S3 bucket.\n\n    \n    \n    CREATE or REPLACE file format csvformat\n      skip_header = 1\n      type = 'CSV';\n    \n    CREATE or REPLACE stage campaign_data_stage\n      file_format = csvformat\n      url = 's3://sfquickstarts/ad-spend-roi-snowpark-python-scikit-learn-streamlit/campaign_spend/';\n    \n    CREATE or REPLACE TABLE CAMPAIGN_SPEND (\n      CAMPAIGN VARCHAR(60), \n      CHANNEL VARCHAR(60),\n      DATE DATE,\n      TOTAL_CLICKS NUMBER(38,0),\n      TOTAL_COST NUMBER(38,0),\n      ADS_SERVED NUMBER(38,0)\n    );\n    \n    COPY into CAMPAIGN_SPEND\n      from @campaign_data_stage;\n\nIn the same SQL worksheet, run the following SQL commands to create table\n**MONTHLY_REVENUE** from data hosted on publicly accessible S3 bucket.\n\n    \n    \n    CREATE or REPLACE stage monthly_revenue_data_stage\n      file_format = csvformat\n      url = 's3://sfquickstarts/ad-spend-roi-snowpark-python-scikit-learn-streamlit/monthly_revenue/';\n    \n    CREATE or REPLACE TABLE MONTHLY_REVENUE (\n      YEAR NUMBER(38,0),\n      MONTH NUMBER(38,0),\n      REVENUE FLOAT\n    );\n    \n    COPY into MONTHLY_REVENUE\n      from @monthly_revenue_data_stage;\n\nIn the same SQL worksheet, run the following SQL commands to create table\n**BUDGET_ALLOCATIONS_AND_ROI** that holds the last six months of budget\nallocations and ROI.\n\n    \n    \n    CREATE or REPLACE TABLE BUDGET_ALLOCATIONS_AND_ROI (\n      MONTH varchar(30),\n      SEARCHENGINE integer,\n      SOCIALMEDIA integer,\n      VIDEO integer,\n      EMAIL integer,\n      ROI float\n    )\n    COMMENT = '{\"origin\":\"sf_sit-is\", \"name\":\"aiml_notebooks_ad_spend_roi\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"is_quickstart\":1, \"source\":\"streamlit\"}}';\n    \n    INSERT INTO BUDGET_ALLOCATIONS_AND_ROI (MONTH, SEARCHENGINE, SOCIALMEDIA, VIDEO, EMAIL, ROI)\n    VALUES\n    ('January',35,50,35,85,8.22),\n    ('February',75,50,35,85,13.90),\n    ('March',15,50,35,15,7.34),\n    ('April',25,80,40,90,13.23),\n    ('May',95,95,10,95,6.246),\n    ('June',35,50,35,85,8.22);\n\nOptionally, you can also open [setup.sql](https://github.com/Snowflake-\nLabs/sfguide-getting-started-dataengineering-ml-snowpark-\npython/blob/main/setup.sql) in Snowsight and run all SQL statements to create\nthe objects and load data from AWS S3.\n\nIMPORTANT: If you use different names for objects created in this section, be\nsure to update scripts and code in the following sections accordingly.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e47f2d16-2f8c-4bb7-8b86-1c23da1e9d00": {"__data__": {"id_": "e47f2d16-2f8c-4bb7-8b86-1c23da1e9d00", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a583d714-c337-4b39-9e86-9e626c953529", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d922cff99c35303ecf1ceff02525e84ef537304ed73e1de0b37637cf31aa7ded", "class_name": "RelatedNodeInfo"}}, "text": "3\\. Data Engineering\n\nThe Notebook linked below covers the following data engineering tasks.\n\n  1. Load data from Snowflake tables into Snowpark DataFrames\n  2. Perform Exploratory Data Analysis on Snowpark DataFrames\n  3. Pivot and Join data from multiple tables using Snowpark DataFrames\n  4. Automate data pipelines using Snowflake Tasks", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48d53300-7a44-4c95-9df9-5fcd8e5a5360": {"__data__": {"id_": "48d53300-7a44-4c95-9df9-5fcd8e5a5360", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2b6cd77-b343-42a4-84bf-b4771a56e23e", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "6007b3f3f8598e651cb263e09f3e39708172aef0e1ba6ed1ff7f0c8d425f7f68", "class_name": "RelatedNodeInfo"}}, "text": "Data Engineering Notebook\n\nTo get started, follow these steps:\n\n  1. Click on Snowpark_For_Python_DE.ipynb to download the Notebook from GitHub. **_(NOTE: Do NOT right-click to download.)_**\n  2. In your Snowflake account:\n\n  * On the left hand navigation menu, click on **Projects** \u00bb **Notebooks**\n  * On the top right, click on **Notebook** down arrow and select **Import .ipynb file** from the dropdown menu\n  * Select the file you downloaded in step 1 above\n\n  3. In the Create Notebook popup\n\n  * For **Notebook location** , select DASH_DB and DASH_SCHEMA\n  * For **SQL warehouse** , select DASH_S\n  * Click on **Create** button\n\nIf all goes well, you should see the following Notebook:\n\n!Snowflake DE NB\n\n  4. On the top right, click on **Packages** and make sure you install `snowflake` package by typing it in the search box and clicking on the first one.\n\n!Snowflake DE NB\n\n  5. On the top right, click on **Start**. **_(NOTE: The first time it will take a couple of mins to install the packages.)_**\n  6. Once the packages are installed and the state changes from **Start** \u00bb **Starting** \u00bb **Active** , you can either click on **Run all** to execute all cells, or you can run individual cells in the order from top to bottom by clicking on the play icon on the top right corner of each cell.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "23e3a298-6ea1-465d-b644-22c2745c7f45": {"__data__": {"id_": "23e3a298-6ea1-465d-b644-22c2745c7f45", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d71cc488-5d71-4826-a19e-fcde02a901fc", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e1ad32ffac32aed7b29f5369a2388a0ff87f9f470a2668be7e00b8eaef00bd9d", "class_name": "RelatedNodeInfo"}}, "text": "4\\. Data Pipelines\n\nYou can also operationalize the data transformations in the form of automated\ndata pipelines running in Snowflake.\n\nIn particular, in the [Data Engineering\nNotebook](https://github.com/Snowflake-Labs/sfguide-getting-started-\ndataengineering-ml-snowpark-python/blob/main/Snowpark_For_Python_DE.ipynb),\nthere's a section that demonstrates how to optionally build and run the data\ntransformations as [Snowflake Tasks](https://docs.snowflake.com/en/user-\nguide/tasks-intro).\n\nFor reference purposes, here are the code snippets.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae29f452-2855-466e-8fef-e4cc83d347e4": {"__data__": {"id_": "ae29f452-2855-466e-8fef-e4cc83d347e4", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cd4c9bb-1162-47a3-8c0a-5ef8c67050f2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "574d70b55d97566500cadd7494b438f8b4ea79f24bfc752aaf90c38c30f2e183", "class_name": "RelatedNodeInfo"}}, "text": "**Campaign Spend**\n\nThis task automates loading campain spend data and performing various\ntransformations.\n\n    \n    \n    def campaign_spend_data_pipeline(session: Session) -> str:\n      # DATA TRANSFORMATIONS\n      # Perform the following actions to transform the data\n    \n      # Load the campaign spend data\n      snow_df_spend_t = session.table('campaign_spend')\n    \n      # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n      snow_df_spend_per_channel_t = snow_df_spend_t.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n          with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n    \n      # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n      snow_df_spend_per_month_t = snow_df_spend_per_channel_t.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n      snow_df_spend_per_month_t = snow_df_spend_per_month_t.select(\n          col(\"YEAR\"),\n          col(\"MONTH\"),\n          col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n          col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n          col(\"'video'\").as_(\"VIDEO\"),\n          col(\"'email'\").as_(\"EMAIL\")\n      )\n    \n      # Save transformed data\n      snow_df_spend_per_month_t.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')\n    \n    # Register data pipeline function as a task\n    root = Root(session)\n    my_task = Task(name='campaign_spend_data_pipeline_task'\n                   , definition=StoredProcedureCall(\n                       campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n                   )\n                   , warehouse='DASH_S'\n                   , schedule=timedelta(minutes=3))\n    \n    tasks = root.databases[session.get_current_database()].schemas[session.get_current_schema()].tasks\n    task_res = tasks.create(my_task,mode=CreateMode.or_replace)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5eaf5487-0725-445c-b6fd-aa36b3783233": {"__data__": {"id_": "5eaf5487-0725-445c-b6fd-aa36b3783233", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "939cc4d6-0010-4590-be52-5307ef2fccd6", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2e945b39008eb170e7662febd96c6c6535b35e06004afafb4316746d7f4d9640", "class_name": "RelatedNodeInfo"}}, "text": "**Monthly Revenue**\n\nThis task automates loading monthly revenue data, performing various\ntransformations, and joining it with transformed campaign spend data.\n\n    \n    \n    def monthly_revenue_data_pipeline(session: Session) -> str:\n      # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n      snow_df_spend_per_month_t = session.table('spend_per_month')\n      snow_df_revenue_t = session.table('monthly_revenue')\n      snow_df_revenue_per_month_t = snow_df_revenue_t.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n    \n      # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n      snow_df_spend_and_revenue_per_month_t = snow_df_spend_per_month_t.join(snow_df_revenue_per_month_t, [\"YEAR\",\"MONTH\"])\n    \n      # SAVE in a new table for the next task\n      snow_df_spend_and_revenue_per_month_t.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "59bc6271-d860-4282-a06d-cc9db401b0a9": {"__data__": {"id_": "59bc6271-d860-4282-a06d-cc9db401b0a9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68f64ab3-96f3-44c9-b985-f615b75345a7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "4a3d0d2ff9fdea0c51a624a4c838ab922ac5e31880795cc7e235ce43f917f7cb", "class_name": "RelatedNodeInfo"}}, "text": "**Tasks DAG**\n\n    \n    \n    # Delete the previous task\n    task_res.delete()\n    \n    with DAG(\"de_pipeline_dag\", schedule=timedelta(minutes=3)) as dag:\n        # Create a task that runs our first pipleine\n        dag_spend_task = DAGTask(name='campaign_spend_data_pipeline_task'\n                            , definition=StoredProcedureCall(\n                                        campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n                                    )\n                            ,warehouse='DASH_S'\n                            )\n        # Create a task that runs our second pipleine\n        dag_revenue_task = DAGTask(name='monthly_revenue_data_pipeline'\n                              , definition=StoredProcedureCall(\n                                    monthly_revenue_data_pipeline, stage_location='@dash_sprocs'\n                                )\n                            ,warehouse='DASH_S'\n                            )\n    \n    # Shift right and left operators can specify task relationships\n    dag_spend_task >> dag_revenue_task  # dag_spend_task is a predecessor of dag_revenue_task\n    \n    schema = root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n    dag_op = DAGOperation(schema)\n    \n    dag_op.deploy(dag)\n    \n    # A DAG is not suspended by default so we will suspend the root task that will suspend the full DAG\n    root_task = tasks[\"DE_PIPELINE_DAG\"]\n    root_task.suspend()", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8646851-2a6b-460d-9eca-c81dfceb4009": {"__data__": {"id_": "b8646851-2a6b-460d-9eca-c81dfceb4009", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "977d2ba1-26ba-4e1f-9ad6-b3e49f26e190", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "e8c0d1455367276247202b43ac9923aa3863a0ce982d86827b0ce6b14446106a", "class_name": "RelatedNodeInfo"}}, "text": "Run DAG\n\nWe can manually run DAGs even if they're suspended.\n\n    \n    \n    # dag_op.run(dag)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3db3c190-8725-4159-b5aa-670947300255": {"__data__": {"id_": "3db3c190-8725-4159-b5aa-670947300255", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5adb3c3-d665-44f5-aeeb-ae061cdab3c2", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "315a0ac5cdfa9b838a8ee4ba859800d552da1c79dc5abbd9ca2aa2e01ba42ded", "class_name": "RelatedNodeInfo"}}, "text": "Resume Task\n\nHere's how you can resume Tasks.\n\n    \n    \n    # root_task = tasks[\"DE_PIPELINE_DAG\"]\n    # root_task.resume()", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cde97679-392c-49ce-9864-c495cfe102ea": {"__data__": {"id_": "cde97679-392c-49ce-9864-c495cfe102ea", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68c8eea7-0e8e-4ccd-9c97-bb909a682b72", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "1bc6d3e6075b6835aef83887377dc1df6e7974ee76ec107da5f80a05a3afec64", "class_name": "RelatedNodeInfo"}}, "text": "Suspend Task\n\nIf you resumed the above tasks, suspend them to avoid unecessary resource\nutilization by uncommenting and executing the following commands.\n\n    \n    \n    # root_task = tasks[\"DE_PIPELINE_DAG\"]\n    # root_task.suspend()", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b3197e30-2954-4f1d-8b2b-c85e39a2a0a0": {"__data__": {"id_": "b3197e30-2954-4f1d-8b2b-c85e39a2a0a0", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "388f6cb4-6216-416f-8f29-a4713a0e820a", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "5e53da8bf9d8742ea9c1f95994f5e790f5a31eb1ca315d76dede01cab4a8cde0", "class_name": "RelatedNodeInfo"}}, "text": "Tasks Observability\n\nThese tasks and their [DAGs](https://docs.snowflake.com/en/user-guide/tasks-\nintro#label-task-dag) can be viewed in\n[Snowsight](https://docs.snowflake.com/en/user-guide/ui-snowsight-\ntasks#viewing-individual-task-graphs) as shown below.\n\n!Tasks-Observability", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6fdaf52b-d1f7-4105-ac30-fa2fa1a97fa9": {"__data__": {"id_": "6fdaf52b-d1f7-4105-ac30-fa2fa1a97fa9", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b59e7ddf-4e06-45f8-98fd-1be93facbf52", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "3f5564a4f41f2dd6fa6a392a141fb8dfadda7c372124415c19b828a08e4c29cb", "class_name": "RelatedNodeInfo"}}, "text": "Error Notificatons For Tasks\n\nYou can also enable push notifications to a cloud messaging service when\nerrors occur while tasks are being executed. For more information, please\nrefer to the [documentation](https://docs.snowflake.com/en/user-guide/tasks-\nerrors).", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "892707aa-c62a-4f1c-a02a-3be6d7268ed7": {"__data__": {"id_": "892707aa-c62a-4f1c-a02a-3be6d7268ed7", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b96cdbe8-fc7e-4740-89ea-3fb497343439", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d193f0eea77dec7cd008603c6de5a6cb997f8ff0504d96875d0a9facd8515d3e", "class_name": "RelatedNodeInfo"}}, "text": "5\\. Machine Learning\n\nPREREQUISITE: Successful completion of steps outlined under [Data\nEngineering](https://github.com/Snowflake-Labs/sfguide-getting-started-\ndataengineering-ml-snowpark-python/blob/main/Snowpark_For_Python_DE.ipynb).\n\nThe Notebook linked below covers the following machine learning tasks.\n\n  1. Load features and target from Snowflake table into Snowpark DataFrame\n  2. Prepare features for model training\n  3. Train ML model using Snowpark ML in Snowflake\n  4. Register ML model and use it for inference from Snowflake Model Registry", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bdbd137-a951-49f7-a0f3-a2e61c89ee81": {"__data__": {"id_": "5bdbd137-a951-49f7-a0f3-a2e61c89ee81", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dafe8e9-9c65-4a5b-88c1-6c01a380db43", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c028c8f7ae625353508cc9d4ef22edefb14079eefc52590adbecd7e676c5becb", "class_name": "RelatedNodeInfo"}}, "text": "Machine Learning Notebook\n\n  1. Click on Snowpark_For_Python_ML.ipynb to download the Notebook from GitHub. **_(NOTE: Do NOT right-click to download.)_**\n  2. In your Snowflake account:\n\n  * On the left hand navigation menu, click on **Projects** \u00bb **Notebooks**\n  * On the top right, click on **Notebook** down arrow and select **Import .ipynb file** from the dropdown menu\n  * Select the file you downloaded in step 1 above\n\n  3. In the Create Notebook popup\n\n  * For **Notebook location** , select DASH_DB and DASH_SCHEMA\n  * For **SQL warehouse** , select DASH_S\n  * Click on **Create** button\n\nIf all goes well, you should see the following:\n\n!Snowflake DE NB\n\n  4. On the top right, click on **Packages** and make sure you install `snowflake-ml-python` package by typing it in the search box and selecting the first one.\n  5. On the top right, click on **Start**. **_(NOTE: The first time it will take a couple of mins to install the packages.)_**\n  6. Once the packages are installed and the state changes from **Start** \u00bb **Starting** \u00bb **Active** , you can either click on **Run all** to execute all cells, or you can run individual cells in the order from top to bottom by clicking on the play icon on the top right corner of every cell.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6badb99c-27b8-451e-a1e5-8f7a2fa2665a": {"__data__": {"id_": "6badb99c-27b8-451e-a1e5-8f7a2fa2665a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79e8e835-1128-480a-a8c8-4dbd220db150", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "291e0fabe8e3e6b5be75eb80de07bdee715cf9a70f5807ac267d78920b1e2f2c", "class_name": "RelatedNodeInfo"}}, "text": "6\\. Streamlit Application\n\nFollow these steps to build Streamlit application in Snowsight.\n\n**Step 1.** Click on **Streamlit** on the left navigation menu\n\n**Step 2.** Click on **\\+ Streamlit App** on the top right\n\n**Step 3.** Enter **App title**\n\n**Step 4.** Select **App location** (DASH_DB and DASH_SCHEMA) and **App\nwarehouse** (DASH_S)\n\n**Step 5.** Click on **Create**\n\n  * At this point, you will be provided code for an example Streamlit application\n\n**Step 6.** Replace sample application code displayed in the code editor on\nthe left with the code provided in\n[Snowpark_Streamlit_Revenue_Prediction_SiS.py](https://github.com/Snowflake-\nLabs/sfguide-ad-spend-roi-snowpark-python-streamlit-scikit-\nlearn/blob/main/Snowpark_Streamlit_Revenue_Prediction_SiS.py)\n\n**Step 7.** Click on **Run** on the top right\n\nIf all goes well, you should see the application in Snowsight as shown below.\n\n!Streamlit-in-Snowflake\n\n**Step 8.** Save data to Snowflake\n\nIn the application, adjust the advertising budget sliders to see the predicted\nROI for those allocations. You can also click on **Save to Snowflake** button\nto save the current allocations and predicted ROI into\nBUDGET_ALLOCATIONS_AND_ROI Snowflake table.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb24ccbf-9161-49f3-bd4d-1574b0dab7ba": {"__data__": {"id_": "cb24ccbf-9161-49f3-bd4d-1574b0dab7ba", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b5c33d1-cbfc-4552-8761-8277156d2981", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "fd19e08f983ee44ebc83a50108e45aa5375721e3803b145a7e3f398a7052e781", "class_name": "RelatedNodeInfo"}}, "text": "7\\. Cleanup\n\nIf you started/resumed the tasks as part of the **Data Engineering** or **Data\nPipelines** sections, then it is important that you run the following commands\nto suspend those tasks in order to avoid unecessary resource utilization.\n\n_Note: Suspending the root task will suspend the full DAG._\n\n    \n    \n    root_task = tasks[\"DE_PIPELINE_DAG\"]\n    root_task.suspend()", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d7429b22-b3fa-4ca3-8d2e-fca40bcdb02b": {"__data__": {"id_": "d7429b22-b3fa-4ca3-8d2e-fca40bcdb02b", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ded36305-a0ed-4456-9622-353b15d30a01", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "c95fd847b35771fdcc8484d41a4c76e64ae5fbdc834e850ca3f05b89ce51b99b", "class_name": "RelatedNodeInfo"}}, "text": "8\\. Conclusion And Resources\n\nCongratulations! You've successfully performed data engineering tasks and\ntrained a Linear Regression model to predict future ROI (Return On Investment)\nof variable advertising spend budgets across multiple channels including\nSearch, Video, Social Media, and Email using Snowpark for Python and scikit-\nlearn. And then you created a Streamlit application that uses that model to\ngenerate predictions on new budget allocations based on user input.\n\nWe would love your feedback on this QuickStart Guide! Please submit your\nfeedback using this Feedback Form.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf756990-45c6-48ce-8dc9-dbae27209b69": {"__data__": {"id_": "cf756990-45c6-48ce-8dc9-dbae27209b69", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbff4927-6f8a-4717-a7b5-4faca5c3ca75", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "165fa0b757047d6088ec0349c7979ff60ca3d11c62d1654f24e422ff16361563", "class_name": "RelatedNodeInfo"}}, "text": "What You Learned\n\n  * How to analyze data and perform data engineering tasks using Snowpark DataFrames and APIs\n  * How to use open-source Python libraries from curated Snowflake Anaconda channel\n  * How to create Snowflake Tasks to automate data pipelines\n  * How to train ML model using Snowpark ML in Snowflake\n  * How to register ML model and use it for inference from Snowflake Model Registry\n  * How to create Streamlit application that uses the ML Model for inference based on user input", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1a8a8ba-e2a0-4d78-8326-d22cf10e2b4a": {"__data__": {"id_": "c1a8a8ba-e2a0-4d78-8326-d22cf10e2b4a", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "202dbead-1d12-4e62-93dc-001ca0f99504", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "d352910ee73bf7ee1026a2daeb9b99df95eadc5b7aca207e8115524aabfdcf94", "class_name": "RelatedNodeInfo"}}, "text": "Related Resources\n\n  * Source Code on GitHub\n  * Snowpark for Python Developer Guide\n  * Snowpark for Python API Reference\n  * Snowpark ML Modeling\n  * Snowpark ML Model Registry\n\nBack\n\nNextDone\n\n!\n!\n\n![](https://bat.bing.com/action/0?ti=25015801&tm=gtm002&Ver=2&mid=3a285634-a0f6-47cb-8a33-20b860800747&sid=93707510865a11efb826119f960e6912&vid=93708ed0865a11ef8391199cf4acf1c2&vids=1&msclkid=N&pi=0&lg=en-\nUS&sw=1800&sh=1169&sc=30&nwd=1&tl=Getting%20Started%20with%20Data%20Engineering%20and%20ML%20using%20Snowpark%20for%20Python%20and%20Snowflake%20Notebooks&p=https%3A%2F%2Fquickstarts.snowflake.com%2Fguide%2Fgetting_started_with_dataengineering_ml_using_snowpark_python%2F%230&r=&lt=668&evt=pageLoad&sv=1&cdb=AQAQ&rn=41130)!\n\n!\n\n!", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c43ec9d4-300f-4667-aafb-70ee67eb608e": {"__data__": {"id_": "c43ec9d4-300f-4667-aafb-70ee67eb608e", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf6128b9-70c3-4494-a4ae-99105d7678e7", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "20ccbf5d3dd914f01857b46d8fafb5705715b566ae66142d400361f4dd29b9f9", "class_name": "RelatedNodeInfo"}}, "text": "Snowflake's Use of Cookies\n\nWe use cookies to enhance your experience and to analyze site traffic as\ndescribed in our Cookie Statement. By accepting, you consent to our use of\ncookies.[Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-\nstatement/)\n\nCookies Settings Reject All Accept All Cookies\n\n![Company\nLogo](https://cdn.cookielaw.org/logos/cb85e692-4053-4d0a-8dda-d24b5daa8b06/ff6c124b-1473-4861-9ca3-9eaf6debb37d/SNO-\nSnowflakeLogo_blue.png)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e159ac80-5025-4508-afc1-2c0a6fc23180": {"__data__": {"id_": "e159ac80-5025-4508-afc1-2c0a6fc23180", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1df3f0a-8d73-4c0b-8721-17fe836343ba", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "00968f8ef28aa0dd697be334122ec1349818b1ea03d58dfdfc117130c3e380bb", "class_name": "RelatedNodeInfo"}}, "text": "Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary Cookies\n\n  * ### Performance Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting Cookies", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "734abeab-32a4-4daa-a9f1-de9549ac0522": {"__data__": {"id_": "734abeab-32a4-4daa-a9f1-de9549ac0522", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "147d18ed-d32d-443e-82e6-76ab6e61e411", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "a2baa829439fc75a1aab37db3a8a6c865661cce44d2a5228e1f563c23a106e83", "class_name": "RelatedNodeInfo"}}, "text": "Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.  \nMore information", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8f44c2d0-feb2-4854-98d7-8cd6310b9d3d": {"__data__": {"id_": "8f44c2d0-feb2-4854-98d7-8cd6310b9d3d", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e29c248-6458-428c-8f85-9c0c1761f29d", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "cc376b8c44100e262b556067a57b59cfb7f25dc1adf4a1c5e3c955031c799fac", "class_name": "RelatedNodeInfo"}}, "text": "Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff. They are usually only set in response to actions made by you which amount\nto a request for services, such as setting your privacy preferences, logging\nin or filling in forms. You can set your browser to block or alert you about\nthese cookies, but some parts of the site will not then work. These cookies do\nnot store any personally identifiable information.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4610c535-7ea7-442e-94e0-10d70077b49c": {"__data__": {"id_": "4610c535-7ea7-442e-94e0-10d70077b49c", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c90174a3-70e5-46d9-a27c-48b681479604", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "60846caf586e38134940fe025b640101fccf65c0fd13d4b94c07399720f42fa3", "class_name": "RelatedNodeInfo"}}, "text": "Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site.    All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec74560d-6930-4d06-934d-93d544f32f70": {"__data__": {"id_": "ec74560d-6930-4d06-934d-93d544f32f70", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ddc942e-33ac-48a5-a9d3-ec7613333513", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "9f94c3fbbe70513fa3a314cec5cddf6630a0a2ddc29a247802ba0604add2d0fa", "class_name": "RelatedNodeInfo"}}, "text": "Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages.    If you do not allow these cookies then\nsome or all of these services may not function properly.\n\nCookies Details\u200e", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1e7ea5c-59d1-4714-a57a-0910efc22dc7": {"__data__": {"id_": "a1e7ea5c-59d1-4714-a57a-0910efc22dc7", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44c59420-9342-4295-9c90-2ca56c95ccbc", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "2c48533b4ec4ef3f10d1cc91eac1f09da996fd36ac9e1b8681992eada77cacb4", "class_name": "RelatedNodeInfo"}}, "text": "Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly identifiable\npersonal information, but are based on uniquely identifying your browser and\ninternet device. If you do not allow these cookies, you will experience less\ntargeted advertising.\n\nCookies Details\u200e\n\nBack Button", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f8180db-7320-48f6-9d9f-f078abe7c774": {"__data__": {"id_": "5f8180db-7320-48f6-9d9f-f078abe7c774", "embedding": null, "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "264e76d8-6025-4ff0-a8cb-16c7fbcac64b", "node_type": "4", "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}, "hash": "14f46dbb82a2daf9e36357eaa6e096d237ad1f8f57fad1e9dc92ae6695489d4b", "class_name": "RelatedNodeInfo"}}, "text": "Cookie List\n\nFilter Button\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[![Powered by\nOnetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-\nconsent/)", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5232df30-d306-4b3f-913f-1d15d4fa847f": {"__data__": {"id_": "5232df30-d306-4b3f-913f-1d15d4fa847f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document titled \"Cloud Native Data Engineering with Matillion and Snowflake\" provides an overview of how modern businesses can leverage Snowflake and Matillion to convert raw data into actionable insights. The lab scenario involves managing a team of traders across different industries and creating an end-to-end data transformation pipeline for financial services data. The steps include acquiring historical stock data, launching Matillion ETL, ingesting data from an S3 bucket, creating transformation pipelines, and leveraging the Yahoo Finance API for real-time data.\n\nThe document guides users on getting started with Snowflake by accessing data from the Snowflake Marketplace, creating a new database, and locating account information. It then explains how to launch Matillion ETL from Partner Connect, create a new project, set up AWS and Snowflake connections, and configure default values. The goal is to enable users to build a comprehensive data transformation pipeline for financial services data using Matillion and Snowflake.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "733e5456-7fb7-44c0-b5da-2f1b07323d5a": {"__data__": {"id_": "733e5456-7fb7-44c0-b5da-2f1b07323d5a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on creating an orchestration job in Matillion for Snowflake. The job involves loading trading history data from AWS S3 to Snowflake, modifying warehouse size, creating transformation jobs for complex calculations and joins, and scaling down the warehouse after completion. The orchestration job includes steps like using the S3 Load Generator component, altering warehouse size, and creating tables in Snowflake. Additionally, it covers creating a transformation job to determine the current position based on trading history data, filtering actions, calculating investments, aggregating data, and joining different flows together. The document includes step-by-step instructions, screenshots, and configuration details for each component used in the process.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0ff8f71b-233e-422f-97cc-9e18e3f235cb": {"__data__": {"id_": "0ff8f71b-233e-422f-97cc-9e18e3f235cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a step-by-step guide on how to create a transformation job using Matillion and Snowflake. It includes instructions on joining buy and sell aggregations, calculating investment amounts, average stock prices, and writing the results to a table. Additionally, it explains how to filter data to include the most recent stock close date and join it with the current position dataset. The document also covers calculating realized and unrealized gains/losses for each trader. The process involves using various components such as Join, Calculator, and Rewrite Table, and concludes with running the transformation job to validate the results.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32dccfd9-54d0-4ee0-8042-cda283c63da9": {"__data__": {"id_": "32dccfd9-54d0-4ee0-8042-cda283c63da9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on how to calculate realized and unrealized gains using Matillion and Snowflake. It includes steps on how to set up a calculator component, edit properties, add expressions for calculations, write results to a new table, aggregate gains per trader, and create a view to store the aggregation result. Additionally, it explains how to orchestrate the job, scale down the warehouse size, and run the job. A bonus section covers pulling current stock price data from Yahoo Finance using Matillion's Universal Connectivity feature. It includes steps on creating an orchestration job, managing grid variables, incorporating a Python script, setting up an API extract to pull stock price data, and transforming the extracted data. The document also includes instructions on flattening semi-structured data and extracting the necessary values.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15132885-82ae-4f91-9a0c-7a90628fca79": {"__data__": {"id_": "15132885-82ae-4f91-9a0c-7a90628fca79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on cloud-native data engineering with Matillion and Snowflake. It includes steps to extract, transform, and load data using Matillion's GUI, perform joins, filters, and aggregations, and scale up/down Snowflake's virtual warehouses. The process involves reading TRADER_PNL_TODAY data, filtering Cersei's trades, joining data with the Yahoo API, calculating win/loss logic, and writing profits back to Snowflake. The final flow displays stock information, quantity, average prices, and realized gains for Cersei. The conclusion celebrates the successful development of a data engineering pipeline. The document also mentions the benefits of using Matillion ETL for Snowflake and provides related resources for further information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cbeebb7b-90a3-40ee-9da1-731ed89da8c8": {"__data__": {"id_": "cbeebb7b-90a3-40ee-9da1-731ed89da8c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information includes details about different types of cookies used on a website, such as Strictly Necessary Cookies, Performance Cookies, Functional Cookies, and Targeting Cookies. Each type of cookie serves a specific purpose related to website functionality, performance measurement, enhanced functionality, and targeted advertising. Additionally, there is information about data engineering pipelines with Snowpark Python, covering topics like building and maintaining data pipelines, data engineering process, Snowflake features, prerequisites, and what will be learned during the Quickstart. The Quickstart aims to help users build a robust data engineering pipeline using Snowpark Python stored procedures, process data incrementally, orchestrate tasks with Snowflake, and deploy via a CI/CD pipeline.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d9f7c2f-2508-4971-abbf-7173f6e18224": {"__data__": {"id_": "2d9f7c2f-2508-4971-abbf-7173f6e18224", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "To begin the data engineering pipelines with Snowpark Python, you will need a Snowflake account, a Snowflake user with ACCOUNTADMIN permissions, and Anaconda Terms & Conditions accepted. Additionally, a GitHub account is required. \n\nDuring the quickstart, you will load Parquet data to Snowflake, set up access to Snowflake Marketplace data, create a Python UDF to convert temperature, build a data engineering pipeline with Python stored procedures, orchestrate the pipelines with tasks, monitor them with Snowsight, and deploy the Snowpark Python stored procedures via a CI/CD pipeline.\n\nTo set up, fork the Quickstart repository, enable GitHub Actions, and create a GitHub Codespace. The Codespace will include the setup of a container environment, Anaconda installation, Snowpark Python library installation, VS Code configuration, and Snowflake VS Code extension installation.\n\nConfigure Snowflake credentials by editing the `~/.snowflake/connections.toml` file. Verify that the Anaconda environment named `snowflake-demo` is activated in the terminal.\n\nSetup Snowflake using the Snowflake extension for VS Code. Run the `steps/01_setup_snowflake.sql` script to create the necessary objects in Snowflake for the quickstart.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4cfcc553-4d6d-47bc-920d-1113db94514b": {"__data__": {"id_": "4cfcc553-4d6d-47bc-920d-1113db94514b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "In this data engineering pipeline with Snowpark Python, the process involves loading raw Tasty Bytes POS and Customer loyalty data from Parquet files in an S3 bucket to Snowflake schemas. The process is orchestrated from a laptop using Python and the Snowpark Python API. The script `02_load_raw.py` is executed from the terminal to load the raw data into Snowflake. The Snowpark Python code is run locally, creating a Snowpark session and calling the `load_all_raw_tables(session)` method. Snowflake's Query History logs every query run against the account, showcasing the scalability and compute power of Snowflake. Schema inference is done automatically when using the `session.read()` method. Data ingestion into Snowflake tables is done using the `copy_into_table()` method. Snowflake's table format eliminates the need to manage a file-based data lake. Virtual warehouses in Snowflake allow for dynamic scaling of compute clusters. The process also involves accessing weather data from Weather Source in the Snowflake Marketplace without the need for custom ETL processes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a34e275b-5d72-4eee-83c5-942ada88e87f": {"__data__": {"id_": "a34e275b-5d72-4eee-83c5-942ada88e87f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The process of creating data engineering pipelines with Snowpark Python involves several steps. \n\n1. Running a script to query data from the Snowflake Marketplace.\n2. Creating a view to simplify the raw POS schema by joining tables and selecting necessary columns using the Snowpark DataFrame API.\n3. Running a script to create the view and stream for incremental processing.\n4. Defining a Snowflake view using the Snowpark DataFrame API and creating a view with the `create_or_replace_view()` method.\n5. Utilizing Snowflake streams for incremental processing, which eliminates the need to track high watermarks manually.\n6. Creating a stream on a view to track changes in multiple tables included in the view.\n7. Creating and deploying a user-defined function (UDF) to convert Fahrenheit to Celsius using Snowpark Python objects and SnowCLI.\n8. Testing the UDF locally and deploying it to Snowflake using SnowCLI.\n9. Running the UDF in Snowflake through SQL queries or the SnowCLI utility. \n\nThese steps demonstrate the ease and efficiency of building data engineering pipelines with Snowpark Python and Snowflake's capabilities for incremental processing and UDF deployment.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38a3d2ba-d63a-4716-bee8-8bdea6a79f00": {"__data__": {"id_": "38a3d2ba-d63a-4716-bee8-8bdea6a79f00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The SnowCLI tool is a command line tool for developers that simplifies the development and deployment of Snowflake objects such as Snowpark Python UDFs, Snowpark Python Stored Procedures, and Streamlit Applications. It handles tasks like dealing with third-party packages, creating zip files of projects, copying files to Snowflake stages, and creating Snowflake function or stored procedure objects. The tool allows for developing and testing Python applications without worrying about wrapping them in corresponding Snowflake database objects. The SnowCLI tool is still in preview as of 3/7/2024.\n\nA Python UDF was deployed to Snowflake using the SnowCLI tool, generating a SQL query for the function. Similarly, a Python stored procedure was created and deployed to Snowflake, with the SnowCLI tool handling the SQL DDL syntax. The Snowpark API is used for data transformations, providing functionality similar to the Spark SQL API. The API includes methods for creating objects in Snowflake, checking object status, and merging changes from source views to target tables using DataFrames.\n\nIn a separate step, a second Snowpark Python stored procedure was created to join data from the `HARMONIZED.ORDERS` table with Weather Source data to create an aggregated table for analysis named `ANALYTICS.DAILY_CITY_METRICS`. The procedure can be tested locally and deployed to Snowflake using the SnowCLI tool.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f1891453-d80f-4530-9c69-93367a8b6fda": {"__data__": {"id_": "f1891453-d80f-4530-9c69-93367a8b6fda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a guide on deploying stored procedures (sprocs) in Snowflake using the SnowCLI tool. It explains how to deploy the sproc to Snowflake and run it through SQL or the SnowCLI utility. Additionally, it discusses data modeling best practices for creating tables in Snowflake. The document also covers a complex aggregation query and how to orchestrate jobs using Snowflake's native orchestration feature named Tasks. It explains how to run tasks manually and provides information on task metadata and monitoring tasks in Snowflake's Snowsight UI.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3d87735f-c4ee-456f-b1dd-2b91a5e68a78": {"__data__": {"id_": "3d87735f-c4ee-456f-b1dd-2b91a5e68a78", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The data engineering pipeline with Snowpark Python involves processing data incrementally using Snowflake's stream/CDC capabilities. The process includes adding new data to POS order tables, running the pipeline, loading data through SQL, resizing the warehouse for faster data loading, and running task DAGs to process new data. Viewing task history and querying task history for tasks are important steps to monitor the pipeline. Deploying changes via CI/CD involves updating a UDF, testing changes locally, configuring GitHub secrets for Snowflake credentials, pushing changes to a forked repository, and viewing GitHub Actions workflow results. The workflow includes reviewing the code, monitoring the deployment job, and checking the output for familiar information.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d40067f8-4c45-4c79-9a07-be5407fe3e78": {"__data__": {"id_": "d40067f8-4c45-4c79-9a07-be5407fe3e78", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document discusses building a data engineering pipeline using Snowpark Python stored procedures. It covers various topics such as Snowflake's Table Format, data ingestion with COPY, schema inference, data sharing, streams for incremental processing, Python UDFs, Python Stored Procedures, Snowpark DataFrame API, warehouse elasticity, Visual Studio Code extension, SnowCLI tool, tasks with Stream triggers, task observability, and GitHub Actions integration. The conclusion highlights the importance of exploring more possibilities with Snowpark. The document also includes information on how to clean up after completing the Quickstart by running a teardown script. Additionally, related resources such as a demo on Snowflake Demo Hub, source code on GitHub, Snowpark Developer Guide for Python, and related tools are provided.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4435a791-582a-48ff-ac93-357740d43bee": {"__data__": {"id_": "4435a791-582a-48ff-ac93-357740d43bee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "This summary provides an overview of a guide on Data Engineering with Apache Airflow, Snowflake, Snowpark, dbt, and Cosmos. The guide covers setting up the environment, creating CSV data files, building dbt models, preparing the Airflow environment, creating DAGs, adding connections, running docker-compose for Airflow, incorporating Snowpark, and creating DAGs with Cosmos and Snowpark. The guide also includes information on prerequisites, what will be learned, what is needed, and what will be built. It emphasizes using Airflow to create data schedulers, writing DAGs, building scalable pipelines with dbt, Airflow, and Snowflake, and using Snowpark to interact with Snowflake data using Python. The guide assumes a basic knowledge of Python, SQL, and dbt.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9cae9f41-4e8a-4d9f-a373-806188db1987": {"__data__": {"id_": "9cae9f41-4e8a-4d9f-a373-806188db1987", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on setting up a dbt project in Snowflake using Apache Airflow. It includes steps for configuring the dbt project, creating tables in Snowflake, setting up dbt models for analysis and transformation, preparing the Airflow environment, building a dbt DAG in Airflow, starting the Airflow environment, and running the docker-compose file for Airflow. Key details include creating CSV data files, setting up dbt models for analysis and transformation, creating a virtual environment for dbt, and using Cosmos' dbtDAG class to create an Apache Airflow DAG for executing the dbt project. The document also covers creating connections in Airflow and starting the Airflow environment using the Astro CLI.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9e4072d-302e-442c-892d-90d3799dcd8e": {"__data__": {"id_": "d9e4072d-302e-442c-892d-90d3799dcd8e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The summary covers the process of setting up Apache Airflow, dbt, Snowflake, and Snowpark to run data analysis tasks. It starts with running the `cosmos_dag` to see dbt models in action within the Snowflake database. The next step involves incorporating Snowpark for data analysis with Python, requiring changes to existing files and adding new requirements to the local Airflow environment. A new DAG called `dbt_snowpark` is created to combine Cosmos and Snowpark, with a task to find the hotel with the highest cost in a Snowflake database. The process includes restarting the Airflow environment and running the new DAG. Finally, the analyzed data can be viewed on a Streamlit dashboard by connecting to the Airflow webserver container. The conclusion encourages further exploration of Airflow and Snowflake capabilities. Additional resources and tutorials are provided for further learning.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4109ca5e-1a25-47aa-bbfd-d7af66b5a60c": {"__data__": {"id_": "4109ca5e-1a25-47aa-bbfd-d7af66b5a60c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document discusses privacy preferences and the use of cookies on websites. It explains the different types of cookies - strictly necessary, performance, functional, and targeting cookies. Strictly necessary cookies are essential for website functionality, while performance cookies help measure and improve site performance. Functional cookies enhance website functionality, and targeting cookies are used by advertising partners to show relevant ads. Users can adjust their cookie preferences to enhance their browsing experience. Additionally, the document also covers data engineering with Snowpark Python and dbt, providing an overview and steps to create Python models with dbt.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94ba990e-9836-4650-9f03-f31df2444a7b": {"__data__": {"id_": "94ba990e-9836-4650-9f03-f31df2444a7b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides an overview of data engineering, focusing on the data transformation phase. It discusses the use of SQL and Python for data transformation, with Python becoming increasingly popular. The introduction of Python-based models in dbt version 1.3 allows for more flexibility in data transformation. This is made possible by Snowflake's Snowpark Python capabilities, which include a Python API, UDFs, and integration with Anaconda. The guide outlines prerequisites for getting started with Snowpark Python and dbt, including having a Snowflake account, Anaconda, and dbt installed. It also lists the key learnings, such as creating Python-based models in dbt and using Python UDFs. The document details the steps to create a dbt project and a simple Python model within the project.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a6d937ef-dcab-4123-9293-1ee32e7b2ecc": {"__data__": {"id_": "a6d937ef-dcab-4123-9293-1ee32e7b2ecc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on creating Python models in dbt with Snowflake Snowpark. It starts by explaining how to create a basic Python model named `my_first_python_model.py` in the `models/example` folder. The model must define a `model` method with specific configurations and must be materialized as a table or incremental view. The document highlights key points such as not needing Jinja, automatic Snowflake Snowpark Python library import, and the supported materializations.\n\nIt then delves into understanding how the simple dbt Python model works by reviewing the executed queries in Snowflake's Query History. The overview of executed queries includes listing schemas, creating a stored procedure, materializing the model, and dropping the stored procedure.\n\nThe document proceeds to explain the stored procedure code generated by dbt for Python models, emphasizing the separation of model definition and execution in Snowflake. It provides a detailed breakdown of the generated Python code, including the `dbtObj` class, materialization logic, and the `main` function for executing the model.\n\nNext, it introduces creating a Python model with a User-Defined Function (UDF) named `my_second_python_model.py`. The model includes a simple UDF called `add_one()` that increments a number. The process of defining and registering a UDF within a Python model is explained, along with the execution steps when running the model.\n\nThe document also covers where to define UDFs in Python models, highlighting the importance of defining and registering functions inside the `main()` handler function to avoid errors. An example from dbt's documentation is provided to illustrate the correct approach.\n\nLastly, it explores how the dbt Python model with a UDF works by reviewing the executed queries in Snowflake, similar to the previous model but with additional queries related to creating and using the UDF. The complexity of handling UDFs in Snowflake's Python stored procedure is discussed, including steps like pickling the UDF content, creating a temporary stage, and defining the UDF in Snowflake.\n\nOverall, the document serves as a comprehensive guide for data engineers looking to leverage Python models in dbt with Snowflake Snowpark, covering basic model creation, UDF implementation, and the execution process in Snowflake.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e886dc2-85f8-4cbf-bc6e-d6b8a6d35a89": {"__data__": {"id_": "8e886dc2-85f8-4cbf-bc6e-d6b8a6d35a89", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The combination of dbt Python models and Snowflake platform is powerful for data engineers to build robust data pipelines. While SQL and Python transformations are supported in dbt, Python can be used for cases where SQL is insufficient. Snowflake's native Python support and Snowpark API eliminate the need for separate infrastructure to run Python code. Debugging dbt Python models can be done within dbt or directly in Python using tools like Visual Studio Code. The basics of Snowpark Python, creating Python-based models in dbt, using Python UDFs, and the integration between Snowpark Python and dbt's Python models were covered. Related resources include the Snowpark Developer Guide for Python, Snowpark API Reference, and dbt Python models. Snowflake's use of cookies and privacy preferences were also discussed.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0077b45-902c-4a43-be8a-d0a18cb600c2": {"__data__": {"id_": "e0077b45-902c-4a43-be8a-d0a18cb600c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The provided information discusses the use of Snowpark for Python and Snowflake Notebooks in data engineering and machine learning tasks. It covers setting up the environment, data engineering, data pipelines, machine learning, and building interactive Streamlit applications. Snowpark allows for the execution of Python and other programming languages next to data in Snowflake, enabling the development of data pipelines, ML models, and apps. Snowflake ML provides capabilities for end-to-end machine learning workflows within a single platform. The guide focuses on using Snowpark ML Modeling API and Snowflake Model Registry for feature engineering, model training, and model management. Additionally, it highlights the integration of Streamlit with Snowflake for data visualization and application development. Learners will gain knowledge on analyzing data, automating data pipelines, training ML models, and creating Streamlit applications using Snowpark and Snowflake. The practical projects include building a data engineering pipeline, a machine learning model, and a Streamlit application.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d999afd1-e232-441f-9ad9-6979f9b6eefe": {"__data__": {"id_": "d999afd1-e232-441f-9ad9-6979f9b6eefe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "This document provides a guide on getting started with data engineering and machine learning using Snowpark Python in Snowflake. The prerequisites include having a Snowflake account with Anaconda Packages enabled by ORGADMIN and a login with ACCOUNTADMIN role. The setup involves creating warehouses, databases, and schemas, as well as creating tables and loading data from Amazon S3. The data engineering tasks include loading data into Snowpark DataFrames, performing exploratory data analysis, pivoting and joining data, and automating data pipelines using Snowflake Tasks. The document also covers creating data pipelines and tasks for automating data transformations. Additionally, it includes instructions on downloading and importing a Data Engineering Notebook in Snowflake. Finally, it provides code snippets for automating tasks related to campaign spend data and monthly revenue data, as well as setting up a Tasks DAG for task scheduling.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58f4e91c-010e-4039-a292-ae413046daf2": {"__data__": {"id_": "58f4e91c-010e-4039-a292-ae413046daf2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a guide on getting started with data engineering and machine learning using Snowpark Python in Snowflake. It includes information on resuming and suspending tasks, observing tasks in Snowsight, enabling error notifications, and prerequisites for machine learning tasks. It also covers setting up a machine learning notebook, building a Streamlit application in Snowsight, and cleaning up tasks. The guide concludes with a summary of the tasks performed, including data analysis, data engineering, ML model training, and creating a Streamlit application for ROI prediction.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4cc77699-cebb-49f0-a0c6-48a9423f5c5b": {"__data__": {"id_": "4cc77699-cebb-49f0-a0c6-48a9423f5c5b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides information on getting started with data engineering and machine learning using Snowpark for Python and Snowflake Notebooks. It includes related resources such as the Source Code on GitHub, Snowpark for Python Developer Guide, Snowpark for Python API Reference, Snowpark ML Modeling, and Snowpark ML Model Registry. The document also discusses Snowflake's use of cookies and privacy preferences, including options for Strictly Necessary Cookies, Performance Cookies, Functional Cookies, and Targeting Cookies. It explains how cookies are used to enhance user experience and provides details on each type of cookie. The document also includes a Cookie List with options to filter and manage cookie preferences.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35dda69f-47d7-4d01-938b-557c210df6ff": {"__data__": {"id_": "35dda69f-47d7-4d01-938b-557c210df6ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document titled \"Cloud Native Data Engineering with Matillion and Snowflake\" provides a comprehensive guide on leveraging Snowflake and Matillion to convert raw data into actionable insights. The lab scenario involves managing a team of traders across different industries and creating an end-to-end data transformation pipeline for financial services data. The steps include acquiring historical stock data, launching Matillion ETL, ingesting data from an S3 bucket, creating transformation pipelines, and leveraging the Yahoo Finance API for real-time data.\n\nThe document guides users on getting started with Snowflake by accessing data from the Snowflake Marketplace, creating a new database, and locating account information. It then explains how to launch Matillion ETL from Partner Connect, create a new project, set up AWS and Snowflake connections, and configure default values. The goal is to enable users to build a comprehensive data transformation pipeline for financial services data using Matillion and Snowflake.\n\nThe document provides detailed guides on creating orchestration jobs, transformation jobs, and calculating realized and unrealized gains using Matillion and Snowflake. It includes step-by-step instructions, screenshots, and configuration details for each component used in the process.\n\nAdditionally, the document covers data engineering pipelines with Snowpark Python, including steps to load Parquet data to Snowflake, create Python UDFs, build data engineering pipelines with Python stored procedures, orchestrate tasks, monitor with Snowsight, and deploy via a CI/CD pipeline. Users need a Snowflake account, a Snowflake user with ACCOUNTADMIN permissions, and a GitHub account to begin the process.\n\nThe document also explains the process of creating data engineering pipelines with Snowpark Python, involving steps like querying data from the Snowflake Marketplace, creating views, utilizing Snowflake streams for incremental processing, deploying UDFs, and testing them locally and in Snowflake.\n\nFurthermore, the document introduces the SnowCLI tool for simplifying the development and deployment of Snowflake objects like UDFs and stored procedures. The tool handles tasks like dealing with third-party packages, creating zip files of projects, copying files to Snowflake stages, and creating Snowflake function or stored procedure objects.\n\nOverall, the document provides a detailed overview of cloud-native data engineering with Matillion and Snowflake, showcasing the capabilities of both platforms in building efficient data transformation pipelines for various use cases.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f707390c-e85f-4864-bb7a-e2fa9df457ce": {"__data__": {"id_": "f707390c-e85f-4864-bb7a-e2fa9df457ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a detailed guide on deploying stored procedures in Snowflake using the SnowCLI tool, data modeling best practices, complex aggregation queries, and orchestrating jobs with Snowflake Tasks. It also covers data engineering pipelines with Snowpark Python, incremental data processing, warehouse resizing, and CI/CD deployment. The guide includes information on setting up dbt projects in Snowflake with Apache Airflow, creating DAGs, and incorporating Snowpark for data analysis. It discusses privacy preferences and the use of cookies on websites. Additionally, the document explores data transformation using SQL and Python, Python models in dbt with Snowflake Snowpark, and the execution process in Snowflake. The combination of dbt Python models and Snowflake is highlighted for building robust data pipelines, with a focus on Python transformations and UDF implementation. Key details include creating Python models, defining UDFs, and debugging Python models within dbt or Visual Studio Code. The document emphasizes the integration between Snowpark Python and dbt's Python models for efficient data engineering processes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7de2996a-063e-41d0-aa45-83104914b21e": {"__data__": {"id_": "7de2996a-063e-41d0-aa45-83104914b21e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "text": "The document provides a comprehensive guide on utilizing Snowpark for Python and Snowflake Notebooks in data engineering and machine learning tasks. It covers setting up the environment, data engineering, data pipelines, machine learning, and building interactive Streamlit applications. Snowpark allows for the execution of Python and other programming languages alongside data in Snowflake, enabling the development of data pipelines, ML models, and apps. Snowflake ML provides capabilities for end-to-end machine learning workflows within a single platform. The guide focuses on using Snowpark ML Modeling API and Snowflake Model Registry for feature engineering, model training, and model management. Additionally, it highlights the integration of Streamlit with Snowflake for data visualization and application development. Learners will gain knowledge on analyzing data, automating data pipelines, training ML models, and creating Streamlit applications using Snowpark and Snowflake. The practical projects include building a data engineering pipeline, a machine learning model, and a Streamlit application. The document also includes information on setting up warehouses, databases, schemas, tables, and loading data from Amazon S3 for data engineering tasks. It covers automating data pipelines, creating data pipelines and tasks, and provides code snippets for automating tasks related to campaign spend data and monthly revenue data. Additionally, it includes instructions on resuming and suspending tasks, observing tasks in Snowsight, enabling error notifications, and prerequisites for machine learning tasks. The guide also covers setting up a machine learning notebook, building a Streamlit application in Snowsight, and cleaning up tasks. Furthermore, the document provides related resources such as the Source Code on GitHub, Snowpark for Python Developer Guide, Snowpark for Python API Reference, Snowpark ML Modeling, and Snowpark ML Model Registry. It also discusses Snowflake's use of cookies and privacy preferences, including options for different types of cookies and how they are used to enhance user experience.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"a733ce94-88ff-4886-9e0b-6d820b3450c3": {"node_ids": ["5d2b6663-bfa5-41b9-b417-66edc8457af8"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b00d7971-ba19-4e91-80b7-79e173259844": {"node_ids": ["a6235638-b32c-4ef1-b4f6-b14ed4f57dba"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "f5520389-916a-4f5b-b7ad-5f2704df39ff": {"node_ids": ["2b05e1a6-2fda-4981-9c3a-fff8e4e9dd3b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a0c0e3ca-96fa-425a-ac47-934a5b00f302": {"node_ids": ["5d0a84cd-6cb1-4c76-88f7-ce6cf4887e02"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "9d01a50e-edba-4f36-8e93-0d3b04349de8": {"node_ids": ["3bf8ad8a-a03a-4bb9-aac9-f3d0e211afd2"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "79196d52-ee0a-4677-b3a7-608151870f60": {"node_ids": ["23e91978-8728-435a-b1eb-deb923d1b20e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8e26a0b2-a45c-44d0-ae18-832336084a45": {"node_ids": ["f0761b1f-60b6-47ab-969b-409e541859a3"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "f0187d12-3c4a-4514-8b06-2b5587c530a2": {"node_ids": ["6a18737f-5ec5-45cb-87d2-baaec0f24708"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b8fde317-d3b2-447a-8ca6-1f48a51493e7": {"node_ids": ["9fe3054c-a13b-48a5-bf79-a618b8859d4d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "7fb7dadb-8891-473a-8a34-8bffc19aa4a1": {"node_ids": ["fd95fa48-f7c7-491b-9fe6-7fa5a654c47d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "aa235d21-c392-45e9-96aa-9896d9459fed": {"node_ids": ["e69719e5-1b5d-4d59-b54c-3e7a1964bea4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ea6bd15d-3ce7-4541-a567-133997af210e": {"node_ids": ["67208868-b858-48be-8736-7add805b496e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "adc92171-2048-4d45-83a7-78ec6015cbf3": {"node_ids": ["3dc6f43c-ee22-42dd-a83f-f5f67c8ff76a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5ccb705f-32df-4078-8164-e24e18aba0b3": {"node_ids": ["48ff5b0d-e13c-4d5c-801a-a4b6e7ad2abf"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "32214cd6-3dc7-47ce-b6b3-a9e381e23ffd": {"node_ids": ["fb12d6fa-7d97-4c1f-9292-e78f73c8aa72"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cebfe9c4-47a4-4b54-89b2-82e9dbcc77bc": {"node_ids": ["6dddb114-d2c5-4913-adb0-0727f23b2680"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2a02ca36-b912-4254-a375-2d5a9381b4e2": {"node_ids": ["ea810547-dc8f-4e4e-a5a1-a18ab5a8b61c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b9074f0b-0481-4945-83bd-37f0f50ca259": {"node_ids": ["ec4da695-7c10-403f-a985-b884c88307cf"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "9e931095-a299-4167-be3b-24230692d612": {"node_ids": ["3ecacc0f-5b65-4cf9-9d6f-11a60ff2de42"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b594d96c-ee54-472f-a677-93acc0f02801": {"node_ids": ["756dddd6-c3c7-458e-bf6e-8b72f89e10f9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ecbbc8a8-4438-49bb-b4a6-d0d04a57121d": {"node_ids": ["33f73657-1e4c-4b7f-8322-883943963d96"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c9a88c71-575c-4fec-bf08-a8c33c93ccd1": {"node_ids": ["a2d96439-173d-43b9-a7a3-e8ce4d71144b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5b8bf068-4a8d-4980-bd1a-23f5a9e5c6af": {"node_ids": ["a5465e7b-a35b-4da9-b5ca-a66f36b5f1e4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "7110e9e1-dbc6-4d81-b637-7b742604fac3": {"node_ids": ["3f920df6-f97d-416c-9f7b-1b278a02c490"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1606dffe-c3f2-4317-9b82-d8650be2c097": {"node_ids": ["d1e67f00-0ece-4632-bcc1-d26e71984054"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "57f4a40d-606e-46ed-9fb9-f5809e036e03": {"node_ids": ["367a84c9-208b-488a-9988-638b80c5a0ad"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "27a84d3b-52ca-4584-b2f5-884794e5b582": {"node_ids": ["ec526738-0282-41cf-a0de-313334a93d70"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "618328f2-b119-4312-a059-9b30454bfda0": {"node_ids": ["392d3bae-4b0f-4304-84e6-f5173ab19f0f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0c9304bc-f643-46e2-9836-15d41e33e71d": {"node_ids": ["38562437-e31e-4e7b-bfbb-6e2682e30ee9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "fb4d764d-4c1a-4708-8793-ab32c615f94a": {"node_ids": ["83655f7e-595a-4372-80f8-64f3a84cc172"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "3fb8747b-1767-4289-94bb-077baed77c24": {"node_ids": ["ceef73dc-f07e-4c71-b419-444d35e5e383"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "44e31ce9-c501-4c3d-8663-239a6a9aeebb": {"node_ids": ["068d4911-a197-4c66-83a0-c353e89fdebf"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "de1ce3c1-eb1c-4fd8-b5e4-8a0f8e85f417": {"node_ids": ["c13691d8-0a4f-458a-bb29-a03df2027d5a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1ae2ac07-2489-4f80-84d2-a7304d412185": {"node_ids": ["8c366fd1-8c9b-4938-8638-c456f5fcca6a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e55948c6-d85c-420c-b00f-28fb5ca2c259": {"node_ids": ["8fdec053-3cc2-4fae-86a1-028cd2b4dc8e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "538fe6aa-ce33-4c43-ab73-74f921399f3d": {"node_ids": ["0edef6de-a2c6-4b04-8719-c35191181190"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ef50c065-26ed-4c35-adc7-a4396ccb7ef4": {"node_ids": ["9e04ebb0-a7bb-46da-9e77-ab856d3142db"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ccfc5bef-2c66-49ea-8648-74470273d1d1": {"node_ids": ["6beef081-ee2a-4e8a-81f1-0e597a7c2668"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "78a62381-83b1-4285-82d2-7560cfc42b6d": {"node_ids": ["042925c9-6d16-4844-a556-030f20972bf3"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "db699eaf-ef94-463f-a91f-cef0e7bec50b": {"node_ids": ["90167495-0368-42a9-8fdf-c74274f2a9e4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "912a9072-7a67-4441-8ede-0931561f03de": {"node_ids": ["9726bc30-5961-431d-b58a-16f0023b6e76"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6b7a554c-7339-420d-bc4a-9851472bd4ab": {"node_ids": ["8175030f-2acd-4a52-8dee-aa8dfc5954bb"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "20f5fae9-e9ca-4a02-92c6-957811cc5480": {"node_ids": ["ebc7e6f9-42dd-4c4f-8b97-c6aa52e92a18"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "17a26717-b9ce-4080-93e4-d6d7b8327b1a": {"node_ids": ["5f07d091-422a-470f-a382-75a192f7b20b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "4431a0b2-ff50-474a-87be-251ac8adefdd": {"node_ids": ["00b183b0-f5f8-4274-9c1c-7231d917af0c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "08c578ed-4907-4a25-9475-a0c0946a2e29": {"node_ids": ["0e148fd4-0955-4a37-8a25-8c8ce265aaf2"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "89d38fdf-8098-4191-8d54-80f5a9088055": {"node_ids": ["f730207f-749d-477b-af7a-3a81cc7c21bf"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "22e42d19-9c1f-4972-bdf6-a919f6adb096": {"node_ids": ["bf05d258-264e-4544-aaff-749b1390b51d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "73f1c556-d756-498c-9334-4f18bb8743fd": {"node_ids": ["64b973f3-b332-4da1-b068-945986a9e3b6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "7f7fefea-5060-47f4-b742-7dcd757dc6ce": {"node_ids": ["58beacb6-5cdf-47c8-b231-9a646b86ebd4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "420c7222-1f01-4417-be2e-acbe12b5f0b6": {"node_ids": ["188baac2-b610-401a-aae4-c3b009c292ec"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d55814fa-8638-4141-8418-bfeb18d7ec28": {"node_ids": ["615c8611-8c13-4c60-be41-077e144055e2"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "508ab91a-2f2e-4a50-a8b9-7c203c9c9e11": {"node_ids": ["38f88f33-7cc5-456e-93c8-37d1881ff767"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "691f3498-b4b5-471b-9f41-1cdc3e0f13fd": {"node_ids": ["f3b7b744-3f39-46c1-8c89-9109bbed095b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1a7ec057-f8cc-49aa-ae5c-5aecd2755502": {"node_ids": ["c03dcf62-dfa5-42cc-98a3-b4f31122381c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_name": "cloud_native_data_engineering_with_matillion_and_snowflake.md", "file_size": 43603, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "07c63003-cb6b-4d7d-88cd-8c8d85dd6a13": {"node_ids": ["2d4e0e90-6ce3-4474-84a8-8831602a5859"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "43fae96a-eeea-4793-b587-dcfa3bd5d161": {"node_ids": ["52b8265f-1227-453b-9d58-fcd43121ef5f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "7f2de0fd-1545-464f-af39-77c2b309ef45": {"node_ids": ["8040c4aa-f781-4c72-82bc-7c0a91f57eb0"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0e2e1c9f-ba94-4652-8c35-1a29af359be3": {"node_ids": ["d9ccbe0f-301e-479d-b543-2d6efc573db3"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "93f00707-9930-46d8-9f06-0fe4c663b71e": {"node_ids": ["6ce5e8d4-78c2-417a-afa0-2001e6c46de6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "14fbac6c-813b-4aa2-9bdd-bd28b6bf26de": {"node_ids": ["30731fcb-d003-4657-b818-4b4fb419f461"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ad700b0f-aa45-43ae-83a2-be28190729d3": {"node_ids": ["77c56b04-ca29-4acc-939d-d5d45197091c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0f5ad8b7-1624-4629-ace6-bcacee58fd7a": {"node_ids": ["1f0517d6-21a0-4d6c-8925-be1034d8a24f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d573fc26-40e4-4053-8a73-065d4e7195a2": {"node_ids": ["1335defa-aa12-4033-aeea-4a2305b37ce9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "37f9b070-a014-4040-a651-7bdd7ceead4d": {"node_ids": ["e922d88a-e3d3-497b-a825-404210e20d1c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d8abe616-ae9a-45ee-9812-de5e3ec23f53": {"node_ids": ["418eb099-3ef3-44c1-bf97-c592e09e8579"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b20fb21f-1865-4b97-9360-ad5e7fe7887c": {"node_ids": ["c54409ad-989e-49fd-ad89-ed4570c0a31d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cc8b1baa-3cfc-46c7-8ce5-2d2b3346356c": {"node_ids": ["ca00c42d-0770-48b7-ae2b-4f1c781e8d4c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "503f0a38-5972-4d79-95ff-23d521f6cb1c": {"node_ids": ["f2dceb09-3e69-472c-bc53-e826db2be258"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5c423d2f-9155-4291-b662-d574bf78b36a": {"node_ids": ["71f5e318-a189-4fde-b39f-f947426ac059"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "120f2c2a-99fd-417b-999d-3d57d3f141e1": {"node_ids": ["63544937-a439-4096-8802-1ffefcfa1d9d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ac24a6f5-5d32-40b9-aa87-38cc1e8388b4": {"node_ids": ["edc709b9-24eb-40c5-856a-7a2346821c35"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a7137453-c90a-419f-9667-aeb74d5b922d": {"node_ids": ["30a4a5ec-6abd-40f1-9879-59d193f41e51"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1f5b1576-4cb3-49fb-9f7a-b0f115c61c00": {"node_ids": ["591444bc-4af0-4aa7-af3a-b37dbdc7754f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1f9fcb0b-9b01-4bcc-87f3-1f874bdd3beb": {"node_ids": ["c24476fa-86b9-4ccd-a4d5-bbdfdc4d0891"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a3b26254-6fc9-4f1d-acb5-e0955092485a": {"node_ids": ["84f1bbea-95d9-47f7-894b-816ec83880f2"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "483d1b09-d12b-4cb5-89b0-41060cf45f71": {"node_ids": ["41715f74-6bb0-4894-bccc-873c3fbe9d05"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d3328c64-b8ea-4e21-a342-3f888c306283": {"node_ids": ["7c6ea3be-5f06-470c-8076-c639723823ce"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6187b8ad-6bbc-485a-9982-605a29a083ff": {"node_ids": ["a7c1653b-bbfb-4578-ade0-c8eeabdbce73"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "53787251-c893-45bc-9459-e85fed273463": {"node_ids": ["901fc56b-8dc7-4027-a250-4c8eed419dbd"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e5a35427-a7b4-46a1-8687-c0191f6a1bb2": {"node_ids": ["43942304-f46d-4def-a0e0-30461f666486"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d2d99ca6-f126-44d4-9b71-596df4d5d36c": {"node_ids": ["dd3500c1-9674-4ef5-a9da-1c73a7db2d83"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b3cc1582-c7b3-48e2-92e0-663037e43711": {"node_ids": ["844227b3-6eee-4736-81e7-b501de1c62ff"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "9d2ee27e-3301-4b04-bc24-070959617957": {"node_ids": ["786a2024-7669-4e46-885d-e176b9c1f52c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b43d78fb-9919-4b37-80fb-47199f52e96a": {"node_ids": ["3c4e4c71-d3d4-40fc-8716-0ccf19496c08"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a5d69d17-b9cc-4386-848f-0d5ce4bdc6b6": {"node_ids": ["b9069655-a8af-4294-821b-9ebab8a39a7b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0d2d79df-746d-497f-8113-2f8762d21247": {"node_ids": ["61667a92-ebad-442d-a1ee-ece1402be1b3"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "18c40c4a-dcba-41f1-ad88-ad6af3c275b9": {"node_ids": ["9fd86b9e-3224-4bdb-8d17-eb43ab2104a7"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c68b55c3-28af-4588-90f3-ef2a438c3974": {"node_ids": ["d6a1accf-9f0e-48f5-a6f8-7e6752763ef4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cc362518-dce9-434d-be8c-82259f3d8849": {"node_ids": ["a53baff8-cd4e-4f34-a3ed-3ff460a6119d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "bbf3bb87-bb52-431b-8664-2f7fd39c79db": {"node_ids": ["11a24237-4ed1-40bb-ba44-3f060ed53019"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1eda91c2-f7e0-4241-aaf5-4cbbf5f70d56": {"node_ids": ["c1932a9a-8359-441a-820f-c5b90cc71bbb"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b480a476-3b85-4054-ab87-6f772877103a": {"node_ids": ["69f3e43a-1c26-49d0-a744-1c213352d812"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "519557be-c726-43f5-9451-55eb94154174": {"node_ids": ["d1b88e21-fad4-40e1-a393-53890dcb9995"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0ac2473f-4d16-4a8e-8c20-a65723025631": {"node_ids": ["bf3677e0-c9ea-4344-9851-7530f2a73a9a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ae36a23c-fce4-4d72-9dab-e9de1f88bd75": {"node_ids": ["e0717bf6-5865-4a1c-95c0-458d929dcc6d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e791a38f-9916-47d0-806c-cdcc8ac25756": {"node_ids": ["b726a81b-1aae-4f8f-9f23-1b1d81896f12"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2f41d404-ece0-47fb-9dc4-af44212a0394": {"node_ids": ["5f7b4cea-f295-457d-943e-29b7a4e589cf"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8bc41016-0ed5-46eb-a0ba-538cafc881c2": {"node_ids": ["e44431c4-52e8-4f72-91cc-2b7a5af611b6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "dd73e4dd-2354-4d4a-bf6a-b8ab18138c44": {"node_ids": ["1af41e5a-afbd-486a-9c77-f61c4187cbbe"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "97c90058-8c5f-41b1-b0b3-4689824bd4b5": {"node_ids": ["bc0d0604-ead4-40d1-a9c9-bb61c3a70551"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ccd05586-212f-4737-b792-275a5a88886f": {"node_ids": ["9b2ab42e-2b31-4d8b-90e2-fd5a7eb5c847"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6b6d9dc6-154a-4247-9bd6-cf4b482b002f": {"node_ids": ["68640ba3-5b21-4f9d-8bbc-70f4cb0aea44"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8bc124b2-cbb0-4133-9bbd-d964a0304bd0": {"node_ids": ["0683b035-25be-412d-b37e-b721ebfaa012"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5df8b2c7-12a6-4784-a1a3-d954ab6c2ac7": {"node_ids": ["c438f145-78c5-45b0-8a87-80151b5e1c0d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "64466d9f-fd2e-4f5e-b572-c45223a0592c": {"node_ids": ["28fd5933-2014-4bff-830e-3185e3e03cc7"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "034d36c7-152e-4f6a-afef-dc4ce449c2d3": {"node_ids": ["a34e35d0-1275-4cc5-8849-9ac9f5eec6c5"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "4f5c7ad8-94b4-467a-85d5-7168af917740": {"node_ids": ["5b6bbf0d-b5d1-4eaf-82de-b440c8de0ad5"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "3d88415e-8318-4ddb-b469-dc9ff55175d4": {"node_ids": ["1df25d21-5bb4-42a9-9b25-ff87ef0f2dde"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "77b98518-12d7-458e-acf9-a9b44ee63255": {"node_ids": ["7e0ae116-320d-4bac-a6db-269d8a9b8eac"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "09b082f7-50be-4097-a775-7ff5b1174ca4": {"node_ids": ["cf260c36-2e62-490f-a348-cd6b6a705735"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a2e2b7ec-fa03-4daf-bb58-038b18c00c08": {"node_ids": ["43c06e1b-66b7-4e1d-a6d3-bdb79b299565"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8d1609f8-c946-4208-ad59-6d85e89286b1": {"node_ids": ["7f362c69-2ce7-4699-8cdd-cc2d1e39b77b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "78d6d00c-3aba-4352-8a81-297fe1e48080": {"node_ids": ["bf950538-c27d-4a43-b571-3725bcba7b16"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2ef0aaf4-f004-4d9d-b4a4-4a699e880b0c": {"node_ids": ["d18a704c-b7f6-4ac9-9c8c-9efbcbd87e32"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "43462acc-a4e5-4aa4-9726-7f2cdca2ea0b": {"node_ids": ["3260cbd3-00a1-4e35-b98e-8a69a61dc99f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "38ddfa0d-00e4-4f26-8860-1efe4b85eb91": {"node_ids": ["743616f4-5b01-4dde-9dd7-96cb495b478d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "11567799-1cda-4c36-a5b1-7e53d7a7a5bd": {"node_ids": ["53e52ceb-d20e-4cd3-b675-21823768669d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2400f6aa-f12f-47ab-9ec0-ba379edd013c": {"node_ids": ["a42142e9-583a-4cd2-9596-9c8d9863ea98"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ed01398f-a4d6-4179-9219-161d9510fc80": {"node_ids": ["d4a3eba8-9828-42b7-bb04-feec0b4de226"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1a7dd1f0-b8bb-48c8-9830-397c055d758f": {"node_ids": ["b8789a67-460d-47a6-9fc6-02c0db3428ad"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e0e6b4e5-595e-4ea1-add2-34c110f5c71c": {"node_ids": ["292e71ce-f384-46e4-bf32-78ba5bbc6d47"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0ed0ff19-5549-4533-95b0-0277a217e31b": {"node_ids": ["c58c5f80-1598-4aa2-976c-4bfdaffa4498"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "89baa835-38a8-4904-87d5-102bcb0e7361": {"node_ids": ["445e167a-4689-4331-a409-1e370b452599"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6b31014a-eef2-41c3-9719-6f34d12a09e2": {"node_ids": ["60de9efb-6aa1-4b4e-9756-6dad713abea2"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6ca2c4c3-5820-4c8b-bbb7-1ab56dc84eb1": {"node_ids": ["4e3e983b-ea5d-4cfe-a39c-3c2a214cfd76"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cedd2731-34a2-494a-90c5-1bc11c9b9fb9": {"node_ids": ["3e72652f-02a6-43d9-bb43-49bc513ba4f4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c3c6d656-037e-4bd2-9e97-1df1c58225e1": {"node_ids": ["7bf7ccc9-7c8e-4621-9344-a672aec1be2c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "3b09d4df-7047-4119-8caf-61645c291808": {"node_ids": ["f82ed1b4-935a-4ffb-a3f3-5306607decba"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1d321286-2cbe-46fd-ac0b-5b8f5d721643": {"node_ids": ["e8ada223-9db8-40e7-91e9-2eae49f34c17"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "768217be-95f5-4827-93d1-0fce03320ca7": {"node_ids": ["c2154083-8137-4184-9cbd-d8cda3dc56a0"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1070558e-1571-4897-840b-14d0f7aaf36c": {"node_ids": ["90a6c7e9-4b00-4e51-9c4d-512bf5d639b6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_pipelines_with_snowpark_python.md", "file_name": "data_engineering_pipelines_with_snowpark_python.md", "file_size": 61871, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8e123543-6967-4d54-b0bc-43260d86f3ee": {"node_ids": ["1c7707de-f394-4264-aa7a-2d470ee00f17"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d84de500-3e2b-4d48-a8eb-4043be4cb958": {"node_ids": ["aeffc19e-b955-4967-ae15-f84aaecd67ca"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "f196f29c-9898-42c6-a88b-bf11d66c381b": {"node_ids": ["0d475db3-41dc-4bad-b9d7-ac51a7a43026"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "bf8c9660-008d-4b09-84fe-d3503830c223": {"node_ids": ["0cf1b229-5b3d-442b-93ed-7bfb11cc2aab"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d8ae9318-28d9-45d5-8281-af39698cf6c1": {"node_ids": ["7c393514-7a78-4fa6-b116-e710527cb20a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ef20831b-1347-4943-88b0-20c8feaeb95b": {"node_ids": ["cb5cac62-8a8e-4b0f-84cb-602f79eae657"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2f0920b8-0f84-45b1-bf24-f9c08140496c": {"node_ids": ["5aa29ed7-fa53-4131-b69d-02503a6d9968"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1d2987ff-b8b2-43de-8580-453697c32c65": {"node_ids": ["ee0c215b-8152-48fb-b5ad-4d7f0c56fe7c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6ee05085-cccc-4bae-b76a-17e5e282cf37": {"node_ids": ["7145d89d-b33d-47af-a6a6-6a8b5d9435e8", "69f1c7b4-8fe1-4035-8ce5-5897e3bcf694"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "828605ba-5131-4e7e-8acb-6f58ee4f9e30": {"node_ids": ["c5a452c5-b0f9-4ea2-9953-8d5cf5f8c1ed"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d08dfede-3afb-4489-822a-7baa362c62b7": {"node_ids": ["762461c3-1ef7-4dfc-84fa-600fe234ec74"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1b7c521c-b322-4871-b154-cb009d64055d": {"node_ids": ["4288bf7b-dcc8-40c8-b1de-d8b47c34c282"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cb6712de-db0a-4ce5-b7af-044b7e734d8d": {"node_ids": ["a2ecec2c-ffa6-4aaf-b106-5dff918770d4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "4a2633dc-9e5e-4263-bc05-80db13c174d2": {"node_ids": ["dcd8b469-9fb2-481d-978d-575e744d615c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "75839b32-9fad-4693-925b-b66c76b23b3c": {"node_ids": ["498666a1-d7b0-4fe2-acfd-5de631d924dd"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "41154eb6-1efe-48ef-a419-bfc90bf9147d": {"node_ids": ["2e42307c-1e89-4fd3-a115-838fdd8283b6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ca7a12fd-763a-4702-a99c-571c4d379705": {"node_ids": ["916dcd27-48d7-4aa6-b5e3-2fbc2daef0bd"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "40463d32-8daf-4a36-8770-499d5ccf226d": {"node_ids": ["db39e369-5f2d-4e16-8262-2785cb7290f4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5d096a7b-6ece-4607-bbb1-bd33aec2bceb": {"node_ids": ["04e1b5a9-60bb-49e1-9736-5dd07f82ca8d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "fb39b745-b644-41a1-872e-94da3cc1e42c": {"node_ids": ["b8e90324-ec46-48a8-a092-55eddabd02ed"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "f9c0f9cb-6fec-44bc-8ed1-0f2f6b6ab300": {"node_ids": ["bbbc5c48-7fd2-4ed7-a6aa-5919688f154d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5b59bd34-a1ea-4d8e-b737-b22ac7698046": {"node_ids": ["f59e818b-28d6-497f-9218-8ef20a3b9f08"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "9ccdc480-3dc8-482d-9c6f-7abe0d886db8": {"node_ids": ["7aa2eff4-307a-43c4-97de-e0173d4f3cc1"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "98294a4b-8e60-4209-9d18-6a97ca746522": {"node_ids": ["8386241a-f297-47b1-8f02-3fe4471a473d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "dd6b45a7-b792-4530-a976-dae2d96a08b9": {"node_ids": ["f830c562-8ba3-44cd-a4fc-0304d76ac599"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "fcc786f0-f412-41fc-88d7-7af3ff617a73": {"node_ids": ["5b45e197-453b-4179-ae28-61f5741c8d48"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "dfb3ed91-2fff-4c1c-a57b-07476bc54a56": {"node_ids": ["af77e483-1b0f-4f46-a24f-a6d1a9ca54b0"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "6dd8cb00-c892-41df-a601-7d9ecdd8c329": {"node_ids": ["882434e1-92d6-489a-99a7-67aadb38ae59"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "38d78a61-473a-4827-b6c4-da6dcf9941f9": {"node_ids": ["f807e6a9-6f78-4400-836b-451cb960375b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ad594b28-ca2a-4efe-ab33-34d74644e6fe": {"node_ids": ["4c3a62d4-a515-4616-a8c9-9b121e0a8954"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d4a63ea7-31af-4e4e-b525-98b9f6562d2e": {"node_ids": ["d18e74c6-aed3-4ecb-bfab-28292bd17697"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1d269188-7a16-434c-8bde-f23e8896f10d": {"node_ids": ["29039804-9cfd-4cc5-943f-2a817869666b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d9b8fa70-5582-4dc6-8999-b43d6b85d217": {"node_ids": ["b8d2c07b-9d4e-4d1d-8216-e5a36d512466"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e9d9002a-756c-48f4-9bcb-3107519b31a2": {"node_ids": ["c3c58786-4fcb-41a7-92e6-1c26c23e5f49"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_apache_airflow.md", "file_name": "data_engineering_with_apache_airflow.md", "file_size": 36387, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "738bf1a2-eaa3-4dde-91da-dee0de861383": {"node_ids": ["43bc2fcd-08a3-4eea-ab8b-62811f76b31d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "808c1c41-6d28-441d-81cb-b6120b8f9dab": {"node_ids": ["27c6a234-3a46-493d-83cf-98070463b42e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "af78da87-a865-428e-b2d3-940b2483d507": {"node_ids": ["8e0f8ecd-4e52-48e6-b294-5f8afef9b61f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2b1ded5d-f7ae-48c5-9d8d-fe2bc4859c33": {"node_ids": ["f833fc80-f5b2-43c2-b0c6-0ea92d6c935e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "95437d44-1e4a-4f50-a06b-a70eca0d15b0": {"node_ids": ["35e7b24e-6c4c-4254-9756-0d4129057640"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ed1a9377-cdfc-4f41-b2b7-d2d1c1a66073": {"node_ids": ["baa07f0e-84d8-46a7-abb5-d323e0cb8ccc"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "9b0cadaf-a01a-4018-b59b-bc92ec0d4da0": {"node_ids": ["d6b4b736-3119-464a-bfdc-f2dfeb90012e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "070616f9-0c5f-42b5-adac-e3751842b2e7": {"node_ids": ["33fe586e-3838-44aa-8b6e-03619939c9d6"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "0228951c-1a5a-4a45-bde8-20e01de18878": {"node_ids": ["4362f1ee-fdf4-4261-bc07-a512169ece67"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "af3d69e9-ff54-431e-8eac-f06dd4eb3e76": {"node_ids": ["7967424c-25c1-4b7c-a598-acbe39882de9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "86d2c220-0a8b-4337-ac14-86b64db7f6e1": {"node_ids": ["4400a42d-b508-4e26-a90f-8bddc97f9446"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "66dafcac-9034-400d-b7d1-6cb021583260": {"node_ids": ["c97fbcc8-5500-4b11-8660-2e0fbf202b79"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "24d4f034-b9e4-4369-9c49-3119b310b548": {"node_ids": ["721f456c-7bf4-4a24-85e4-6e45ce3e6574"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "dc87abf2-5f82-4618-a906-f262c7e32818": {"node_ids": ["b5b19af8-2842-4176-91bd-3ddacab95a4f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "1e170d7e-0219-44ef-90ac-680024e4a38f": {"node_ids": ["5cd927f3-7c2b-4720-9431-321cdb1ae55b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2a3b5954-007d-43a5-904d-29a6a9052e82": {"node_ids": ["eefab3a0-fb96-4be7-83c3-dd085ce1c263"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "fcca436e-32b6-4131-9965-551529181e79": {"node_ids": ["65f6b9ef-98c2-4030-a7cb-368f752b212d", "ee4a0c98-3a55-4f71-a0c3-d5fefc536e44"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "541021e3-62fe-4a44-af91-cbc6f109976a": {"node_ids": ["631cac94-e2cd-472d-886b-9b3f38e5ba32"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "16ac756b-c246-4cff-b668-49cda942e4b3": {"node_ids": ["2ab02b58-f0f0-44de-8328-9afe8253a9fa"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "319c2a65-fcff-4bdb-bc01-11ae4ad79a6e": {"node_ids": ["2775871e-a372-4724-af90-b61a4d694032"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ba434c09-b641-4a78-b697-b9e3ea4e85f8": {"node_ids": ["6827eb2b-db43-4da4-ad8d-d08429642a91"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "59d99ede-5036-4de8-9cbd-9c179de7dd1d": {"node_ids": ["c4ab0e8f-c2f6-4718-aea7-b62eabab107d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "2dfbf854-899d-4813-9a03-9ac74644979f": {"node_ids": ["14ad0550-d114-4ef8-b9c5-6a684abb6126"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "56ddda2a-f3a5-4595-a525-a6234ecdd8c2": {"node_ids": ["d81971b1-5ece-45cf-b76d-9dd7155fd164"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "60ab73d7-371e-4eb4-86d0-dac99b341355": {"node_ids": ["eff7c45b-8876-4552-9c0a-4f48e5331e39"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "826696e2-85bc-4f46-a80e-0281e76111bb": {"node_ids": ["2444b2a1-fa23-4023-ad27-72bbd98c297e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "95f884ae-7871-4dd0-8826-34b4a1b8892f": {"node_ids": ["7fa9b4a3-008e-49a5-849b-55cb1b58aa4e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "84ea060b-8647-46f1-b040-ff3ba0bac1e2": {"node_ids": ["3f9fe088-82a8-4953-b649-3d801781ac02"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8c092a1d-422d-4aaa-b04d-d595eebe7a2e": {"node_ids": ["393670cc-d306-4499-84f0-ca41247fb5de"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "608aafea-5510-4f18-b150-b146f3027a75": {"node_ids": ["348009a3-6d66-4603-813f-31143d202cc9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b49ef040-ef86-4025-834e-5632d2352d13": {"node_ids": ["c497fa09-6589-4b07-b9f2-27798f0857d9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "258ec2ca-b291-4034-8c40-8bc47c56f7d1": {"node_ids": ["79e31419-4df2-48d1-8527-70a0e2caee0a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ffc11a9c-7ce3-4d0b-b52c-d14b46f8c798": {"node_ids": ["a728d1c7-b7d2-40cd-bbe7-6814eaa3c218"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c5fd03f5-cc00-47d8-ba88-1c4574a3f31c": {"node_ids": ["b235cb7f-a146-46c7-bc27-1cb248e26112"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/data_engineering_with_snowpark_python_and_dbt.md", "file_name": "data_engineering_with_snowpark_python_and_dbt.md", "file_size": 32864, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e19d7283-f1f9-4a4b-8aa2-3edd8c2b6448": {"node_ids": ["25acd883-9570-4d9e-8863-6a6007877b53"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b55bbe6f-bb9d-49a8-a54d-bc137e07bfca": {"node_ids": ["c6fc1c36-c758-469e-8b64-9b93b55412ba"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d2d9ce3b-0ce5-4437-b8f4-e7b2a73f6326": {"node_ids": ["f3a7eacc-d017-4ab0-a460-2d24f50a3e95"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "3a57f718-94cc-4f09-9700-3543e95c4e50": {"node_ids": ["14fbc204-f12a-42c8-ae50-38ee75535741"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "cc6917de-9329-4048-9f16-c251e2d1863d": {"node_ids": ["d044ef7b-4eb4-45f2-bdb4-cf9801ee3b5f"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "25ce4b9f-9800-435a-97aa-6f0425f75594": {"node_ids": ["57060b29-5d65-4dc2-af2b-82ef79c2238c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c0a67858-64ea-4182-a21f-07453bc5a663": {"node_ids": ["411371eb-a753-463c-86ea-933296b29622"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5582b301-a846-491a-b4f9-88eb44b26f81": {"node_ids": ["62de375d-817f-445d-af47-285008aafb00"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ec540a68-1ac1-4c09-a70d-d55fcaaf2d6a": {"node_ids": ["33f630c9-c9d7-4305-911a-0960e42bb20b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ab18cadd-350e-48ee-afdb-a0751b03d4ed": {"node_ids": ["17b89515-8c0e-4643-913e-49f9149af623"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b2c71e46-5c60-4cf4-974e-891020707f46": {"node_ids": ["386e8c27-bcb2-4af4-b520-268b22193dca"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a583d714-c337-4b39-9e86-9e626c953529": {"node_ids": ["e47f2d16-2f8c-4bb7-8b86-1c23da1e9d00"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d2b6cd77-b343-42a4-84bf-b4771a56e23e": {"node_ids": ["48d53300-7a44-4c95-9df9-5fcd8e5a5360"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "d71cc488-5d71-4826-a19e-fcde02a901fc": {"node_ids": ["23e3a298-6ea1-465d-b644-22c2745c7f45"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "7cd4c9bb-1162-47a3-8c0a-5ef8c67050f2": {"node_ids": ["ae29f452-2855-466e-8fef-e4cc83d347e4"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "939cc4d6-0010-4590-be52-5307ef2fccd6": {"node_ids": ["5eaf5487-0725-445c-b6fd-aa36b3783233"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "68f64ab3-96f3-44c9-b985-f615b75345a7": {"node_ids": ["59bc6271-d860-4282-a06d-cc9db401b0a9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "977d2ba1-26ba-4e1f-9ad6-b3e49f26e190": {"node_ids": ["b8646851-2a6b-460d-9eca-c81dfceb4009"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "a5adb3c3-d665-44f5-aeeb-ae061cdab3c2": {"node_ids": ["3db3c190-8725-4159-b5aa-670947300255"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "68c8eea7-0e8e-4ccd-9c97-bb909a682b72": {"node_ids": ["cde97679-392c-49ce-9864-c495cfe102ea"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "388f6cb4-6216-416f-8f29-a4713a0e820a": {"node_ids": ["b3197e30-2954-4f1d-8b2b-c85e39a2a0a0"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b59e7ddf-4e06-45f8-98fd-1be93facbf52": {"node_ids": ["6fdaf52b-d1f7-4105-ac30-fa2fa1a97fa9"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "b96cdbe8-fc7e-4740-89ea-3fb497343439": {"node_ids": ["892707aa-c62a-4f1c-a02a-3be6d7268ed7"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5dafe8e9-9c65-4a5b-88c1-6c01a380db43": {"node_ids": ["5bdbd137-a951-49f7-a0f3-a2e61c89ee81"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "79e8e835-1128-480a-a8c8-4dbd220db150": {"node_ids": ["6badb99c-27b8-451e-a1e5-8f7a2fa2665a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5b5c33d1-cbfc-4552-8761-8277156d2981": {"node_ids": ["cb24ccbf-9161-49f3-bd4d-1574b0dab7ba"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "ded36305-a0ed-4456-9622-353b15d30a01": {"node_ids": ["d7429b22-b3fa-4ca3-8d2e-fca40bcdb02b"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "bbff4927-6f8a-4717-a7b5-4faca5c3ca75": {"node_ids": ["cf756990-45c6-48ce-8dc9-dbae27209b69"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "202dbead-1d12-4e62-93dc-001ca0f99504": {"node_ids": ["c1a8a8ba-e2a0-4d78-8326-d22cf10e2b4a"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "bf6128b9-70c3-4494-a4ae-99105d7678e7": {"node_ids": ["c43ec9d4-300f-4667-aafb-70ee67eb608e"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "e1df3f0a-8d73-4c0b-8721-17fe836343ba": {"node_ids": ["e159ac80-5025-4508-afc1-2c0a6fc23180"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "147d18ed-d32d-443e-82e6-76ab6e61e411": {"node_ids": ["734abeab-32a4-4daa-a9f1-de9549ac0522"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "5e29c248-6458-428c-8f85-9c0c1761f29d": {"node_ids": ["8f44c2d0-feb2-4854-98d7-8cd6310b9d3d"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "c90174a3-70e5-46d9-a27c-48b681479604": {"node_ids": ["4610c535-7ea7-442e-94e0-10d70077b49c"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "8ddc942e-33ac-48a5-a9d3-ec7613333513": {"node_ids": ["ec74560d-6930-4d06-934d-93d544f32f70"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "44c59420-9342-4295-9c90-2ca56c95ccbc": {"node_ids": ["a1e7ea5c-59d1-4714-a57a-0910efc22dc7"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}, "264e76d8-6025-4ff0-a8cb-16c7fbcac64b": {"node_ids": ["5f8180db-7320-48f6-9d9f-f078abe7c774"], "metadata": {"file_path": "/Users/pikeche/Documents/snowflake/udemy/streamlit-for-snowflake-test/chatgpt-rag/pages/getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_name": "getting_started_with_dataengineering_ml_using_snowpark_python.md", "file_size": 28602, "creation_date": "2024-10-09", "last_modified_date": "2024-10-09"}}}}